{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GCP ML Engineer Certification Study Guide","text":"<p>A comprehensive study guide for AWS ML professionals preparing for the Google Cloud Professional Machine Learning Engineer certification.</p>"},{"location":"#overview","title":"Overview","text":"<p>This guide leverages your AWS Machine Learning expertise to accelerate GCP learning through:</p> <ul> <li>Direct service-to-service comparisons (GCP \u2194 AWS)</li> <li>Parallel implementation examples</li> <li>Architectural pattern mappings</li> <li>Hands-on lab recommendations</li> </ul>"},{"location":"#study-guide-contents","title":"Study Guide Contents","text":"<ul> <li>GCP &amp; AWS Comparison - Core ML services and their AWS equivalents</li> <li>Technical Aspects - Deep dive into training, deployment, and monitoring</li> <li>Architectural Patterns - Common ML architecture patterns for both platforms</li> <li>ML Concepts - ML/DS concepts in GCP context with AWS comparisons</li> <li>Quick Reference - Service mapping tables for rapid review</li> <li>Common Scenarios - Real-world scenarios with detailed solutions</li> <li>Hands-On Labs - Practical exercises and preparation checklist</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p> <p>\u2139\ufe0f Note: This Vector Search guide is created with the help of LLMs. Please refer to the license file for full terms of use.</p>"},{"location":"LICENSE/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2025 Shreedhar</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"concepts/","title":"ML and Data Science Concepts in GCP Context","text":"<p>Section Overview: This section covers fundamental ML/DS concepts and their implementation in GCP services, with AWS comparisons.</p>"},{"location":"concepts/#41-model-selection-and-evaluation","title":"4.1 Model Selection and Evaluation","text":""},{"location":"concepts/#algorithm-selection-by-problem-type","title":"Algorithm Selection by Problem Type:","text":"Problem Type Algorithms GCP Service AWS Service Binary Classification Logistic Regression, Random Forest, XGBoost, Neural Networks Vertex AI AutoML, BigQuery ML SageMaker Autopilot Multi-class Classification Softmax Regression, Random Forest, Neural Networks Vertex AI AutoML, BigQuery ML SageMaker Built-in Algorithms Regression Linear Regression, Random Forest, XGBoost, Neural Networks Vertex AI AutoML, BigQuery ML SageMaker Autopilot Time Series ARIMA, Prophet, LSTM BigQuery ML ARIMA_PLUS, Vertex AI Amazon Forecast, SageMaker Clustering K-Means, DBSCAN BigQuery ML K-Means SageMaker K-Means Recommendation Matrix Factorization, Neural CF BigQuery ML, Retail API Amazon Personalize"},{"location":"concepts/#evaluation-metrics","title":"Evaluation Metrics:","text":"<p>Classification: <pre><code># BigQuery ML evaluation\nSELECT *\nFROM ML.EVALUATE(MODEL `project.dataset.classification_model`)\n\n# Returns:\n# - precision: TP / (TP + FP)\n# - recall: TP / (TP + TP)\n# - accuracy: (TP + TN) / Total\n# - f1_score: 2 * (precision * recall) / (precision + recall)\n# - log_loss: -mean(y * log(p) + (1-y) * log(1-p))\n# - roc_auc: Area under ROC curve\n</code></pre></p> <p>Key Takeaways:</p> <ul> <li>Know which metrics to optimize for imbalanced datasets (AUC-PR &gt; AUC-ROC)</li> <li>Understand precision vs recall tradeoffs</li> <li>Remember cross-validation prevents overfitting</li> <li>Know when to use different evaluation metrics</li> </ul>"},{"location":"concepts/#42-bias-and-fairness","title":"4.2 Bias and Fairness","text":""},{"location":"concepts/#detecting-bias","title":"Detecting Bias:","text":"<p>Vertex AI Fairness Indicators: <pre><code># Analyze fairness metrics by group\nquery = \"\"\"\nSELECT\n  gender,\n  race,\n  COUNT(*) as total,\n  AVG(CAST(prediction AS FLOAT64)) as approval_rate,\n  STDDEV(CAST(prediction AS FLOAT64)) as approval_stddev\nFROM `project.dataset.predictions`\nGROUP BY gender, race\n\"\"\"\n\n# Check for disparate impact\n# Disparate Impact Ratio = (Approval Rate for Protected Group) / (Approval Rate for Reference Group)\n# Should be &gt; 0.8 to avoid discrimination\n</code></pre></p> <p>Key Takeaways:</p> <ul> <li>Know metrics: demographic parity, equal opportunity, equalized odds</li> <li>Understand pre-training, in-training, and post-training mitigation</li> <li>Remember legal implications (disparate impact ratio)</li> <li>Know sensitive attributes should not be direct features</li> </ul>"},{"location":"concepts/#43-explainability-and-interpretability","title":"4.3 Explainability and Interpretability","text":""},{"location":"concepts/#vertex-ai-explainable-ai","title":"Vertex AI Explainable AI:","text":"<p>Integrated Gradients (default for neural networks): <pre><code>from google.cloud import aiplatform\n\n# Configure explanations\nexplanation_metadata = {\n    'inputs': {\n        'features': {\n            'input_tensor_name': 'input_1',\n            'encoding': 'IDENTITY',\n            'modality': 'numeric',\n            'index_feature_mapping': ['age', 'income', 'credit_score']\n        }\n    }\n}\n\n# Deploy with explanations\nmodel.deploy(\n    endpoint=endpoint,\n    explanation_spec={\n        'metadata': explanation_metadata,\n        'parameters': explanation_parameters\n    }\n)\n\n# Get predictions with explanations\ninstances = [{'age': 35, 'income': 75000, 'credit_score': 720}]\nresponse = endpoint.explain(instances=instances)\n</code></pre></p> <p>BigQuery ML Feature Importance: <pre><code>-- Global feature importance\nSELECT *\nFROM ML.FEATURE_IMPORTANCE(MODEL `project.dataset.my_model`)\nORDER BY importance_weight DESC\n</code></pre></p> <p>Key Takeaways:</p> <ul> <li>Integrated Gradients for deep learning, SHAP for trees</li> <li>Know global (all predictions) vs local (single prediction) explanations</li> <li>Understand baseline selection affects attributions</li> <li>Remember explanations add latency to predictions</li> </ul>"},{"location":"concepts/#44-hyperparameter-tuning","title":"4.4 Hyperparameter Tuning","text":""},{"location":"concepts/#tuning-strategies","title":"Tuning Strategies:","text":"<ul> <li>Grid Search: Exhaustive search over parameter grid</li> <li>Random Search: Random sampling from parameter space</li> <li>Bayesian Optimization: Uses previous results to inform next trials</li> </ul> <p>Key Takeaways:</p> <ul> <li>Bayesian optimization is most efficient (fewer trials needed)</li> <li>Grid search guarantees finding best in grid but expensive</li> <li>Random search good baseline, easy to parallelize</li> <li>Early stopping saves compute costs</li> <li>log scale for learning rates, linear for layer counts</li> </ul>"},{"location":"concepts/#45-training-serving-skew","title":"4.5 Training-Serving Skew","text":""},{"location":"concepts/#causes","title":"Causes:","text":"<p>Training-serving skew can result from:</p> <ul> <li>Different preprocessing: Training and serving use different transforms</li> <li>Different features: Missing features in serving</li> <li>Data drift: Distribution changes over time</li> </ul> <p>Prevention with TensorFlow Transform: <pre><code># Same preprocessing for training and serving\nimport tensorflow_transform as tft\n\ndef preprocessing_fn(inputs):\n    \"\"\"Shared preprocessing function\"\"\"\n    outputs = {}\n\n    # Normalize (uses full-pass statistics)\n    outputs['normalized_feature'] = tft.scale_to_z_score(inputs['feature'])\n\n    # Vocabulary (computed once on training data)\n    outputs['categorical_encoded'] = tft.compute_and_apply_vocabulary(\n        inputs['category'],\n        top_k=1000\n    )\n\n    return outputs\n</code></pre></p> <p>Key Takeaways:</p> <ul> <li>TFT guarantees training-serving consistency</li> <li>Skew monitoring requires training data baseline</li> <li>Common causes: timezone differences, encoding issues, missing values</li> <li>Prevention better than detection</li> </ul>"},{"location":"gcp-aws-compare/","title":"GCP ML Services and AWS Comparables","text":"<p>Section Overview: This section maps all GCP ML services to their AWS equivalents, providing you with a comprehensive reference for service-to-service comparisons.</p> <p>Learning Objectives:</p> <ul> <li>Identify GCP ML services and their AWS counterparts</li> <li>Understand key architectural differences between GCP and AWS approaches</li> <li>Recognize when to use each service based on use case requirements</li> <li>Master key features and integration patterns</li> </ul>"},{"location":"gcp-aws-compare/#11-core-ml-services","title":"1.1 Core ML Services","text":""},{"location":"gcp-aws-compare/#vertex-ai-gcps-unified-ml-platform","title":"Vertex AI (GCP's Unified ML Platform)","text":"<p>AWS Equivalent: Amazon SageMaker</p> <p>Service Overview: Vertex AI is Google Cloud's unified, end-to-end ML platform that consolidates all ML services under one roof. It brings together what was previously AI Platform, AutoML, and other standalone services into a single cohesive experience.</p> <p>Key Components:</p> <ul> <li>Vertex AI Workbench: Managed Jupyter notebooks (AWS: SageMaker Studio/Notebook Instances)</li> <li>Vertex AI Training: Custom and distributed training (AWS: SageMaker Training Jobs)</li> <li>Vertex AI Prediction: Online and batch inference (AWS: SageMaker Endpoints/Batch Transform)</li> <li>Vertex AI Pipelines: ML workflow orchestration (AWS: SageMaker Pipelines)</li> <li>Vertex AI Feature Store: Centralized feature repository (AWS: SageMaker Feature Store)</li> <li>Vertex AI Model Monitoring: Drift and skew detection (AWS: SageMaker Model Monitor)</li> <li>Vertex AI Experiments: Tracking and comparison (AWS: SageMaker Experiments)</li> </ul> <p>Architectural Differences:</p> <p>When comparing GCP and AWS ML platforms, key differences include:</p> <ul> <li>GCP: Unified console and API surface across all ML tasks</li> <li>AWS: More modular approach with separate services for different ML workflows</li> <li>GCP: Strong integration with BigQuery for data processing</li> <li>AWS: Tight integration with S3 and broader AWS data ecosystem</li> </ul> <p>Key Features:</p> <ul> <li>Custom Training Jobs: Containerized training with support for TensorFlow, PyTorch, scikit-learn</li> <li>Hyperparameter Tuning: Automated hyperparameter optimization with configurable search strategies</li> <li>Distributed Training: Built-in support for data and model parallelism</li> <li>Model Registry: Version control and deployment management</li> <li>Metadata Management: Automatic tracking of artifacts, lineage, and experiments</li> </ul> <p>Common Use Cases:</p> <p>Vertex AI is ideal for these production ML scenarios:</p> <ul> <li>End-to-end ML workflows from data preparation to deployment</li> <li>Large-scale distributed training on TPUs or GPUs</li> <li>Production model serving with auto-scaling</li> <li>MLOps pipelines with continuous training and deployment</li> </ul> <p>Integration Points:</p> <ul> <li>BigQuery for data warehousing and feature engineering</li> <li>Cloud Storage for dataset and model artifact storage</li> <li>Cloud Build for CI/CD integration</li> <li>Cloud Monitoring for observability</li> </ul> <p>Terminology Mapping:</p> GCP Term AWS Term Description Training Job Training Job Model training execution Endpoint Endpoint Deployed model for predictions Model Model Trained ML model artifact Pipeline Pipeline ML workflow orchestration Custom Container BYOC User-provided container images <p>Practical Example - Training a Custom Model:</p> <p>GCP (Vertex AI): <pre><code>from google.cloud import aiplatform\n\naiplatform.init(project='my-project', location='us-central1')\n\n# Define custom training job\njob = aiplatform.CustomTrainingJob(\n    display_name='fraud-detection-training',\n    container_uri='gcr.io/my-project/fraud-model:latest',\n    requirements=['pandas', 'scikit-learn'],\n    model_serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest'\n)\n\n# Execute training\nmodel = job.run(\n    dataset=dataset,\n    model_display_name='fraud-detector-v1',\n    args=['--epochs=100', '--batch-size=32'],\n    replica_count=1,\n    machine_type='n1-standard-4',\n    accelerator_type='NVIDIA_TESLA_T4',\n    accelerator_count=1\n)\n</code></pre></p> <p>AWS (SageMaker): <pre><code>import sagemaker\nfrom sagemaker.estimator import Estimator\n\nsession = sagemaker.Session()\nrole = 'arn:aws:iam::123456789:role/SageMakerRole'\n\n# Define training job\nestimator = Estimator(\n    image_uri='123456789.dkr.ecr.us-east-1.amazonaws.com/fraud-model:latest',\n    role=role,\n    instance_count=1,\n    instance_type='ml.p3.2xlarge',\n    output_path='s3://my-bucket/models/',\n    sagemaker_session=session\n)\n\n# Execute training\nestimator.fit({'training': 's3://my-bucket/data/'})\n</code></pre></p> <p>Key Differences:</p> <p>Comparing the two implementations reveals important API design differences:</p> <ul> <li>GCP uses container_uri vs AWS uses image_uri</li> <li>GCP specifies accelerator separately vs AWS includes GPU in instance type</li> <li>GCP has unified init vs AWS requires session and role management</li> <li>GCP integrates model serving container at training time</li> </ul>"},{"location":"gcp-aws-compare/#ai-platform-legacy--deprecated","title":"AI Platform (Legacy) \u26a0\ufe0f DEPRECATED","text":"<p>Status: Discontinued January 31, 2025. All functionality migrated to Vertex AI.</p> <p>Migration Path: All AI Platform features are now available in Vertex AI. AI Platform is the predecessor to Vertex AI.</p> <p>Why This Matters: You may encounter migration scenarios where knowledge of both AI Platform and Vertex AI is relevant.</p>"},{"location":"gcp-aws-compare/#automl-vertex-ai-automl","title":"AutoML (Vertex AI AutoML)","text":"<p>AWS Equivalent: SageMaker Autopilot, SageMaker Canvas</p> <p>Service Overview: AutoML on Vertex AI provides automated machine learning capabilities for tabular, image, text, and video data. It automates feature engineering, model selection, hyperparameter tuning, and deployment with minimal code.</p> <p>Supported Data Types:</p> <p>AutoML supports four primary data modalities:</p> <ul> <li>AutoML Tables: Structured/tabular data (classification, regression)</li> <li>AutoML Vision: Image classification, object detection</li> <li>AutoML Natural Language: Text classification, entity extraction, sentiment analysis</li> <li>AutoML Video: Video classification, object tracking</li> </ul> <p>Key Features:</p> <p>AutoML provides these automation capabilities:</p> <ul> <li>Automatic data preprocessing and feature engineering</li> <li>Neural architecture search for optimal model design</li> <li>Built-in model explainability</li> <li>One-click deployment to production</li> </ul> <p>When to Use AutoML vs Custom Training:</p> <p>Use AutoML When:</p> <ul> <li>Quick proof-of-concept needed</li> <li>Limited ML expertise on the team</li> <li>Standard use cases (classification, regression, common CV/NLP tasks)</li> <li>Want to establish baseline model performance</li> <li>Dataset is well-structured and labeled</li> </ul> <p>Use Custom Training When:</p> <ul> <li>Specialized algorithms or architectures required</li> <li>Need fine-grained control over training process</li> <li>Custom loss functions or metrics needed</li> <li>Unusual data formats or preprocessing requirements</li> <li>Cost optimization through custom resource allocation</li> </ul> <p>AWS Comparison:</p> Feature Vertex AI AutoML SageMaker Autopilot SageMaker Canvas Target Users Data scientists &amp; developers Data scientists Business analysts Data Types Tabular, image, text, video Tabular only Tabular, image, text Code Required Minimal Python Python API No-code UI Explainability Built-in Built-in Built-in Deployment One-click API-based One-click <p>Practical Example - AutoML Tables:</p> <p>GCP: <pre><code>from google.cloud import aiplatform\n\n# Create dataset\ndataset = aiplatform.TabularDataset.create(\n    display_name='customer-churn',\n    gcs_source='gs://my-bucket/churn-data.csv'\n)\n\n# Train AutoML model\njob = aiplatform.AutoMLTabularTrainingJob(\n    display_name='churn-prediction',\n    optimization_prediction_type='classification',\n    optimization_objective='maximize-au-prc'\n)\n\nmodel = job.run(\n    dataset=dataset,\n    target_column='churned',\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    budget_milli_node_hours=1000,  # 1 hour\n    model_display_name='churn-model-v1'\n)\n</code></pre></p> <p>AWS (Autopilot): <pre><code>import sagemaker\nfrom sagemaker import AutoML\n\nsession = sagemaker.Session()\nautoml = AutoML(\n    role='arn:aws:iam::123:role/SageMakerRole',\n    target_attribute_name='churned',\n    output_path='s3://my-bucket/autopilot-output/',\n    sagemaker_session=session\n)\n\nautoml.fit(\n    inputs='s3://my-bucket/churn-data.csv',\n    job_name='churn-prediction',\n    wait=False\n)\n</code></pre></p>"},{"location":"gcp-aws-compare/#bigquery-ml","title":"BigQuery ML","text":"<p>AWS Equivalent: Amazon Redshift ML, Amazon Athena ML</p> <p>Service Overview: BigQuery ML enables data analysts to create and execute machine learning models using SQL queries directly within BigQuery. No need to export data or use separate ML tools.</p> <p>Supported Model Types:</p> <p>BigQuery ML supports a wide range of algorithms for different tasks:</p> <ul> <li>Linear Regression: Numeric prediction</li> <li>Logistic Regression: Binary/multiclass classification</li> <li>K-Means Clustering: Unsupervised grouping</li> <li>Matrix Factorization: Recommendation systems</li> <li>Time Series (ARIMA_PLUS): Forecasting</li> <li>Boosted Trees (XGBoost): Classification and regression</li> <li>Deep Neural Networks (DNN): Complex patterns</li> <li>AutoML Tables: Automated model selection</li> <li>Imported TensorFlow/ONNX models: Use pre-trained models</li> </ul> <p>Key Advantages:</p> <p>BigQuery ML offers significant benefits for data analysts:</p> <ul> <li>SQL-based: No Python/ML expertise required</li> <li>Data stays in place: No data movement overhead</li> <li>Scalable: Leverages BigQuery's processing power</li> <li>Fast iteration: Quick model training and evaluation</li> </ul> <p>Common Use Cases:</p> <p>Typical applications of BigQuery ML include:</p> <ul> <li>Customer segmentation with K-Means</li> <li>Churn prediction with classification models</li> <li>Product recommendations with matrix factorization</li> <li>Demand forecasting with ARIMA_PLUS</li> <li>Anomaly detection with clustering</li> </ul> <p>Practical Example - Customer Churn Prediction:</p> <p>GCP (BigQuery ML): <pre><code>-- Create and train model\nCREATE OR REPLACE MODEL `mydataset.churn_model`\nOPTIONS(\n  model_type='LOGISTIC_REG',\n  input_label_cols=['churned'],\n  auto_class_weights=TRUE,\n  data_split_method='AUTO_SPLIT',\n  max_iterations=50\n) AS\nSELECT\n  customer_age,\n  tenure_months,\n  monthly_charges,\n  total_charges,\n  contract_type,\n  payment_method,\n  churned\nFROM `mydataset.customer_data`;\n\n-- Evaluate model\nSELECT *\nFROM ML.EVALUATE(MODEL `mydataset.churn_model`);\n\n-- Make predictions\nSELECT\n  customer_id,\n  predicted_churned,\n  predicted_churned_probs[OFFSET(1)].prob as churn_probability\nFROM ML.PREDICT(MODEL `mydataset.churn_model`,\n  TABLE `mydataset.new_customers`\n);\n\n-- Feature importance\nSELECT *\nFROM ML.FEATURE_IMPORTANCE(MODEL `mydataset.churn_model`);\n</code></pre></p> <p>AWS (Redshift ML): <pre><code>-- Create and train model\nCREATE MODEL churn_model\nFROM customer_data\nTARGET churned\nFUNCTION predict_churn\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftMLRole'\nSETTINGS (\n  S3_BUCKET 'my-bucket',\n  MAX_RUNTIME 5000\n);\n\n-- Make predictions\nSELECT\n  customer_id,\n  predict_churn(\n    customer_age,\n    tenure_months,\n    monthly_charges,\n    total_charges,\n    contract_type,\n    payment_method\n  ) as predicted_churn\nFROM new_customers;\n</code></pre></p> <p>Key Differences:</p> <p>When comparing BigQuery ML to Redshift ML:</p> <ul> <li>BigQuery ML has richer model options (ARIMA, Matrix Factorization, DNN)</li> <li>BigQuery ML provides built-in model evaluation and feature importance</li> <li>Redshift ML relies more on SageMaker Autopilot in the background</li> <li>BigQuery ML syntax is more ML-focused, Redshift ML simpler</li> </ul> <p>Key Consideration: Choose BigQuery ML for data analysts, SQL-based teams, and data already in BigQuery. Choose Vertex AI for complex ML workflows, Python-based teams, and when you need MLOps capabilities.</p>"},{"location":"gcp-aws-compare/#12-pre-trained-ai-apis","title":"1.2 Pre-trained AI APIs","text":""},{"location":"gcp-aws-compare/#vision-ai","title":"Vision AI","text":"<p>AWS Equivalent: Amazon Rekognition</p> <p>Capabilities:</p> <p>Vision AI provides these pre-trained image analysis features:</p> <ul> <li>Image labeling and classification</li> <li>Face detection and recognition</li> <li>Optical Character Recognition (OCR)</li> <li>Object detection and tracking</li> <li>Explicit content detection</li> <li>Logo detection</li> <li>Landmark recognition</li> </ul> <p>When to Use: Need quick image analysis without training custom models</p>"},{"location":"gcp-aws-compare/#natural-language-ai","title":"Natural Language AI","text":"<p>AWS Equivalent: Amazon Comprehend</p> <p>Capabilities:</p> <p>Natural Language AI offers these text analysis capabilities:</p> <ul> <li>Entity extraction (people, places, organizations)</li> <li>Sentiment analysis</li> <li>Syntax analysis</li> <li>Content classification</li> <li>Entity sentiment</li> </ul> <p>When to Use: Text analysis tasks with standard requirements</p>"},{"location":"gcp-aws-compare/#translation-api","title":"Translation API","text":"<p>AWS Equivalent: Amazon Translate</p> <p>Capabilities:</p> <p>Translation API supports:</p> <ul> <li>Text translation across 100+ languages</li> <li>Batch and real-time translation</li> <li>Custom glossaries</li> </ul>"},{"location":"gcp-aws-compare/#speech-to-text","title":"Speech-to-Text","text":"<p>AWS Equivalent: Amazon Transcribe</p> <p>Capabilities:</p> <p>Speech-to-Text provides:</p> <ul> <li>Audio transcription</li> <li>Real-time streaming recognition</li> <li>Speaker diarization</li> <li>Profanity filtering</li> <li>Custom vocabulary</li> </ul>"},{"location":"gcp-aws-compare/#text-to-speech","title":"Text-to-Speech","text":"<p>AWS Equivalent: Amazon Polly</p> <p>Capabilities:</p> <p>Text-to-Speech enables:</p> <ul> <li>Natural-sounding speech synthesis</li> <li>Multiple voices and languages</li> <li>Custom voice creation (in preview)</li> <li>SSML support</li> </ul>"},{"location":"gcp-aws-compare/#13-data-processing--storage-services","title":"1.3 Data Processing &amp; Storage Services","text":""},{"location":"gcp-aws-compare/#bigquery","title":"BigQuery","text":"<p>AWS Equivalent: Amazon Redshift, Amazon Athena</p> <p>Service Overview: BigQuery is Google's fully managed, serverless, petabyte-scale data warehouse designed for analytics and ML workloads. It's central to most GCP ML architectures.</p> <p>Key Features:</p> <p>BigQuery provides these capabilities for ML workloads:</p> <ul> <li>Serverless: No infrastructure management</li> <li>Fast queries: Parallel processing across thousands of nodes</li> <li>Streaming inserts: Real-time data ingestion</li> <li>ML integration: Native BigQuery ML support</li> <li>Cost-effective: Pay only for queries run and storage used</li> </ul> <p>Architectural Differences from Redshift:</p> <p>Comparing BigQuery to Redshift reveals fundamental design differences:</p> <ul> <li>GCP (BigQuery): Fully serverless, automatic scaling</li> <li>AWS (Redshift): Cluster-based, manual scaling required</li> <li>GCP: Separates storage and compute billing</li> <li>AWS: Combined cluster pricing</li> </ul> <p>Key Features:</p> <ul> <li>Partitioning: Table partitioning by date/timestamp for query optimization</li> <li>Clustering: Group related data together for better performance</li> <li>Materialized Views: Pre-computed query results for faster access</li> <li>Federated Queries: Query external data sources (Cloud Storage, Cloud SQL)</li> <li>Data Transfer Service: Automated data imports from SaaS applications</li> </ul> <p>Best Practices for ML Workloads:</p> <ul> <li>Partition training data by date for efficient querying</li> <li>Use clustering on high-cardinality columns used in WHERE clauses</li> <li>Create materialized views for frequently used feature engineering queries</li> <li>Leverage slot reservations for predictable ML pipeline costs</li> </ul> <p>Practical Example - Feature Engineering: <pre><code>-- Create partitioned table for ML features\nCREATE OR REPLACE TABLE `project.dataset.customer_features`\nPARTITION BY DATE(feature_timestamp)\nCLUSTER BY customer_id\nAS\nSELECT\n  customer_id,\n  feature_timestamp,\n  -- Aggregated features\n  AVG(transaction_amount) OVER (\n    PARTITION BY customer_id\n    ORDER BY feature_timestamp\n    ROWS BETWEEN 30 PRECEDING AND CURRENT ROW\n  ) as avg_30day_spend,\n  COUNT(*) OVER (\n    PARTITION BY customer_id\n    ORDER BY feature_timestamp\n    RANGE BETWEEN INTERVAL 7 DAY PRECEDING AND CURRENT ROW\n  ) as transactions_last_7days,\n  -- One-hot encoding\n  IF(customer_segment = 'premium', 1, 0) as is_premium\nFROM `project.dataset.transactions`;\n\n-- Create materialized view for common features\nCREATE MATERIALIZED VIEW `project.dataset.daily_customer_stats`\nAS\nSELECT\n  DATE(transaction_time) as date,\n  customer_id,\n  COUNT(*) as transaction_count,\n  SUM(amount) as total_spend,\n  AVG(amount) as avg_transaction\nFROM `project.dataset.transactions`\nGROUP BY date, customer_id;\n</code></pre></p>"},{"location":"gcp-aws-compare/#cloud-storage","title":"Cloud Storage","text":"<p>AWS Equivalent: Amazon S3</p> <p>Service Overview: Cloud Storage is Google's object storage service for unstructured data. It's the primary storage for datasets, model artifacts, and pipeline outputs.</p> <p>Storage Classes:</p> <p>Cloud Storage offers four storage tiers optimized for different access patterns:</p> <ul> <li>Standard: Frequently accessed data (AWS: S3 Standard)</li> <li>Nearline: Monthly access (AWS: S3 Standard-IA)</li> <li>Coldline: Quarterly access (AWS: S3 Glacier Instant Retrieval)</li> <li>Archive: Annual access (AWS: S3 Glacier Deep Archive)</li> </ul> <p>Key Features for ML:</p> <p>Cloud Storage provides these ML-specific features:</p> <ul> <li>Autoclass: Automatic lifecycle management</li> <li>Uniform bucket-level access: Simplified IAM</li> <li>Object versioning: Track model artifact versions</li> <li>Signed URLs: Temporary access for secure data sharing</li> </ul> <p>Best Practices:</p> <ul> <li>Use Standard class for training datasets and active models</li> <li>Use Nearline/Coldline for archived experiments and old model versions</li> <li>Organize with consistent naming: gs://bucket/datasets/{train,val,test}/</li> <li>Enable versioning for model artifacts</li> </ul> <p>Example - Organizing ML Assets: <pre><code>gs://ml-project-bucket/\n\u251c\u2500\u2500 datasets/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u2502   \u2514\u2500\u2500 data-2024-01-15.csv\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 validation/\n\u2502   \u2502   \u2514\u2500\u2500 test/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 fraud-detector/\n\u2502   \u2502   \u251c\u2500\u2500 v1/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model.pkl\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 metadata.json\n\u2502   \u2502   \u2514\u2500\u2500 v2/\n\u2502   \u2514\u2500\u2500 churn-predictor/\n\u251c\u2500\u2500 experiments/\n\u2502   \u2514\u2500\u2500 experiment-2024-01-15/\n\u2502       \u251c\u2500\u2500 hyperparameters.json\n\u2502       \u2514\u2500\u2500 metrics.json\n\u2514\u2500\u2500 pipelines/\n    \u2514\u2500\u2500 training-pipeline-v1/\n</code></pre></p>"},{"location":"gcp-aws-compare/#dataflow","title":"Dataflow","text":"<p>AWS Equivalent: AWS Glue (batch), Amazon Kinesis Data Analytics/Apache Flink (streaming)</p> <p>Service Overview: Dataflow is Google's fully managed service for stream and batch data processing based on Apache Beam. It's crucial for data preprocessing, feature engineering, and ETL in ML pipelines.</p> <p>Key Capabilities:</p> <p>Dataflow provides these powerful features:</p> <ul> <li>Unified programming: Same code for batch and streaming</li> <li>Auto-scaling: Dynamically adjust workers based on data volume</li> <li>Exactly-once processing: Guarantees for streaming data</li> <li>Windowing: Time-based aggregations for streaming data</li> </ul> <p>Common ML Use Cases:</p> <p>Dataflow is ideal for these ML scenarios:</p> <ul> <li>Large-scale data preprocessing before training</li> <li>Real-time feature computation from streaming events</li> <li>Batch prediction result processing</li> <li>Data validation with TensorFlow Data Validation (TFDV)</li> <li>Feature transformation with TensorFlow Transform (TFT)</li> </ul> <p>Architectural Pattern - Streaming Feature Engineering: <pre><code>Pub/Sub \u2192 Dataflow \u2192 Feature Store (online)\n                  \u2192 BigQuery (offline)\n</code></pre></p> <p>Practical Example - Feature Engineering Pipeline: <pre><code>import apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef compute_features(element):\n    \"\"\"Compute features from raw event\"\"\"\n    return {\n        'user_id': element['user_id'],\n        'avg_session_duration': element['session_duration'] / element['page_views'],\n        'days_since_last_visit': (datetime.now() - element['last_visit']).days,\n        'total_purchases': element['purchase_count']\n    }\n\n# Define pipeline\noptions = PipelineOptions(\n    runner='DataflowRunner',\n    project='my-project',\n    region='us-central1',\n    temp_location='gs://my-bucket/temp'\n)\n\nwith beam.Pipeline(options=options) as pipeline:\n    (pipeline\n     | 'Read from Pub/Sub' &gt;&gt; beam.io.ReadFromPubSub(\n         subscription='projects/my-project/subscriptions/user-events')\n     | 'Parse JSON' &gt;&gt; beam.Map(json.loads)\n     | 'Compute Features' &gt;&gt; beam.Map(compute_features)\n     | 'Write to BigQuery' &gt;&gt; beam.io.WriteToBigQuery(\n         'my-project:dataset.user_features',\n         schema='user_id:STRING,avg_session_duration:FLOAT,...',\n         write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n     ))\n</code></pre></p>"},{"location":"gcp-aws-compare/#14-compute-services","title":"1.4 Compute Services","text":""},{"location":"gcp-aws-compare/#compute-engine","title":"Compute Engine","text":"<p>AWS Equivalent: Amazon EC2</p> <p>Use for ML:</p> <p>Compute Engine is suitable for these ML scenarios:</p> <ul> <li>Custom training environments with specific configurations</li> <li>Long-running training jobs requiring manual management</li> <li>Legacy ML code migration from on-premises</li> </ul> <p>When to Use vs Vertex AI Training:</p> <ul> <li>Use Compute Engine: Maximum control, custom OS/software, non-standard ML frameworks</li> <li>Use Vertex AI Training: Managed ML workflows, easier scaling, built-in integrations</li> </ul>"},{"location":"gcp-aws-compare/#google-kubernetes-engine-gke","title":"Google Kubernetes Engine (GKE)","text":"<p>AWS Equivalent: Amazon EKS</p> <p>Use for ML:</p> <p>GKE excels at these ML use cases:</p> <ul> <li>Deploying custom model serving infrastructure</li> <li>Running Kubeflow for ML pipelines</li> <li>Multi-tenant ML platforms</li> <li>Batch inference jobs with complex orchestration</li> </ul>"},{"location":"gcp-aws-compare/#cloud-run","title":"Cloud Run","text":"<p>AWS Equivalent: AWS Fargate, AWS App Runner</p> <p>Use for ML:</p> <p>Cloud Run is ideal for:</p> <ul> <li>Serverless model serving</li> <li>Lightweight prediction APIs</li> <li>Batch inference jobs triggered by events</li> <li>Cost-effective for sporadic prediction workloads</li> </ul>"},{"location":"gcp-aws-compare/#15-mlops--development-tools","title":"1.5 MLOps &amp; Development Tools","text":""},{"location":"gcp-aws-compare/#cloud-build","title":"Cloud Build","text":"<p>AWS Equivalent: AWS CodeBuild</p> <p>Use for ML:</p> <p>Cloud Build supports these ML workflows:</p> <ul> <li>Building custom training containers</li> <li>CI/CD for ML code</li> <li>Automated model retraining pipelines</li> <li>Container image creation for Vertex AI</li> </ul>"},{"location":"gcp-aws-compare/#artifact-registry","title":"Artifact Registry","text":"<p>AWS Equivalent: Amazon ECR</p> <p>Use for ML:</p> <p>Artifact Registry provides:</p> <ul> <li>Store Docker images for training and serving</li> <li>Manage ML model artifacts</li> <li>Version control for containers</li> <li>Vulnerability scanning for security</li> </ul>"},{"location":"gcp-aws-compare/#16-monitoring--management","title":"1.6 Monitoring &amp; Management","text":""},{"location":"gcp-aws-compare/#cloud-monitoring-formerly-stackdriver","title":"Cloud Monitoring (formerly Stackdriver)","text":"<p>AWS Equivalent: Amazon CloudWatch</p> <p>Key Metrics for ML:</p> <p>Monitor these critical ML metrics:</p> <ul> <li>Training job progress and resource utilization</li> <li>Prediction latency and throughput</li> <li>Model endpoint health</li> <li>Custom metrics (model accuracy, drift scores)</li> </ul>"},{"location":"gcp-aws-compare/#cloud-logging","title":"Cloud Logging","text":"<p>AWS Equivalent: Amazon CloudWatch Logs</p> <p>ML Logging Best Practices:</p> <p>Follow these logging practices for ML systems:</p> <ul> <li>Log training hyperparameters and final metrics</li> <li>Capture prediction requests for debugging</li> <li>Track data quality issues</li> <li>Monitor feature values for anomalies</li> </ul>"},{"location":"gcp-aws-compare/#17-security--governance","title":"1.7 Security &amp; Governance","text":""},{"location":"gcp-aws-compare/#iam-identity-and-access-management","title":"IAM (Identity and Access Management)","text":"<p>AWS Equivalent: AWS IAM</p> <p>Key Roles for ML:</p> <ul> <li>roles/aiplatform.user: Use Vertex AI resources</li> <li>roles/aiplatform.admin: Manage Vertex AI resources</li> <li>roles/bigquery.dataViewer: Read BigQuery data</li> <li>roles/bigquery.jobUser: Run BigQuery jobs</li> <li>roles/storage.objectViewer: Read Cloud Storage objects</li> <li>roles/ml.developer: Full ML development access</li> </ul>"},{"location":"gcp-aws-compare/#vpc-service-controls","title":"VPC Service Controls","text":"<p>AWS Equivalent: AWS VPC Endpoints, AWS PrivateLink</p> <p>Purpose: Create security perimeters around GCP resources to prevent data exfiltration</p> <p>ML Use Cases:</p> <p>VPC Service Controls protect ML resources:</p> <ul> <li>Protect sensitive training data in BigQuery</li> <li>Secure model artifacts in Cloud Storage</li> <li>Isolate Vertex AI resources within network perimeter</li> </ul>"},{"location":"gcp-aws-compare/#cloud-kms-key-management-service","title":"Cloud KMS (Key Management Service)","text":"<p>AWS Equivalent: AWS KMS</p> <p>Use for ML:</p> <p>Cloud KMS secures ML data and artifacts:</p> <ul> <li>Encrypt training data at rest</li> <li>Protect model artifacts</li> <li>Encrypt BigQuery datasets</li> <li>Secure sensitive feature data</li> </ul>"},{"location":"glossary/","title":"GCP ML Terminology Glossary","text":"<p>A comprehensive reference guide to key terms, services, and acronyms used in Google Cloud Machine Learning, with AWS equivalents where applicable.</p>"},{"location":"glossary/#table-of-contents","title":"Table of Contents","text":"<ul> <li>A | B | C | D | F | I | K | M | P | R | S | T | V</li> </ul>"},{"location":"glossary/#a","title":"A","text":""},{"location":"glossary/#automl","title":"AutoML","text":"Attribute Details Description Automated machine learning service that automates model selection, hyperparameter tuning, and deployment AWS Equivalent SageMaker Autopilot Use Case Quick ML development without deep ML expertise"},{"location":"glossary/#auc-area-under-curve","title":"AUC (Area Under Curve)","text":"Attribute Details Description Performance metric measuring the area under the ROC curve Range 0.0 to 1.0 (higher is better) Use Case Evaluating binary classification models"},{"location":"glossary/#b","title":"B","text":""},{"location":"glossary/#bigquery-ml","title":"BigQuery ML","text":"Attribute Details Description Create and execute ML models using SQL directly in BigQuery AWS Equivalent Redshift ML Key Features SQL-based model training, no data movement required"},{"location":"glossary/#c","title":"C","text":""},{"location":"glossary/#cmek-customer-managed-encryption-keys","title":"CMEK (Customer-Managed Encryption Keys)","text":"Attribute Details Description User-controlled encryption keys for data security AWS Equivalent Customer managed keys in KMS Use Case Regulatory compliance, data sovereignty"},{"location":"glossary/#ct-continuous-training","title":"CT (Continuous Training)","text":"Attribute Details Description Automated retraining of models based on triggers or schedules Components Data monitoring, training pipeline, deployment automation"},{"location":"glossary/#d","title":"D","text":""},{"location":"glossary/#dag-directed-acyclic-graph","title":"DAG (Directed Acyclic Graph)","text":"Attribute Details Description Graph structure representing ML pipeline dependencies Use Case Workflow orchestration in Vertex AI Pipelines, Kubeflow"},{"location":"glossary/#dataflow","title":"Dataflow","text":"Attribute Details Description Managed service for stream and batch data processing based on Apache Beam AWS Equivalent AWS Glue (batch), Kinesis Analytics (stream) Key Features Unified programming model, auto-scaling, windowing"},{"location":"glossary/#dnn-deep-neural-network","title":"DNN (Deep Neural Network)","text":"Attribute Details Description Neural network with multiple hidden layers Use Case Complex pattern recognition, computer vision, NLP"},{"location":"glossary/#f","title":"F","text":""},{"location":"glossary/#feature-store","title":"Feature Store","text":"Attribute Details Description Centralized repository for storing, serving, and managing ML features AWS Equivalent SageMaker Feature Store Components Online store (real-time), Offline store (training)"},{"location":"glossary/#i","title":"I","text":""},{"location":"glossary/#iam-identity-and-access-management","title":"IAM (Identity and Access Management)","text":"Attribute Details Description Access control service for GCP resources AWS Equivalent AWS IAM Key Roles aiplatform.user, aiplatform.admin, bigquery.dataViewer"},{"location":"glossary/#k","title":"K","text":""},{"location":"glossary/#kfp-kubeflow-pipelines","title":"KFP (Kubeflow Pipelines)","text":"Attribute Details Description ML workflow framework used in Vertex AI Pipelines AWS Equivalent SageMaker Pipelines (different framework) Components Pipeline definition, components, artifacts"},{"location":"glossary/#kms-key-management-service","title":"KMS (Key Management Service)","text":"Attribute Details Description Service for creating and managing cryptographic keys AWS Equivalent AWS KMS Use Case Encryption key management, CMEK implementation"},{"location":"glossary/#m","title":"M","text":""},{"location":"glossary/#mae-mean-absolute-error","title":"MAE (Mean Absolute Error)","text":"Attribute Details Description Average of absolute differences between predictions and actual values Formula MAE = (1/n) \u00d7 \u03a3|predicted - actual| Use Case Regression model evaluation"},{"location":"glossary/#mse-mean-squared-error","title":"MSE (Mean Squared Error)","text":"Attribute Details Description Average of squared differences between predictions and actual values Formula MSE = (1/n) \u00d7 \u03a3(predicted - actual)\u00b2 Use Case Regression model evaluation, sensitive to outliers"},{"location":"glossary/#mwaa-managed-workflows-for-apache-airflow","title":"MWAA (Managed Workflows for Apache Airflow)","text":"Attribute Details Description AWS managed Apache Airflow service GCP Equivalent Cloud Composer Use Case Workflow orchestration, ETL pipelines"},{"location":"glossary/#p","title":"P","text":""},{"location":"glossary/#pubsub","title":"Pub/Sub","text":"Attribute Details Description Messaging service for event ingestion and distribution AWS Equivalent Amazon Kinesis, SNS/SQS Key Features At-least-once delivery, ordering, topic-based routing"},{"location":"glossary/#r","title":"R","text":""},{"location":"glossary/#rmse-root-mean-squared-error","title":"RMSE (Root Mean Squared Error)","text":"Attribute Details Description Square root of MSE, in same units as target variable Formula RMSE = \u221aMSE Use Case Regression model evaluation"},{"location":"glossary/#roc-receiver-operating-characteristic","title":"ROC (Receiver Operating Characteristic)","text":"Attribute Details Description Plot of true positive rate vs false positive rate Use Case Visualizing classification performance at different thresholds"},{"location":"glossary/#s","title":"S","text":""},{"location":"glossary/#shap-shapley-additive-explanations","title":"SHAP (SHapley Additive exPlanations)","text":"Attribute Details Description Method for explaining individual predictions Use Case Model interpretability, feature importance"},{"location":"glossary/#smote-synthetic-minority-over-sampling-technique","title":"SMOTE (Synthetic Minority Over-sampling Technique)","text":"Attribute Details Description Technique for handling imbalanced datasets by generating synthetic examples Use Case Improving minority class representation"},{"location":"glossary/#t","title":"T","text":""},{"location":"glossary/#tabular-workflows","title":"Tabular Workflows","text":"Attribute Details Description Enhanced AutoML with granular pipeline control AWS Equivalent Custom SageMaker Pipelines Use Case Structured data ML with customization needs"},{"location":"glossary/#tft-tensorflow-transform","title":"TFT (TensorFlow Transform)","text":"Attribute Details Description Library for preprocessing data in TensorFlow Key Feature Ensures training-serving consistency Use Case Feature engineering, preprocessing pipelines"},{"location":"glossary/#tfdv-tensorflow-data-validation","title":"TFDV (TensorFlow Data Validation)","text":"Attribute Details Description Library for exploring and validating ML data Capabilities Schema inference, anomaly detection, drift detection"},{"location":"glossary/#tfx-tensorflow-extended","title":"TFX (TensorFlow Extended)","text":"Attribute Details Description End-to-end platform for deploying production ML pipelines Components Data validation, transform, training, serving"},{"location":"glossary/#tpu-tensor-processing-unit","title":"TPU (Tensor Processing Unit)","text":"Attribute Details Description Google's custom chip optimized for ML workloads (GCP exclusive) AWS Equivalent AWS Trainium/Inferentia (similar concept) Best For Large-scale training, matrix operations, TensorFlow models"},{"location":"glossary/#v","title":"V","text":""},{"location":"glossary/#vertex-ai","title":"Vertex AI","text":"Attribute Details Description Unified ML platform consolidating training, deployment, and monitoring AWS Equivalent Amazon SageMaker Key Components Workbench, Training, Prediction, Pipelines, Feature Store"},{"location":"glossary/#vpc-virtual-private-cloud","title":"VPC (Virtual Private Cloud)","text":"Attribute Details Description Isolated network environment for GCP resources AWS Equivalent Amazon VPC Use Case Network security, resource isolation"},{"location":"glossary/#vpc-sc-vpc-service-controls","title":"VPC-SC (VPC Service Controls)","text":"Attribute Details Description Security perimeter for GCP resources to prevent data exfiltration AWS Equivalent VPC Endpoints, PrivateLink Use Case Data security, compliance requirements <p>This glossary provides essential terminology for Google Cloud ML certification preparation, with AWS comparisons to leverage existing cloud knowledge.</p>"},{"location":"labs/","title":"Hands-On Lab Recommendations","text":""},{"location":"labs/#lab-1-end-to-end-automl-pipeline","title":"Lab 1: End-to-End AutoML Pipeline","text":"<p>Objective: Build complete pipeline from data to deployed model</p> <p>Steps:</p> <ol> <li>Upload dataset to Cloud Storage</li> <li>Create Vertex AI Dataset</li> <li>Train AutoML model</li> <li>Evaluate model performance</li> <li>Deploy to endpoint</li> <li>Make online predictions</li> <li>Run batch predictions</li> </ol> <p>AWS Comparison: Build same pipeline in SageMaker</p>"},{"location":"labs/#lab-2-custom-training-with-tensorflow","title":"Lab 2: Custom Training with TensorFlow","text":"<p>Objective: Train custom model on Vertex AI</p> <p>Steps:</p> <ol> <li>Write TensorFlow training script</li> <li>Create Docker container</li> <li>Push to Artifact Registry</li> <li>Create Custom Training Job</li> <li>Monitor training with Cloud Logging</li> <li>Export model to Cloud Storage</li> <li>Deploy and serve</li> </ol> <p>AWS Comparison: SageMaker Training Job with BYOC</p>"},{"location":"labs/#lab-3-streaming-ml-pipeline","title":"Lab 3: Streaming ML Pipeline","text":"<p>Objective: Real-time feature engineering and prediction</p> <p>Steps:</p> <ol> <li>Set up Pub/Sub topic</li> <li>Create Dataflow streaming pipeline</li> <li>Write to Feature Store (online)</li> <li>Call Vertex AI Endpoint for predictions</li> <li>Write results to BigQuery</li> <li>Monitor pipeline</li> </ol> <p>AWS Comparison: Kinesis + Flink + SageMaker</p>"},{"location":"labs/#lab-4-mlops-with-vertex-ai-pipelines","title":"Lab 4: MLOps with Vertex AI Pipelines","text":"<p>Objective: Build CI/CD/CT pipeline</p> <p>Steps:</p> <ol> <li>Write KFP pipeline definition</li> <li>Set up Cloud Build trigger</li> <li>Configure automated testing</li> <li>Deploy pipeline on code commit</li> <li>Set up model monitoring</li> <li>Create retraining trigger</li> </ol> <p>AWS Comparison: SageMaker Pipelines with CodePipeline</p>"},{"location":"labs/#final-preparation-checklist","title":"Final Preparation Checklist","text":""},{"location":"labs/#-services-understanding","title":"\u2705 Services Understanding","text":"<ul> <li> Know all Vertex AI components</li> <li> Understand BigQuery ML capabilities</li> <li> Know when to use AutoML vs custom training</li> <li> Understand data processing services (Dataflow, Dataproc)</li> </ul>"},{"location":"labs/#-architectural-patterns","title":"\u2705 Architectural Patterns","text":"<ul> <li> Real-time prediction architecture</li> <li> Batch prediction pipeline</li> <li> Streaming ML pipeline</li> <li> MLOps/CI/CD patterns</li> </ul>"},{"location":"labs/#-technical-deep-dives","title":"\u2705 Technical Deep-Dives","text":"<ul> <li> Hyperparameter tuning strategies</li> <li> Distributed training (data vs model parallelism)</li> <li> Feature Store (online vs offline serving)</li> <li> Model monitoring (skew vs drift)</li> </ul>"},{"location":"labs/#-best-practices","title":"\u2705 Best Practices","text":"<ul> <li> Training-serving consistency (TFT)</li> <li> Security (CMEK, VPC-SC, IAM)</li> <li> Cost optimization (preemptible VMs, batch vs online)</li> <li> Performance optimization (TPUs, caching)</li> </ul>"},{"location":"labs/#-aws-comparisons","title":"\u2705 AWS Comparisons","text":"<ul> <li> Know equivalent services for all major GCP services</li> <li> Understand architectural differences</li> <li> Know unique GCP features (TPUs, BigQuery ML, Reduction Server)</li> </ul> <p>Good luck with your Google Cloud Professional Machine Learning Engineer certification!</p>"},{"location":"patterns/","title":"Architectural Patterns and Design Principles","text":"<p>Section Overview: This section covers common ML architecture patterns, with detailed implementation guidance for both GCP and AWS.</p> <p>Learning Objectives:</p> <ul> <li>Recognize standard ML architecture patterns</li> <li>Choose appropriate services for different use cases</li> <li>Design scalable and cost-effective solutions</li> <li>Understand trade-offs between architectural approaches</li> </ul>"},{"location":"patterns/#31-real-time-prediction-architecture","title":"3.1 Real-Time Prediction Architecture","text":"<p>Pattern Description: Low-latency (&lt;100ms) prediction serving for interactive applications</p> <p>GCP Implementation: <pre><code>API Gateway / Cloud Load Balancer\n          \u2193\n    Cloud Run (API)\n          \u2193\n    Memorystore (Redis) - [Cache Layer]\n          \u2193\n  Vertex AI Endpoint\n  (Multiple models with traffic splitting)\n          \u2193\n    Model (deployed on n1-standard-4 + GPU)\n</code></pre></p> <p>AWS Implementation: <pre><code>API Gateway / Application Load Balancer\n          \u2193\n    Lambda or ECS (API)\n          \u2193\n    ElastiCache (Redis) - [Cache Layer]\n          \u2193\n  SageMaker Endpoint\n  (Multiple variants with traffic splitting)\n          \u2193\n    Model (deployed on ml.m5.xlarge + GPU)\n</code></pre></p> <p>Key Differences:</p> Aspect GCP AWS API Layer Cloud Run offers automatic scaling with zero-to-scale capability Lambda for serverless or ECS for containerized workloads Cache Memorystore for Redis is fully managed ElastiCache requires more configuration but offers more control Testing Support traffic splitting and A/B testing at the endpoint level Support traffic splitting and A/B testing at the endpoint level <p>When to Use This Pattern:</p> <ul> <li>Latency requirements &lt;100ms</li> <li>High request volume with repeated queries</li> <li>Need global availability</li> <li>Variable traffic patterns</li> </ul>"},{"location":"patterns/#32-batch-prediction-pipeline","title":"3.2 Batch Prediction Pipeline","text":"<p>Pattern Description: Large-scale offline inference for millions of predictions</p> <p>GCP Implementation: <pre><code>Cloud Scheduler\n      \u2193\nCloud Functions (Trigger)\n      \u2193\nBigQuery (Input Data) \u2192 Vertex AI Batch Prediction \u2192 BigQuery (Results)\n      \u2193\nDataflow (Post-processing)\n      \u2193\nBigQuery (Final Results)\n</code></pre></p> <p>AWS Implementation: <pre><code>EventBridge (CloudWatch Events)\n      \u2193\nLambda (Trigger)\n      \u2193\nAthena/Redshift (Input Data) \u2192 SageMaker Batch Transform \u2192 S3 (Results)\n      \u2193\nAWS Glue (Post-processing)\n      \u2193\nRedshift/Athena (Final Results)\n</code></pre></p> <p>Key Differences:</p> Aspect GCP AWS Data Storage BigQuery provides unified data warehouse and prediction input/output Separate services for storage (S3), querying (Athena/Redshift), and batch inference Processing Dataflow offers unified batch and stream processing Glue for batch ETL, separate from streaming (Kinesis) Scheduling Cloud Scheduler is dedicated service EventBridge/CloudWatch Events <p>When to Use This Pattern:</p> <p>Batch prediction is suitable when:</p> <ul> <li>Millions of predictions needed</li> <li>Not time-sensitive (can take hours)</li> <li>Cost optimization important</li> <li>Periodic/scheduled inference</li> </ul>"},{"location":"patterns/#33-streaming-ml-pipeline","title":"3.3 Streaming ML Pipeline","text":"<p>Pattern Description: Real-time feature engineering and prediction on streaming data</p> <p>GCP Implementation: <pre><code>Pub/Sub (Event Stream)\n      \u2193\nDataflow (Feature Engineering)\n      \u2193   \u2193   \u2193\n      |   |   +\u2192 BigQuery (Offline Storage)\n      |   +\u2192 Feature Store (Online)\n      +\u2192 Vertex AI Endpoint (Predictions)\n           \u2193\n      Pub/Sub (Prediction Results)\n           \u2193\n      Application (Actions)\n</code></pre></p> <p>AWS Implementation: <pre><code>Kinesis Data Streams (Event Stream)\n      \u2193\nKinesis Data Analytics / Flink (Feature Engineering)\n      \u2193   \u2193   \u2193\n      |   |   +\u2192 S3 + Athena (Offline Storage)\n      |   +\u2192 SageMaker Feature Store (Online)\n      +\u2192 SageMaker Endpoint (Predictions)\n           \u2193\n      Kinesis Data Streams (Prediction Results)\n           \u2193\n      Lambda / Application (Actions)\n</code></pre></p> <p>Key Differences:</p> Aspect GCP AWS Messaging Pub/Sub is fully managed message queue with global availability Kinesis Data Streams requires shard management Stream Processing Dataflow provides unified SDK (Apache Beam) for batch and stream Kinesis Analytics or managed Flink for stream processing Feature Store Feature Store integrated with Vertex AI ecosystem SageMaker Feature Store with separate integration requirements Multi-Destination Single Dataflow job can write to multiple destinations May need separate Lambda functions or Kinesis Firehose for multi-destination writes <p>When to Use This Pattern:</p> <p>Streaming ML pipelines excel when:</p> <ul> <li>Real-time decision making required</li> <li>Continuous event streams</li> <li>Need for feature aggregation across time windows</li> <li>Low-latency requirements (seconds, not milliseconds)</li> </ul>"},{"location":"patterns/#34-end-to-end-automl-workflow","title":"3.4 End-to-End AutoML Workflow","text":"<p>Pattern Description: Automated workflow from raw data to deployed model</p> <p>GCP Implementation (Vertex AI Pipelines): <pre><code>from kfp import dsl\nfrom google.cloud.aiplatform import pipeline_jobs\n\n@dsl.pipeline(name='automl-e2e-pipeline')\ndef automl_pipeline(\n    project_id: str,\n    dataset_uri: str,\n    target_column: str,\n    model_name: str,\n    budget_hours: int = 1\n):\n    # Create dataset, train AutoML model, create endpoint, deploy model\n    pass\n\n# Execute\njob = pipeline_jobs.PipelineJob(\n    display_name='automl-churn-pipeline',\n    template_path='automl_pipeline.yaml',\n    parameter_values={\n        'project_id': 'my-project',\n        'dataset_uri': 'gs://my-bucket/churn_data.csv',\n        'target_column': 'churned',\n        'model_name': 'churn-predictor',\n        'budget_hours': 2\n    }\n)\n\njob.run()\n</code></pre></p> <p>AWS Implementation (SageMaker Pipelines): <pre><code>from sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep\nfrom sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.estimator import Estimator\n\n# Define preprocessing step\nsklearn_processor = SKLearnProcessor(\n    framework_version='0.23-1',\n    role=sagemaker_role,\n    instance_type='ml.m5.xlarge',\n    instance_count=1\n)\n\n# Define AutoML training using Autopilot\nfrom sagemaker.automl.automl import AutoML\n\nautoml = AutoML(\n    role=sagemaker_role,\n    target_attribute_name='churned',\n    output_path='s3://my-bucket/autopilot-output',\n    max_candidates=10,\n    max_runtime_per_training_job_in_seconds=3600\n)\n\n# Create and execute pipeline\npipeline = Pipeline(\n    name='automl-churn-pipeline',\n    steps=[preprocessing_step, training_step, model_step, deployment_step]\n)\n\npipeline.upsert(role_arn=sagemaker_role)\nexecution = pipeline.start()\n</code></pre></p> <p>Key Differences:</p> Aspect GCP AWS Pipeline DSL Kubeflow Pipelines DSL with Python decorators (@dsl.pipeline) SageMaker Pipelines with explicit step definitions AutoML Integration AutoML is integrated component in Vertex AI SageMaker Autopilot is separate from Pipelines, requires integration Artifact Storage Vertex AI Pipelines stores artifacts in GCS automatically Must explicitly define S3 paths for all artifacts Model Registry Built-in model registry integration Requires explicit Model Registry step configuration"},{"location":"patterns/#35-mlops-pipeline-cicdct","title":"3.5 MLOps Pipeline (CI/CD/CT)","text":"<p>Pattern Description: Continuous Integration, Deployment, and Training for ML</p> <p>GCP Complete MLOps Architecture: <pre><code>Code Repository (Cloud Source Repo / GitHub)\n      \u2193\nCloud Build (CI: Test, Build, Push)\n      \u2193\nArtifact Registry (Container Storage)\n      \u2193\nVertex AI Pipelines (Training Pipeline)\n      \u2193\nModel Registry (Version Management)\n      \u2193\nVertex AI Endpoints (Deployment)\n      \u2193\nModel Monitoring (Drift Detection)\n      \u2193 (Trigger on Drift)\nCloud Functions \u2192 Re-trigger Training\n</code></pre></p> <p>AWS Complete MLOps Architecture: <pre><code>Code Repository (CodeCommit / GitHub)\n      \u2193\nCodePipeline + CodeBuild (CI: Test, Build, Push)\n      \u2193\nECR (Elastic Container Registry)\n      \u2193\nSageMaker Pipelines (Training Pipeline)\n      \u2193\nSageMaker Model Registry (Version Management)\n      \u2193\nSageMaker Endpoints (Deployment)\n      \u2193\nSageMaker Model Monitor (Drift Detection)\n      \u2193 (Trigger on Drift)\nLambda + EventBridge \u2192 Re-trigger Training\n</code></pre></p> <p>Key Differences:</p> Aspect GCP AWS CI/CD Cloud Build is unified CI/CD service with native GCP integration CodePipeline + CodeBuild require more configuration Artifact Storage Artifact Registry for both containers and language packages ECR for containers, CodeArtifact for packages (separate services) Monitoring Vertex AI provides integrated monitoring with automated triggers SageMaker Model Monitor requires manual EventBridge rule configuration Pipeline Triggering Cloud Functions directly trigger Vertex AI Pipelines Lambda triggers SageMaker Pipelines via SDK calls Console Single Vertex AI console for entire MLOps workflow Multiple consoles (CodePipeline, SageMaker, CloudWatch, Lambda) <p>Continuous Training Triggers:</p> Trigger Type GCP Implementation AWS Implementation Schedule Cloud Scheduler \u2192 Cloud Functions \u2192 Vertex AI Pipeline EventBridge (cron) \u2192 Lambda \u2192 SageMaker Pipeline Drift Detection Vertex AI Model Monitoring \u2192 Pub/Sub \u2192 Cloud Functions SageMaker Model Monitor \u2192 EventBridge \u2192 Lambda Data Quality TFDV + Cloud Functions \u2192 Vertex AI Pipeline Glue Data Quality \u2192 EventBridge \u2192 Lambda Manual Vertex AI Console / gcloud CLI SageMaker Console / AWS CLI <p>Key Takeaways:</p> <ul> <li>Know the difference between CI (code), CD (deployment), CT (continuous training)</li> <li>Understand trigger mechanisms: schedule, drift, data quality</li> <li>Remember approval gates for production deployment</li> <li>Know rollback strategies (traffic splitting, blue/green)</li> <li>GCP offers more integrated MLOps experience; AWS provides more flexibility with separate services</li> </ul>"},{"location":"reference/","title":"Quick Reference Tables","text":""},{"location":"reference/#gcp-ml-services-vs-aws-services","title":"GCP ML Services vs AWS Services","text":"GCP Service AWS Equivalent Primary Use Case Vertex AI Training SageMaker Training Custom model training Vertex AI AutoML SageMaker Autopilot/Canvas Automated ML Vertex AI Prediction SageMaker Endpoints Model serving Vertex AI Pipelines SageMaker Pipelines ML workflow orchestration Vertex AI Feature Store SageMaker Feature Store Feature management Vertex AI Model Monitoring SageMaker Model Monitor Drift detection BigQuery ML Redshift ML, Athena ML SQL-based ML Dataflow AWS Glue, Kinesis Analytics Data processing Dataproc Amazon EMR Spark/Hadoop workloads Pub/Sub Kinesis, SNS/SQS Messaging Cloud Storage Amazon S3 Object storage Cloud Build CodeBuild CI/CD Artifact Registry ECR Container registry Cloud Composer MWAA Workflow orchestration Vision AI Rekognition Image analysis Natural Language AI Comprehend Text analysis Translation API Translate Language translation Speech-to-Text Transcribe Speech recognition Text-to-Speech Polly Speech synthesis"},{"location":"reference/#when-to-use-which-service","title":"When to Use Which Service","text":"Use Case GCP Service Why Quick PoC with tabular data BigQuery ML or AutoML No ML expertise needed, SQL-based Custom deep learning Vertex AI Custom Training Full control, TPU access Large-scale batch predictions Vertex AI Batch Prediction Cost-effective, BigQuery integration Real-time predictions Vertex AI Endpoints Auto-scaling, managed infrastructure Streaming data processing Dataflow Unified batch/streaming, Apache Beam Data warehouse analytics BigQuery Serverless, petabyte-scale ML pipeline orchestration Vertex AI Pipelines ML-specific, metadata tracking Feature engineering at scale Dataflow + TensorFlow Transform Training-serving consistency Time series forecasting BigQuery ML ARIMA_PLUS SQL-based, automatic seasonality Image classification (standard) Vision AI API Pre-trained, no training needed Custom image models Vertex AI AutoML Vision Custom classes, transfer learning"},{"location":"scenarios/","title":"Common Scenarios and Solutions","text":""},{"location":"scenarios/#scenario-1-real-time-fraud-detection","title":"Scenario 1: Real-Time Fraud Detection","text":"<p>Question: Design a system for real-time credit card fraud detection with &lt;50ms latency.</p> <p>Answer: <pre><code>User Transaction \u2192 API Gateway \u2192 Cloud Run (preprocessing)\n                                     \u2193\n                              Feature Store (online serving)\n                                     \u2193\n                              Vertex AI Endpoint (model)\n                                     \u2193\n                                   Decision\n</code></pre></p> <p>Key Points:</p> <ul> <li>Feature Store for low-latency feature retrieval</li> <li>Vertex AI Endpoint with GPU for fast inference</li> <li>Cloud Run for stateless API layer</li> <li>Consider caching for repeat requests</li> </ul>"},{"location":"scenarios/#scenario-2-large-scale-batch-predictions","title":"Scenario 2: Large-Scale Batch Predictions","text":"<p>Question: Process 100M predictions daily, cost-effectively.</p> <p>Answer:</p> <p>Follow this cost-effective approach:</p> <ul> <li>Use Vertex AI Batch Prediction (not online endpoints)</li> <li>Input from BigQuery, output to BigQuery</li> <li>Schedule with Cloud Scheduler</li> <li>Use preemptible VMs for 60-80% cost savings</li> <li>Partition output by date in BigQuery</li> </ul>"},{"location":"scenarios/#scenario-3-model-retraining-on-drift","title":"Scenario 3: Model Retraining on Drift","text":"<p>Question: Automatically retrain model when drift detected.</p> <p>Answer: <pre><code>Vertex AI Model Monitoring (drift detection)\n             \u2193\n        Pub/Sub alert\n             \u2193\n    Cloud Function (trigger)\n             \u2193\n  Vertex AI Pipeline (retraining)\n             \u2193\n    Model Registry (new version)\n             \u2193\n   Endpoint (canary deployment)\n</code></pre></p>"},{"location":"scenarios/#scenario-4-sql-analyst-needs-ml","title":"Scenario 4: SQL Analyst Needs ML","text":"<p>Question: Data analyst team wants to build ML models using SQL.</p> <p>Answer:</p> <p>BigQuery ML is the ideal solution:</p> <ul> <li>BigQuery ML is the right choice</li> <li>No Python/ML expertise required</li> <li>Data stays in BigQuery (no movement)</li> <li>Supports common model types (regression, classification, time series)</li> </ul>"},{"location":"scenarios/#scenario-5-training-serving-consistency","title":"Scenario 5: Training-Serving Consistency","text":"<p>Question: Ensure preprocessing same for training and serving.</p> <p>Answer:</p> <p>Use TensorFlow Transform to guarantee consistency:</p> <ul> <li>Use TensorFlow Transform (TFT)</li> <li>Define preprocessing_fn once</li> <li>Use in both training pipeline and serving</li> <li>Save transform function with model</li> <li>Apply same transform at inference time</li> </ul>"},{"location":"technical/","title":"Key Technical Aspects","text":"<p>Section Overview: This section provides deep technical knowledge of critical topics, with detailed AWS comparisons to leverage your existing expertise.</p> <p>Learning Objectives:</p> <ul> <li>Master Vertex AI training, deployment, and monitoring capabilities</li> <li>Understand data engineering patterns for ML</li> <li>Learn MLOps best practices on GCP</li> <li>Recognize optimal service choices for different scenarios</li> </ul>"},{"location":"technical/#21-model-training","title":"2.1 Model Training","text":""},{"location":"technical/#custom-training-on-vertex-ai","title":"Custom Training on Vertex AI","text":"<p>Training Job Types:</p> <ul> <li>Pre-built Containers: Use Google's managed containers for TensorFlow, PyTorch, scikit-learn</li> <li>Custom Containers: Bring your own Docker image for any framework</li> <li>Python Packages: Submit Python training application without containers</li> </ul>"},{"location":"technical/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>GCP - Vertex AI Hyperparameter Tuning: <pre><code>from google.cloud import aiplatform\nfrom google.cloud.aiplatform import hyperparameter_tuning as hpt\n\n# Define hyperparameter search space\njob = aiplatform.CustomTrainingJob(\n    display_name='hp-tuning-job',\n    container_uri='gcr.io/my-project/trainer:latest'\n)\n\nhp_job = aiplatform.HyperparameterTuningJob(\n    display_name='fraud-detection-tuning',\n    custom_job=job,\n    metric_spec={\n        'auc': 'maximize',  # Optimization objective\n    },\n    parameter_spec={\n        'learning_rate': hpt.DoubleParameterSpec(min=0.0001, max=0.1, scale='log'),\n        'batch_size': hpt.DiscreteParameterSpec(values=[16, 32, 64, 128]),\n        'num_layers': hpt.IntegerParameterSpec(min=1, max=5, scale='linear'),\n        'dropout': hpt.DoubleParameterSpec(min=0.1, max=0.5, scale='linear')\n    },\n    max_trial_count=50,\n    parallel_trial_count=5,\n    search_algorithm='random',  # or 'grid', 'bayesian'\n    max_failed_trial_count=10\n)\n\nhp_job.run()\n</code></pre></p>"},{"location":"technical/#distributed-training","title":"Distributed Training","text":"<p>Strategies:</p> <ul> <li>Data Parallelism: Replicate model across multiple workers, split data</li> <li>Model Parallelism: Split large model across multiple workers</li> <li>Reduction Server: GCP-specific feature for efficient gradient aggregation</li> </ul>"},{"location":"technical/#gpu-and-tpu-selection","title":"GPU and TPU Selection","text":"<p>TPU (Tensor Processing Unit) - GCP Exclusive:</p> <ul> <li>Custom-designed for matrix operations in neural networks</li> <li>Significantly faster for large models (BERT, ResNet, etc.)</li> <li>Cost-effective at scale</li> <li>Best for TensorFlow models</li> </ul> <p>When to Use TPUs:</p> <ul> <li>Large batch sizes (&gt;64)</li> <li>Models dominated by matrix multiplications</li> <li>TensorFlow-based training</li> <li>Need for maximum throughput</li> </ul>"},{"location":"technical/#22-model-deployment-and-serving","title":"2.2 Model Deployment and Serving","text":""},{"location":"technical/#vertex-ai-prediction---online-vs-batch","title":"Vertex AI Prediction - Online vs Batch","text":"<p>Online Prediction (Real-time):</p> <ul> <li>Low-latency requirements (&lt;100ms)</li> <li>Interactive applications</li> <li>Real-time fraud detection</li> <li>Recommendation systems</li> </ul> <p>Batch Prediction:</p> <ul> <li>Large-scale inference (millions of predictions)</li> <li>Non-time-sensitive workloads</li> <li>Cost optimization (no always-on infrastructure)</li> <li>Regular scheduled predictions</li> </ul>"},{"location":"technical/#model-versioning-and-traffic-splitting","title":"Model Versioning and Traffic Splitting","text":"<p>Use Case: Deploy new model version alongside existing version for canary or A/B testing</p> <p>Deployment Strategies:</p> <ul> <li>Blue/Green: Deploy new version, test, then switch all traffic</li> <li>Canary: Gradually increase traffic to new version (10% \u2192 25% \u2192 50% \u2192 100%)</li> <li>A/B Testing: Split traffic evenly for statistical comparison</li> </ul>"},{"location":"technical/#23-data-engineering-for-ml","title":"2.3 Data Engineering for ML","text":""},{"location":"technical/#vertex-ai-feature-store","title":"Vertex AI Feature Store","text":"<p>Architecture:</p> <ul> <li>Online Store: Low-latency serving for real-time predictions (milliseconds)</li> <li>Offline Store: Batch access for training data (BigQuery-based)</li> </ul> <p>Concepts:</p> <ul> <li>Feature: Individual measurable property (e.g., age, total_purchases)</li> <li>Entity: Object being modeled (e.g., customer, product)</li> <li>Entity Type: Collection of features for an entity</li> <li>Feature Store: Container for multiple entity types</li> </ul>"},{"location":"technical/#data-preprocessing-with-dataflow-and-tensorflow-transform","title":"Data Preprocessing with Dataflow and TensorFlow Transform","text":"<p>TensorFlow Transform (TFT) provides:</p> <ul> <li>Consistent preprocessing between training and serving</li> <li>Full-pass statistics over entire dataset</li> <li>TensorFlow graph-based transformations</li> </ul>"},{"location":"technical/#24-ml-pipeline-orchestration","title":"2.4 ML Pipeline Orchestration","text":""},{"location":"technical/#vertex-ai-pipelines-kubeflow-pipelines","title":"Vertex AI Pipelines (Kubeflow Pipelines)","text":"<p>Key Concepts:</p> <ul> <li>Component: Reusable, self-contained unit of work</li> <li>Pipeline: DAG of components</li> <li>Artifact: Data produced/consumed by components</li> <li>Metadata: Lineage and execution tracking</li> </ul>"},{"location":"technical/#25-model-monitoring-and-management","title":"2.5 Model Monitoring and Management","text":""},{"location":"technical/#vertex-ai-model-monitoring","title":"Vertex AI Model Monitoring","text":"<p>Types of Monitoring:</p> <ul> <li>Training-Serving Skew: Distribution difference between training and serving data</li> <li>Prediction Drift: Change in prediction distribution over time</li> <li>Feature Attribution: Understanding which features drive predictions</li> </ul> <p>Monitoring Metrics:</p> <ul> <li>Jensen-Shannon divergence: Measure distribution difference</li> <li>Normalized difference: Feature-level comparison</li> <li>Custom thresholds: Per-feature sensitivity</li> </ul>"},{"location":"technical/#26-mlops-and-cicd","title":"2.6 MLOps and CI/CD","text":""},{"location":"technical/#continuous-training-ct-architecture","title":"Continuous Training (CT) Architecture","text":"<p>Components:</p> <ul> <li>Data Monitoring: Detect data quality issues or drift</li> <li>Trigger: Initiate retraining based on schedule or drift</li> <li>Training Pipeline: Execute model training</li> <li>Validation: Evaluate new model performance</li> <li>Deployment: Deploy if better than current model</li> </ul>"},{"location":"technical/#model-registry-and-versioning","title":"Model Registry and Versioning:","text":"<pre><code>from google.cloud import aiplatform\n\n# Register model with version and metadata\nmodel = aiplatform.Model.upload(\n    display_name='fraud-detector',\n    artifact_uri='gs://my-bucket/models/fraud-detector-v1.5/',\n    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest',\n    labels={\n        'version': 'v1.5',\n        'framework': 'scikit-learn',\n        'task': 'classification',\n        'team': 'fraud-ml'\n    },\n    description='Fraud detection model trained on 2024-01-15',\n    model_id='fraud-detector-v15'\n)\n\n# Add version alias\nmodel.add_version_aliases(['production', 'latest'])\n</code></pre> <p>Key Takeaways:</p> <ul> <li>Know triggers for CT: schedule, drift, data changes, manual</li> <li>Understand model versioning and aliases</li> <li>Remember approval workflows (dev \u2192 staging \u2192 production)</li> <li>Know rollback strategies</li> </ul>"}]}