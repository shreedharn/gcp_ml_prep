{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GCP ML Engineer Certification Study Guide","text":"<p>A comprehensive study guide for AWS professionals preparing for the Google Cloud Professional Machine Learning Engineer certification.</p>"},{"location":"#overview","title":"Overview","text":"<p>This guide leverages your AWS Machine Learning expertise to accelerate GCP learning through:</p> <ul> <li>Direct service-to-service comparisons (GCP \u2194 AWS)</li> <li>Parallel implementation examples</li> <li>Architectural pattern mappings</li> <li>Hands-on lab recommendations</li> </ul>"},{"location":"#study-guide-contents","title":"Study Guide Contents","text":""},{"location":"#core-study-materials","title":"Core Study Materials","text":"<ul> <li>ML Concepts - Fundamental machine learning concepts including algorithms, training, and evaluation</li> <li>Data Science Concepts - Data preparation, preprocessing, and transformation techniques</li> <li>GCP &amp; AWS Comparison - Comprehensive service-to-service mapping and feature comparisons</li> <li>Technical Aspects - Deep dive into Vertex AI, training, deployment, and monitoring</li> <li>Architectural Patterns - Common ML architecture patterns with GCP/AWS implementations</li> <li>MLFlow vs Kubeflow - Detailed comparison of ML orchestration frameworks in both clouds</li> </ul>"},{"location":"#reference-materials","title":"Reference Materials","text":"<ul> <li>Common Scenarios - Real-world exam scenarios with detailed solutions</li> <li>Data Science Techniques - Quick decision guide for data science challenges</li> <li>Data Splitting Strategies - Comprehensive guide to train/test/validation splitting techniques</li> <li>Hands-On Labs - Practical exercises and preparation checklist</li> <li>Quick Reference - Service mapping tables and decision matrices for rapid review</li> <li>Glossary - Comprehensive terminology reference with AWS equivalents</li> </ul>"},{"location":"#\u2139-about","title":"\u2139\ufe0f About","text":"<p>This study guide is created for educational and informational purposes to help AWS professionals prepare for the Google Cloud Professional Machine Learning Engineer certification</p>"},{"location":"#-contributing","title":"\ud83e\udd1d Contributing","text":"<p>If you find errors or have suggestions for improvements, please open an issue at: https://github.com/shreedharn/gcp_ml_prep</p>"},{"location":"#-license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"CLAUDE/","title":"CLAUDE","text":"<ul> <li>This repo is a comprehensive study guide for AWS ML professionals preparing for the Google Cloud Professional Machine Learning Engineer certification.</li> <li>Whenever there is a new markdown file created in the project repo root folder, update the nav section of mkdocs.yml and README.md</li> <li>Always use a more formal, technical tone appropriate for professional reading, avoiding second-person pronouns.</li> <li>Avoid excessive bold in markdown content</li> </ul>"},{"location":"LICENSE/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2025 Shreedhar</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"concepts/","title":"Machine Learning Concepts - Quick Reference","text":""},{"location":"concepts/#overview","title":"Overview","text":"<p>This page provides concise explanations of fundamental machine learning concepts organized by category. Use this as a quick reference when building models, tuning hyperparameters, or evaluating performance.</p>"},{"location":"concepts/#1-core-ml-concepts","title":"1. Core ML Concepts","text":"<p>Fundamental principles underlying machine learning.</p>"},{"location":"concepts/#supervised-learning","title":"Supervised Learning","text":"<p>Learning paradigm where model trains on labeled data (input-output pairs) to learn mapping from inputs to outputs. Includes classification (discrete outputs) and regression (continuous outputs). Requires labeled training data. Examples: predicting house prices, image classification, spam detection.</p>"},{"location":"concepts/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Learning paradigm where model finds patterns in unlabeled data without explicit target variables. Includes clustering, dimensionality reduction, and anomaly detection. No right or wrong answers, discovers hidden structure. Examples: customer segmentation, topic modeling, compression.</p>"},{"location":"concepts/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Learning paradigm where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward. The agent learns through trial and error, receiving feedback as rewards or penalties. No labeled training data; instead learns optimal behavior through interaction. Applications include game playing, robotics, and autonomous systems.</p>"},{"location":"concepts/#2-feature-preprocessing","title":"2. Feature Preprocessing","text":"<p>Transformations applied to features before model training.</p>"},{"location":"concepts/#feature-scaling-normalization","title":"Feature Scaling (Normalization)","text":"<p>Scales features to a fixed range, typically [0, 1] using: (x - min) / (max - min). Preserves original distribution shape and relationships. Sensitive to outliers as they determine min/max. Required for distance-based algorithms like KNN, SVM, neural networks.</p>"},{"location":"concepts/#standardization-z-score-normalization","title":"Standardization (Z-score Normalization)","text":"<p>Transforms features to have mean=0 and standard deviation=1 using: (x - mean) / std. Assumes roughly normal distribution; less sensitive to outliers than min-max scaling. Results in unbounded range (typically -3 to +3 for normal data). Preferred for algorithms assuming normally distributed data.</p>"},{"location":"concepts/#3-data-encoding-techniques","title":"3. Data Encoding Techniques","text":"<p>Methods to convert categorical data into numerical format for machine learning models.</p>"},{"location":"concepts/#one-hot-encoding","title":"One Hot Encoding","text":"<p>Converts categorical variables into binary vectors where only one element is 1 (hot) and others are 0. Each category becomes a separate binary column. For example, colors [Red, Blue, Green] \u2192 Red: [1,0,0], Blue: [0,1,0], Green: [0,0,1]. Prevents model from assuming ordinal relationships between categories.</p>"},{"location":"concepts/#label-encoding","title":"Label Encoding","text":"<p>Converts categorical labels into integers (0, 1, 2, ...) by assigning each unique category a number. Simple and memory-efficient but implies ordinal relationship between categories. Example: [Red, Blue, Green] \u2192 [0, 1, 2]. Best for ordinal data or tree-based models that don't assume ordering.</p>"},{"location":"concepts/#ordinal-encoding","title":"Ordinal Encoding","text":"<p>Similar to label encoding but explicitly preserves meaningful order in categorical variables. User assigns specific integers reflecting the inherent ranking (e.g., Low=1, Medium=2, High=3). Appropriate for ordinal features like education level, satisfaction ratings, or size categories. Encodes domain knowledge about category ordering.</p>"},{"location":"concepts/#target-encoding-mean-encoding","title":"Target Encoding (Mean Encoding)","text":"<p>Replaces each category with the mean of the target variable for that category. For example, if \"Blue\" items have average price $50, all \"Blue\" becomes 50. Risk of overfitting; use with cross-validation or smoothing techniques. Effective for high-cardinality categorical features.</p>"},{"location":"concepts/#binary-encoding","title":"Binary Encoding","text":"<p>Converts categories to integers then to binary code, using fewer columns than one-hot encoding. Each category gets a binary representation (e.g., 5 categories need only 3 binary columns instead of 5). Reduces dimensionality while preserving uniqueness. Useful for high-cardinality features.</p>"},{"location":"concepts/#4-missing-data-imputation","title":"4. Missing Data Imputation","text":"<p>Techniques to handle missing values in datasets.</p>"},{"location":"concepts/#simple-imputation-meanmedianmode","title":"Simple Imputation (Mean/Median/Mode)","text":"<p>Replaces missing values with mean (continuous), median (skewed continuous), or mode (categorical) of the column. Fast and simple but ignores relationships between features and reduces variance. Mean is sensitive to outliers; median is robust. Mode for categorical data.</p>"},{"location":"concepts/#forward-fill--backward-fill","title":"Forward Fill / Backward Fill","text":"<p>Time-series imputation that fills missing values with the previous (forward) or next (backward) observed value. Assumes temporal continuity and that values don't change drastically between time points. Forward fill: copies last known value; backward fill: copies next known value. Common in stock prices, sensor data.</p>"},{"location":"concepts/#knn-imputation","title":"KNN Imputation","text":"<p>Fills missing values using K-Nearest Neighbors; finds k most similar samples and uses their average. Considers feature relationships unlike simple imputation, but computationally expensive for large datasets. Distance metric (usually Euclidean) determines similarity. Typical k values: 3-10.</p>"},{"location":"concepts/#multiple-imputation","title":"Multiple Imputation","text":"<p>Statistical technique that creates multiple plausible imputed datasets, analyzes each, then pools results. Accounts for uncertainty in missing values unlike single imputation methods. MICE (Multivariate Imputation by Chained Equations) is popular implementation. Produces more accurate standard errors and confidence intervals.</p>"},{"location":"concepts/#imputation-with-supervised-learning","title":"Imputation with Supervised Learning","text":"<p>Treats missing value prediction as a machine learning problem using other features as predictors. Train regression model (for continuous) or classification model (for categorical) on complete cases. More sophisticated than mean/median but risk of overfitting. Can capture complex feature relationships.</p>"},{"location":"concepts/#5-sampling-techniques-for-imbalanced-data","title":"5. Sampling Techniques for Imbalanced Data","text":"<p>Methods to address class imbalance in classification problems.</p>"},{"location":"concepts/#smote-synthetic-minority-over-sampling","title":"SMOTE (Synthetic Minority Over-sampling)","text":"<p>Creates synthetic samples for minority class by interpolating between existing minority samples and their nearest neighbors. Generates new points along line segments connecting k-nearest minority neighbors. Reduces overfitting compared to simple oversampling by creating diverse examples. Most popular resampling technique for imbalanced classification.</p>"},{"location":"concepts/#random-oversampling","title":"Random Oversampling","text":"<p>Randomly duplicates examples from minority class until dataset is balanced. Simple but can lead to overfitting as model sees exact same examples multiple times. Works well combined with regularization or ensemble methods. Fast and easy to implement.</p>"},{"location":"concepts/#random-undersampling","title":"Random Undersampling","text":"<p>Randomly removes examples from majority class to balance dataset. Risk of losing potentially important information from majority class. Useful when majority class has redundant information or dataset is very large. Often combined with oversampling (hybrid approach).</p>"},{"location":"concepts/#6-loss-functions","title":"6. Loss Functions","text":"<p>Metrics that quantify how wrong the model's predictions are.</p>"},{"location":"concepts/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>Measures average squared difference between predicted and actual values: (1/n) \u03a3(y - \u0177)\u00b2. Most common loss function for regression problems. Heavily penalizes large errors due to squaring; sensitive to outliers. Differentiable everywhere, making it suitable for gradient-based optimization. Used in linear regression, neural networks for regression tasks. Units are squared (e.g., dollars\u00b2 for price prediction).</p>"},{"location":"concepts/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<p>Measures average absolute difference between predicted and actual values: (1/n) \u03a3|y - \u0177|. More robust to outliers than MSE since errors are not squared. Gives equal weight to all errors regardless of magnitude. Interpretable in original units (e.g., dollars for price prediction). Less sensitive to extreme values; suitable when outliers shouldn't dominate the loss.</p>"},{"location":"concepts/#huber-loss","title":"Huber Loss","text":"<p>Combines benefits of MSE and MAE; acts as MSE for small errors and MAE for large errors. Quadratic for errors below threshold \u03b4, linear for errors above \u03b4. More robust to outliers than MSE while maintaining differentiability. Common \u03b4 values: 1.0 or 1.35. Used in robust regression when dataset contains outliers but gradient-based optimization is needed.</p>"},{"location":"concepts/#binary-cross-entropy-log-loss","title":"Binary Cross-Entropy (Log Loss)","text":"<p>Loss function for binary classification: -[y log(p) + (1-y) log(1-p)], where y is true label (0 or 1) and p is predicted probability. Heavily penalizes confident wrong predictions; small penalty for correct predictions with high confidence. Outputs range from 0 (perfect) to infinity (worst). Equivalent to negative log-likelihood for Bernoulli distribution. Standard loss for logistic regression and binary classification neural networks.</p>"},{"location":"concepts/#categorical-cross-entropy","title":"Categorical Cross-Entropy","text":"<p>Extension of binary cross-entropy for multi-class classification: -\u03a3 y_i log(p_i) across all classes. Compares one-hot encoded true labels with predicted probability distribution from softmax. Minimizing this loss is equivalent to maximizing log-likelihood. Used with softmax activation in neural networks for multi-class problems. Requires mutually exclusive classes (each sample belongs to exactly one class).</p>"},{"location":"concepts/#sparse-categorical-cross-entropy","title":"Sparse Categorical Cross-Entropy","text":"<p>Functionally identical to categorical cross-entropy but accepts integer class labels instead of one-hot encoded vectors. Computationally more efficient for problems with many classes (hundreds or thousands). Example: class label is 5 instead of [0,0,0,0,0,1,0,...]. Commonly used in NLP tasks with large vocabularies and image classification with many categories.</p>"},{"location":"concepts/#hinge-loss","title":"Hinge Loss","text":"<p>Loss function for maximum-margin classification, primarily used in SVMs: max(0, 1 - y\u00b7\u0177) where y \u2208 {-1, 1} and \u0177 is raw prediction. Encourages correct predictions to be beyond a margin; zero loss if prediction is correct and confident. Creates linear decision boundaries. Not probabilistic like cross-entropy; focuses on margin maximization. Used in SVMs and some neural network applications.</p>"},{"location":"concepts/#focal-loss","title":"Focal Loss","text":"<p>Modification of cross-entropy that down-weights easy examples and focuses on hard examples: -\u03b1(1-p)^\u03b3 log(p) for positive class. Parameter \u03b3 (typically 2) controls how much to focus on hard examples; \u03b1 balances positive/negative classes. Addresses extreme class imbalance by reducing loss contribution from well-classified examples. Developed for object detection where easy negatives vastly outnumber hard positives. Particularly effective when 99%+ samples are easy negatives.</p>"},{"location":"concepts/#kullback-leibler-kl-divergence","title":"Kullback-Leibler (KL) Divergence","text":"<p>Measures how one probability distribution differs from another: \u03a3 P(x) log(P(x)/Q(x)). Asymmetric measure (KL(P||Q) \u2260 KL(Q||P)); not a true distance metric. Used in variational autoencoders (VAEs) to match learned distribution to prior. Measures information loss when Q approximates P. Common in generative models and distribution matching tasks.</p>"},{"location":"concepts/#7-optimization-algorithms","title":"7. Optimization Algorithms","text":"<p>Methods for updating model weights during training.</p>"},{"location":"concepts/#sgd-stochastic-gradient-descent","title":"SGD (Stochastic Gradient Descent)","text":"<p>Updates weights using gradient computed from one random sample (or mini-batch) at a time. Faster per iteration than batch gradient descent and can escape local minima due to noise. Converges with fluctuations; learning rate scheduling often needed. Foundation for most modern optimizers.</p>"},{"location":"concepts/#momentum","title":"Momentum","text":"<p>Enhancement to SGD that adds fraction of previous update to current update, helping accelerate in consistent directions. Reduces oscillations and speeds up convergence by building velocity in gradient direction. Typical momentum parameter: 0.9. Think of it as a ball rolling downhill gaining speed.</p>"},{"location":"concepts/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<p>Combines momentum and adaptive learning rates; maintains per-parameter learning rates adapted based on gradient history. Computes adaptive learning rates from first (mean) and second (variance) moments of gradients. Widely used default optimizer; often works well with minimal tuning. Typical hyperparameters: \u03b2\u2081=0.9, \u03b2\u2082=0.999.</p>"},{"location":"concepts/#rmsprop-root-mean-square-propagation","title":"RMSprop (Root Mean Square Propagation)","text":"<p>Adapts learning rate for each parameter based on recent gradient magnitudes using moving average of squared gradients. Divides learning rate by root of this average, preventing oscillations in steep directions. Effective for recurrent neural networks and non-stationary problems. Developed by Geoffrey Hinton.</p>"},{"location":"concepts/#8-training-hyperparameters","title":"8. Training Hyperparameters","text":"<p>Key parameters that control how models learn from data.</p>"},{"location":"concepts/#batch-size","title":"Batch Size","text":"<p>The number of training examples used in one iteration to update model weights. Smaller batches (1-32) provide noisy but frequent updates; larger batches (128-512) provide stable but less frequent updates. Affects training speed, memory usage, and convergence quality. Common values: 32, 64, 128, 256.</p>"},{"location":"concepts/#mini-batch","title":"Mini-batch","text":"<p>A subset of the training data, larger than one example but smaller than the full dataset, used for gradient descent. Combines benefits of stochastic (fast, noisy updates) and batch (stable, accurate) gradient descent. Most common approach in modern deep learning. Typically ranges from 16 to 512 examples.</p>"},{"location":"concepts/#learning-rate","title":"Learning Rate","text":"<p>Controls the step size when updating model weights during training; determines how quickly the model adapts to the problem. Too high causes unstable training or divergence; too low causes slow convergence or getting stuck. Often the most important hyperparameter to tune. Typical starting values: 0.001 to 0.1.</p>"},{"location":"concepts/#9-model-validation-techniques","title":"9. Model Validation Techniques","text":"<p>Strategies to assess model performance and prevent overfitting.</p>"},{"location":"concepts/#train-test-split","title":"Train-Test Split","text":"<p>Divides dataset into separate training set (to fit model) and test set (to evaluate performance). Common splits: 70-30, 80-20, 90-10 depending on data size. Test set must never be used during training or hyperparameter tuning. Simple but can be unreliable with small datasets.</p>"},{"location":"concepts/#cross-validation","title":"Cross-Validation","text":"<p>Evaluates model by training on multiple different subsets of data and averaging results. Provides more reliable performance estimate than single train-test split. Uses all data for both training and validation, maximizing data efficiency. Essential for small datasets and hyperparameter tuning.</p>"},{"location":"concepts/#k-fold-cross-validation","title":"K-Fold Cross-Validation","text":"<p>Divides data into k equal folds; trains k times, each time using k-1 folds for training and 1 for validation. Averages performance across all k runs for final estimate. Common k values: 5 or 10. Stratified k-fold maintains class proportions in each fold for classification.</p>"},{"location":"concepts/#10-evaluation-metrics","title":"10. Evaluation Metrics","text":"<p>Measures to assess model performance, especially for classification tasks.</p>"},{"location":"concepts/#confusion-matrix","title":"Confusion Matrix","text":"<p>A table showing counts of True Positives, True Negatives, False Positives, and False Negatives for classification models. Rows represent actual classes, columns represent predicted classes. Provides complete picture of classification performance beyond simple accuracy. Foundation for calculating precision, recall, F1, and other metrics.</p>"},{"location":"concepts/#accuracy","title":"Accuracy","text":"<p>The proportion of correct predictions out of total predictions: (TP + TN) / (TP + TN + FP + FN). Simple metric but can be misleading with imbalanced datasets. A model predicting all negatives on 95% negative data achieves 95% accuracy but is useless. Best used when classes are balanced and errors are equally costly.</p>"},{"location":"concepts/#precision","title":"Precision","text":"<p>The proportion of positive predictions that are actually correct: True Positives / (True Positives + False Positives). Answers \"Of all items we labeled as positive, how many were truly positive?\" High precision means low false positive rate. Important when false positives are costly (e.g., spam detection, medical diagnosis).</p>"},{"location":"concepts/#recall-sensitivity","title":"Recall (Sensitivity)","text":"<p>The proportion of actual positives that were correctly identified: True Positives / (True Positives + False Negatives). Answers \"Of all actual positive items, how many did we correctly identify?\" High recall means low false negative rate. Important when missing positives is costly (e.g., cancer detection, fraud detection).</p>"},{"location":"concepts/#f1-score","title":"F1 Score","text":"<p>Harmonic mean of precision and recall: 2 \u00d7 (Precision \u00d7 Recall) / (Precision + Recall). Balances precision and recall into a single metric, useful when you need both to be reasonably high. Ranges from 0 to 1, with 1 being perfect. Preferred over accuracy for imbalanced datasets.</p>"},{"location":"concepts/#roc-curve-receiver-operating-characteristic","title":"ROC Curve (Receiver Operating Characteristic)","text":"<p>Graph plotting True Positive Rate (Recall) against False Positive Rate at various classification thresholds. Shows tradeoff between sensitivity and specificity across all possible thresholds. Curve closer to top-left corner indicates better performance. Useful for selecting optimal threshold for your use case.</p>"},{"location":"concepts/#auc-area-under-the-roc-curve","title":"AUC (Area Under the ROC Curve)","text":"<p>Single number (0 to 1) summarizing ROC curve performance; measures probability that model ranks random positive higher than random negative. AUC of 0.5 means random guessing; 1.0 means perfect classification. Threshold-independent metric, useful for comparing models. Robust to class imbalance.</p>"},{"location":"concepts/#auc-pr-area-under-the-precision-recall-curve","title":"AUC-PR (Area Under the Precision-Recall Curve)","text":"<p>Single number (0 to 1) summarizing the precision-recall curve; measures model performance across all classification thresholds by plotting precision against recall. Unlike AUC-ROC, AUC-PR focuses on the positive class performance and is more informative for highly imbalanced datasets where the minority class is of primary interest. Higher AUC-PR indicates better model performance on the positive class. Preferred over AUC-ROC when positive class is rare (fraud detection, rare disease diagnosis, anomaly detection).</p>"},{"location":"concepts/#11-model-performance-issues","title":"11. Model Performance Issues","text":"<p>Understanding when your model learns too much, too little, or just right.</p>"},{"location":"concepts/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<p>Bias is error from wrong assumptions (underfitting); variance is error from sensitivity to training data fluctuations (overfitting). Models with high bias are too simple; high variance models are too complex. Optimal model minimizes total error = bias\u00b2 + variance + irreducible error. Core challenge in model selection.</p>"},{"location":"concepts/#overfitting","title":"Overfitting","text":"<p>When a model learns training data too well, including its noise and peculiarities, causing poor performance on new data. The model memorizes rather than generalizes. Signs include high training accuracy but low validation accuracy. Solution: Use regularization, more data, or simpler models.</p>"},{"location":"concepts/#underfitting","title":"Underfitting","text":"<p>When a model is too simple to capture underlying patterns in the data, resulting in poor performance on both training and test data. The model lacks the capacity to learn the relationship between features and targets. Signs include low accuracy on both training and validation sets. Solution: Use more complex models, add features, or train longer.</p>"},{"location":"concepts/#12-regularization-techniques","title":"12. Regularization Techniques","text":"<p>Methods to prevent overfitting by constraining model complexity.</p>"},{"location":"concepts/#l1-regularization-lasso","title":"L1 Regularization (Lasso)","text":"<p>Adds penalty equal to absolute value of coefficient magnitudes to the loss function, encouraging sparsity by driving some weights to exactly zero. Useful for feature selection as it automatically eliminates less important features. The penalty term is \u03bb\u2211|w|, where \u03bb controls regularization strength. Creates sparse models that are easier to interpret.</p>"},{"location":"concepts/#l2-regularization-ridge","title":"L2 Regularization (Ridge)","text":"<p>Adds penalty equal to square of coefficient magnitudes to the loss function, discouraging large weights but keeping all features. Distributes weights more evenly across features rather than eliminating them. The penalty term is \u03bb\u2211w\u00b2, where \u03bb controls regularization strength. Preferred when all features are potentially relevant.</p>"},{"location":"concepts/#dropout","title":"Dropout","text":"<p>Randomly ignores (drops) a percentage of neurons during each training iteration in neural networks. Forces network to learn redundant representations, preventing over-reliance on specific neurons. Typical dropout rates: 0.2-0.5. Only applied during training, not inference.</p>"},{"location":"concepts/#early-stopping","title":"Early Stopping","text":"<p>Stops training when validation performance stops improving for specified number of epochs (patience). Prevents overfitting by avoiding unnecessary training iterations. Monitors validation loss and saves best model. Simple yet highly effective regularization technique.</p>"},{"location":"concepts/#13-advanced-ml-techniques","title":"13. Advanced ML Techniques","text":"<p>Specialized machine learning approaches for specific use cases.</p>"},{"location":"concepts/#transfer-learning","title":"Transfer Learning","text":"<p>Technique that leverages knowledge from a pre-trained model (trained on large dataset) and adapts it to a new, related task with limited data. Instead of training from scratch, reuse learned features from the source task. Common approach: freeze early layers (general features like edges, textures) and fine-tune later layers (task-specific features). Dramatically reduces training time, data requirements, and computational costs.</p> <p>When to Use:</p> <ul> <li>Limited labeled data for target task</li> <li>Target task is similar to source task (e.g., both are image classification)</li> <li>Want to leverage state-of-the-art pretrained models</li> </ul> <p>Common Approaches:</p> <ul> <li>Feature Extraction: Freeze all pretrained layers, only train new classifier on top</li> <li>Fine-tuning: Unfreeze some layers and retrain them with small learning rate</li> <li>Progressive Unfreezing: Gradually unfreeze layers from top to bottom during training</li> </ul> <p>Popular Pretrained Models:</p> <ul> <li>Vision: ResNet, VGG, EfficientNet, Vision Transformer (ViT)</li> <li>NLP: BERT, GPT, T5, RoBERTa</li> <li>Multi-modal: CLIP, Flamingo</li> </ul> <p>Example Workflow:</p> <ol> <li>Start with model pretrained on ImageNet (1.4M images, 1000 classes)</li> <li>Remove final classification layer</li> <li>Add new layer for your specific classes (e.g., 10 classes)</li> <li>Freeze pretrained layers, train only new layer</li> <li>Optionally fine-tune top layers with low learning rate</li> </ol> <p>Benefits:</p> <ul> <li>Requires 10-100x less data than training from scratch</li> <li>Converges faster (hours vs days/weeks)</li> <li>Often achieves better performance, especially with limited data</li> <li>Reduces computational costs significantly</li> </ul> <p>GCP Implementation: Vertex AI AutoML uses transfer learning automatically with Google's pretrained models. For custom training, TensorFlow Hub and Hugging Face provide pretrained models.</p>"},{"location":"concepts/#collaborative-filtering","title":"Collaborative Filtering","text":"<p>Recommendation technique that makes predictions based on preferences of similar users or items. User-based finds users with similar tastes; item-based finds similar items. Doesn't require explicit feature engineering, works from interaction patterns (ratings, purchases, clicks). Used by Netflix, Amazon, and Spotify for recommendations.</p>"},{"location":"data_split/","title":"Data Splitting Techniques for Machine Learning","text":""},{"location":"data_split/#overview","title":"Overview","text":"<p>Data splitting is the process of dividing a dataset into separate subsets for training, validating, and testing machine learning models. The splitting strategy chosen significantly impacts how well a model performs in production. This guide covers the most common splitting techniques, when to use them, and practical examples.</p>"},{"location":"data_split/#understanding-training-validation-and-test-datasets","title":"Understanding Training, Validation, and Test Datasets","text":"<p>Before diving into splitting techniques, it's essential to understand why data is split in the first place and what each subset is used for.</p> <p>When building machine learning models, all data is not used for the same purpose. Instead, data is divided into three distinct datasets, each serving a specific role in the model development lifecycle:</p> <ul> <li>Training dataset: Where the model learns patterns and relationships</li> <li>Validation dataset: Where the model is evaluated and tuned during development</li> <li>Test dataset: Where an honest, final assessment of model performance is obtained</li> </ul> <p>Think of it like learning a new skill: practice (training), receive feedback and adjust the approach (validation), then take a final test to assess true mastery (testing). Each dataset plays a crucial role in ensuring the model performs well on real-world data it has never seen before.</p> <p>The table below provides a detailed comparison of these three datasets:</p> Aspect Training dataset Validation dataset Test dataset Purpose Train the machine learning model Tune hyperparameters and make model architecture decisions Final, unbiased evaluation of the model What happens Model learns patterns, relationships, and features. Adjusts internal parameters (weights and biases) to minimize errors Evaluate model performance to:\u2022 Choose between models/algorithms\u2022 Tune hyperparameters\u2022 Implement early stopping\u2022 Make architecture decisions Evaluate completely unseen data once for honest assessment of production performance Student analogy Studying from textbooks and practice problems to learn concepts Taking practice tests to identify weak areas and adjust study strategies Taking the final exam once - represents real-world performance Frequency of use Model sees this data multiple times during training Can be evaluated multiple times; make model changes based on results Touch only once at the very end Model interaction Model learns from this data Model is evaluated (not trained) on this data Model is evaluated (never trained or tuned) on this data Typical size 60-80% of total data 10-20% of total data 10-20% of total data Key principle Used to fit model parameters Used to select and tune the model Used only for final performance measurement Critical rule Must be representative of the problem Must never be used for training Never use results to improve the model - prevents bias"},{"location":"data_split/#why-split-data","title":"Why split data?","text":"<p>1. Prevent overfitting</p> <ul> <li>Training and testing on the same data causes the model to memorize rather than learn</li> <li>The model will perform perfectly on training data but fail on new, unseen data</li> </ul> <p>2. Honest performance estimation</p> <ul> <li>Training accuracy is optimistic (model has seen the answers)</li> <li>Test accuracy reflects real-world performance</li> </ul> <p>3. Model selection and tuning</p> <ul> <li>Validation set allows comparison between models without biasing the final evaluation</li> <li>Prevents information leakage from the test set</li> </ul>"},{"location":"data_split/#common-misconception","title":"Common misconception","text":"<p>\u274c Wrong: Using test data to improve the model</p> <ul> <li>This defeats the purpose - training on the test set</li> <li>Performance estimate becomes biased</li> </ul> <p>\u2705 Right: Using validation data to improve the model, then using test data once for final evaluation</p>"},{"location":"data_split/#the-golden-rule","title":"The golden rule","text":"<p>Never touch the test set until model development is completely done. It's the only source of truth for how the model will perform in the real world.</p>"},{"location":"data_split/#1-random-split-simple-holdout","title":"1. Random split (simple holdout)","text":"<p>Brief description: Randomly divide data into training and test sets, typically 70-30 or 80-20.</p> <p>Intuition: Every data point has an equal chance of ending up in either set, like shuffling a deck of cards and dealing them into piles.</p> <p>When to use:</p> <ul> <li>Large datasets with independent observations</li> <li>No temporal dependencies</li> <li>Classes are roughly balanced</li> </ul> <p>Example:</p> <pre><code>Dataset: 1000 samples\nRandom shuffle \u2192 [847, 23, 991, 445, ...]\nTrain: First 800 samples (80%)\nTest: Last 200 samples (20%)\n</code></pre> <p>Pros: Simple, fast, works well for large datasets</p> <p>Cons: Variance in results, may not represent class distribution, wastes data (only trains once)</p>"},{"location":"data_split/#2-stratified-split","title":"2. Stratified split","text":"<p>Brief description: Random split that preserves the proportion of classes in both train and test sets.</p> <p>Intuition: If 30% of the data is class A, both train and test sets will have 30% class A. Like ensuring each hand in a card game has the same ratio of suits.</p> <p>Step-by-step example:</p> <p>Original dataset (100 samples): - class A: 70 samples (70%) - class B: 30 samples (30%)</p> <p>After 80-20 stratified split:</p> <p>Training set (80 samples): - class A: 56 samples (70%) - class B: 24 samples (30%)</p> <p>Test set (20 samples): - class A: 14 samples (70%) - class B: 6 samples (30%)</p> <p>When to use:</p> <ul> <li>Imbalanced datasets</li> <li>Classification problems</li> <li>Small datasets where class distribution matters</li> </ul> <p>Pros: Reduces variance, ensures representative test set</p> <p>Cons: Only works for classification, slightly more complex</p>"},{"location":"data_split/#3-k-fold-cross-validation","title":"3. K-fold cross-validation","text":"<p>Brief description: Split data into K equal parts (folds). Train on K-1 folds, test on 1 fold, and repeat K times so each fold serves as the test set once.</p> <p>Intuition: Everyone gets a turn being the test set. Like rotating team members through different roles.</p> <p>Step-by-step example (5-fold CV):</p> <p>Dataset: 100 samples divided into 5 folds of 20 samples each</p> <pre><code>Iteration 1: Train on [fold 2,3,4,5], test on [fold 1]\nIteration 2: Train on [fold 1,3,4,5], test on [fold 2]\nIteration 3: Train on [fold 1,2,4,5], test on [fold 3]\nIteration 4: Train on [fold 1,2,3,5], test on [fold 4]\nIteration 5: Train on [fold 1,2,3,4], test on [fold 5]\n\nFinal score: Average of all 5 test scores\n</code></pre> <p>When to use:</p> <ul> <li>Small to medium datasets</li> <li>Need robust performance estimate</li> <li>Comparing multiple models</li> </ul> <p>Pros: Uses all data for training and testing, reduces variance, robust estimate</p> <p>Cons: K times slower, still random (not for time series)</p> <p>Common K values: 5 or 10</p>"},{"location":"data_split/#4-stratified-k-fold-cross-validation","title":"4. Stratified K-fold cross-validation","text":"<p>Brief description: K-fold CV where each fold maintains the original class distribution.</p> <p>Intuition: Combines the benefits of stratification with cross-validation.</p> <p>Example:</p> <p>Dataset: 100 samples (70% class A, 30% class B) 5-fold stratified CV</p> <p>Each fold has 20 samples: - Fold 1: 14 class A, 6 class B - Fold 2: 14 class A, 6 class B - Fold 3: 14 class A, 6 class B - Fold 4: 14 class A, 6 class B - Fold 5: 14 class A, 6 class B</p> <p>Same rotation as K-fold CV</p> <p>When to use:</p> <ul> <li>Small, imbalanced classification datasets</li> <li>Gold standard for model evaluation on static data</li> </ul>"},{"location":"data_split/#5-groupcluster-split","title":"5. Group/cluster split","text":"<p>Brief description: Split data ensuring all samples from the same group stay together in either train or test.</p> <p>Intuition: When multiple photos of the same person exist, all photos of person A go into either train or test, never both. Prevents the model from memorizing individuals.</p> <p>Step-by-step example:</p> <p>Medical dataset: patient X-rays</p> <pre><code>Patient 1: [scan_a, scan_b, scan_c]\nPatient 2: [scan_d, scan_e]\nPatient 3: [scan_f, scan_g, scan_h]\nPatient 4: [scan_i]\n\nGroup split (by patient):\nTrain: patient 1, patient 3 \u2192 [scan_a,b,c,f,g,h]\nTest:  patient 2, patient 4 \u2192 [scan_d,e,i]\n\n\u274c WRONG (random): Training on scan_a,b from patient 1, testing on scan_c from patient 1\n</code></pre> <p>When to use:</p> <ul> <li>Multiple samples from same entity (patients, users, locations)</li> <li>Hierarchical data structures</li> <li>Prevent data leakage from correlated samples</li> </ul> <p>Pros: Prevents overfitting to specific groups, realistic evaluation</p> <p>Cons: May need larger datasets, unequal split sizes</p>"},{"location":"data_split/#6-time-series-cross-validation-walk-forwardrolling-window","title":"6. Time series cross-validation (walk-forward/rolling window)","text":"<p>Brief description: For time series, progressively train on expanding (or fixed) windows and test on the next period.</p> <p>Intuition: Simulate real-world deployment where retraining occurs periodically. Like using last month's data to predict next month, then updating.</p> <p>Step-by-step example (expanding window):</p> <p>Time series: [Jan, Feb, Mar, Apr, May, Jun, Jul, Aug]</p> <pre><code>Fold 1: Train [Jan,Feb,Mar] \u2192 test [Apr]\nFold 2: Train [Jan,Feb,Mar,Apr] \u2192 test [May]\nFold 3: Train [Jan,Feb,Mar,Apr,May] \u2192 test [Jun]\nFold 4: Train [Jan,Feb,Mar,Apr,May,Jun] \u2192 test [Jul]\nFold 5: Train [Jan,Feb,Mar,Apr,May,Jun,Jul] \u2192 test [Aug]\n</code></pre> <p>Rolling window variant (fixed size):</p> <pre><code>Fold 1: Train [Jan,Feb,Mar] \u2192 test [Apr]\nFold 2: Train [Feb,Mar,Apr] \u2192 test [May]\nFold 3: Train [Mar,Apr,May] \u2192 test [Jun]\nFold 4: Train [Apr,May,Jun] \u2192 test [Jul]\n</code></pre> <p>When to use:</p> <ul> <li>Time series forecasting</li> <li>Models need retraining over time</li> <li>Evaluating model degradation</li> </ul>"},{"location":"data_split/#7-leave-one-out-cross-validation-loocv","title":"7. Leave-one-out cross-validation (LOOCV)","text":"<p>Brief description: Extreme case of K-fold where K = number of samples. Train on N-1 samples, test on 1.</p> <p>Intuition: Each individual data point gets to be the test set.</p> <p>Example:</p> <p>Dataset: 50 samples</p> <pre><code>Iteration 1: Train on samples [2-50], test on [1]\nIteration 2: Train on samples [1,3-50], test on [2]\n...\nIteration 50: Train on samples [1-49], test on [50]\n</code></pre> <p>When to use:</p> <ul> <li>Very small datasets (&lt; 100 samples)</li> <li>Every data point is precious</li> </ul> <p>Pros: Maximum use of data</p> <p>Cons: Very computationally expensive, high variance</p>"},{"location":"data_split/#8-nested-cross-validation","title":"8. Nested cross-validation","text":"<p>Brief description: Two-level CV: outer loop for model evaluation, inner loop for hyperparameter tuning.</p> <p>Intuition: Prevents hyperparameter tuning from leaking information into the test set. The outer loop never sees the hyperparameter selection process.</p> <p>Step-by-step example:</p> <pre><code>Outer loop (5-fold for evaluation):\n  For each outer fold:\n    Inner loop (3-fold for tuning):\n      - Try different hyperparameters\n      - Select best hyperparameter\n    - Train final model with best params\n    - Evaluate on outer test fold\n\nResult: Unbiased performance estimate\n</code></pre> <p>When to use:</p> <ul> <li>Need both hyperparameter tuning AND unbiased evaluation</li> <li>Comparing models fairly</li> <li>Publication-quality results</li> </ul> <p>Pros: No information leakage, proper estimate</p> <p>Cons: Very computationally expensive</p>"},{"location":"data_split/#9-train-validation-test-split-three-way-split","title":"9. Train-validation-test split (three-way split)","text":"<p>Brief description: Divide data into three sets: train (60%), validation (20%), test (20%).</p> <p>Intuition: Training builds the model, validation tunes it, test evaluates it honestly. Like practice games, scrimmages, and championship matches.</p> <p>Example:</p> <pre><code>Dataset: 1000 samples\n\nTrain: 600 samples (build models)\nValidation: 200 samples (tune hyperparameters, select model)\nTest: 200 samples (final evaluation, touch ONCE)\n</code></pre> <p>When to use:</p> <ul> <li>Large datasets</li> <li>Deep learning (need validation for early stopping)</li> <li>When hyperparameters need tuning</li> </ul> <p>Workflow:</p> <ol> <li>Train multiple models on training set</li> <li>Evaluate on validation set, tune hyperparameters</li> <li>Select best model</li> <li>Final evaluation on test set (once only)</li> </ol>"},{"location":"data_split/#10-blockedpurged-cross-validation","title":"10. Blocked/purged cross-validation","text":"<p>Brief description: For time series, add gaps between train and test to prevent temporal leakage.</p> <p>Intuition: Real-world predictions need time to materialize. When predicting stock prices, same-day information cannot be used.</p> <p>Example:</p> <p>Time series with 2-day purge:</p> <pre><code>Fold 1: Train [Day 1-30] | PURGE [Day 31-32] | test [Day 33-35]\nFold 2: Train [Day 1-35] | PURGE [Day 36-37] | test [Day 38-40]\n\nThe purge period ensures no overlap between training features and test outcomes.\n</code></pre> <p>When to use:</p> <ul> <li>Financial time series</li> <li>Data where features take time to compute</li> <li>Preventing look-ahead bias</li> </ul>"},{"location":"data_split/#quick-selection-guide","title":"Quick selection guide","text":"Scenario Recommended technique Large dataset, independent samples Random split Imbalanced classes Stratified split or stratified K-fold Small dataset Stratified K-fold CV Time series data Temporal split or walk-forward CV Multiple samples per entity Group split Need hyperparameter tuning Train-validation-test or nested CV Financial/trading data Blocked/purged CV"},{"location":"ds_concepts/","title":"Data Science Concepts - Quick Reference","text":""},{"location":"ds_concepts/#overview","title":"Overview","text":"<p>This page provides concise explanations of fundamental data science techniques for data preparation, preprocessing, and transformation. These techniques should be applied before building ML models. Proper data preparation is critical for model performance.</p> <p>Related: See ML Concepts for machine learning algorithms, training, and evaluation techniques.</p>"},{"location":"ds_concepts/#1-feature-preprocessing","title":"1. Feature Preprocessing","text":"<p>Transformations applied to features before model training.</p>"},{"location":"ds_concepts/#feature-scaling-normalization","title":"Feature Scaling (Normalization)","text":"<p>Scales features to a fixed range, typically [0, 1] using: (x - min) / (max - min). Preserves original distribution shape and relationships. Sensitive to outliers as they determine min/max. Required for distance-based algorithms like KNN, SVM, neural networks. Critical preprocessing step before training models with gradient-based optimization algorithms.</p>"},{"location":"ds_concepts/#standardization-z-score-normalization","title":"Standardization (Z-score Normalization)","text":"<p>Transforms features to have mean=0 and standard deviation=1 using: (x - mean) / std. Assumes roughly normal distribution; less sensitive to outliers than min-max scaling. Results in unbounded range (typically -3 to +3 for normal data). Preferred for algorithms assuming normally distributed data.</p>"},{"location":"ds_concepts/#2-data-encoding-techniques","title":"2. Data Encoding Techniques","text":"<p>Methods to convert categorical data into numerical format for machine learning models.</p>"},{"location":"ds_concepts/#one-hot-encoding","title":"One Hot Encoding","text":"<p>Converts categorical variables into binary vectors where only one element is 1 (hot) and others are 0. Each category becomes a separate binary column. For example, colors [Red, Blue, Green] \u2192 Red: [1,0,0], Blue: [0,1,0], Green: [0,0,1]. Prevents model from assuming ordinal relationships between categories.</p>"},{"location":"ds_concepts/#label-encoding","title":"Label Encoding","text":"<p>Converts categorical labels into integers (0, 1, 2, ...) by assigning each unique category a number. Simple and memory-efficient but implies ordinal relationship between categories. Example: [Red, Blue, Green] \u2192 [0, 1, 2]. Best for ordinal data or tree-based models that don't assume ordering.</p>"},{"location":"ds_concepts/#ordinal-encoding","title":"Ordinal Encoding","text":"<p>Similar to label encoding but explicitly preserves meaningful order in categorical variables. User assigns specific integers reflecting the inherent ranking (e.g., Low=1, Medium=2, High=3). Appropriate for ordinal features like education level, satisfaction ratings, or size categories. Encodes domain knowledge about category ordering.</p>"},{"location":"ds_concepts/#target-encoding-mean-encoding","title":"Target Encoding (Mean Encoding)","text":"<p>Replaces each category with the mean of the target variable for that category. For example, if \"Blue\" items have average price $50, all \"Blue\" becomes 50. Risk of overfitting; use with cross-validation or smoothing techniques. Effective for high-cardinality categorical features.</p>"},{"location":"ds_concepts/#binary-encoding","title":"Binary Encoding","text":"<p>Converts categories to integers then to binary code, using fewer columns than one-hot encoding. Each category gets a binary representation (e.g., 5 categories need only 3 binary columns instead of 5). Reduces dimensionality while preserving uniqueness. Useful for high-cardinality features.</p>"},{"location":"ds_concepts/#3-missing-data-imputation","title":"3. Missing Data Imputation","text":"<p>Techniques to handle missing values in datasets.</p>"},{"location":"ds_concepts/#simple-imputation-meanmedianmode","title":"Simple Imputation (Mean/Median/Mode)","text":"<p>Replaces missing values with mean (continuous), median (skewed continuous), or mode (categorical) of the column. Fast and simple but ignores relationships between features and reduces variance. Mean is sensitive to outliers; median is robust. Mode for categorical data.</p>"},{"location":"ds_concepts/#forward-fill--backward-fill","title":"Forward Fill / Backward Fill","text":"<p>Time-series imputation that fills missing values with the previous (forward) or next (backward) observed value. Assumes temporal continuity and that values don't change drastically between time points. Forward fill: copies last known value; backward fill: copies next known value. Common in stock prices, sensor data.</p>"},{"location":"ds_concepts/#knn-imputation","title":"KNN Imputation","text":"<p>Fills missing values using K-Nearest Neighbors; finds k most similar samples and uses their average. Considers feature relationships unlike simple imputation, but computationally expensive for large datasets. Distance metric (usually Euclidean) determines similarity. Typical k values: 3-10.</p>"},{"location":"ds_concepts/#multiple-imputation","title":"Multiple Imputation","text":"<p>Statistical technique that creates multiple plausible imputed datasets, analyzes each, then pools results. Accounts for uncertainty in missing values unlike single imputation methods. MICE (Multivariate Imputation by Chained Equations) is popular implementation. Produces more accurate standard errors and confidence intervals.</p>"},{"location":"ds_concepts/#imputation-with-supervised-learning","title":"Imputation with Supervised Learning","text":"<p>Treats missing value prediction as a machine learning problem using other features as predictors. Train regression model (for continuous) or classification model (for categorical) on complete cases. More sophisticated than mean/median but risk of overfitting. Can capture complex feature relationships.</p>"},{"location":"ds_concepts/#4-sampling-techniques-for-imbalanced-data","title":"4. Sampling Techniques for Imbalanced Data","text":"<p>Methods to address class imbalance in classification problems.</p>"},{"location":"ds_concepts/#smote-synthetic-minority-over-sampling","title":"SMOTE (Synthetic Minority Over-sampling)","text":"<p>Creates synthetic samples for minority class by interpolating between existing minority samples and their nearest neighbors. Generates new points along line segments connecting k-nearest minority neighbors. Reduces overfitting compared to simple oversampling by creating diverse examples. Most popular resampling technique for imbalanced classification. Essential when dealing with class imbalance that affects evaluation metrics like precision and recall.</p>"},{"location":"ds_concepts/#random-oversampling","title":"Random Oversampling","text":"<p>Randomly duplicates examples from minority class until dataset is balanced. Simple but can lead to overfitting as model sees exact same examples multiple times. Works well combined with regularization or ensemble methods. Fast and easy to implement.</p>"},{"location":"ds_concepts/#random-undersampling","title":"Random Undersampling","text":"<p>Randomly removes examples from majority class to balance dataset. Risk of losing potentially important information from majority class. Useful when majority class has redundant information or dataset is very large. Often combined with oversampling (hybrid approach).</p>"},{"location":"ds_techniques/","title":"Data Science Techniques: Quick Decision Reference","text":""},{"location":"ds_techniques/#overview","title":"Overview","text":"<p>This document provides quick guidance on which technique to apply when facing common data science challenges. Each section includes the problem symptoms, when to use the technique, intuition behind it, and practical examples.</p>"},{"location":"ds_techniques/#1-downsampling--upsampling","title":"1. Downsampling / Upsampling","text":"<p>What it addresses: Class imbalance in classification problems</p>"},{"location":"ds_techniques/#when-to-downsample","title":"When to Downsample","text":"<p>Symptoms:</p> <ul> <li>95% class A, 5% class B distribution</li> <li>Model predicts majority class for everything</li> <li>High accuracy but poor performance on minority class</li> </ul> <p>Use when:</p> <ul> <li>Majority class has sufficient samples even after reduction (&gt;10,000 samples)</li> <li>Training time is a constraint</li> <li>Need to balance class representation quickly</li> </ul> <p>Intuition: Remove excess majority class samples so the model sees balanced examples during training.</p> <p>Example:</p> <ul> <li>Original: 95,000 negative reviews, 5,000 positive reviews</li> <li>Action: Randomly sample 5,000 negative reviews</li> <li>Result: 5,000 negative, 5,000 positive (balanced dataset)</li> </ul>"},{"location":"ds_techniques/#when-to-upsample","title":"When to Upsample","text":"<p>Use when:</p> <ul> <li>Minority class has very few samples (&lt;1,000)</li> <li>Cannot afford to lose majority class information</li> <li>Combined with techniques like SMOTE for synthetic samples</li> </ul> <p>Intuition: Create copies or synthetic versions of minority class to match majority class size.</p> <p>Example:</p> <ul> <li>Original: 10,000 fraud cases, 990,000 legitimate transactions</li> <li>Action: Use SMOTE to create synthetic fraud cases</li> <li>Result: 990,000 of each class</li> </ul>"},{"location":"ds_techniques/#2-learning-rate-adjustments","title":"2. Learning Rate Adjustments","text":"<p>What it addresses: Model convergence speed and training stability</p>"},{"location":"ds_techniques/#when-to-decrease-learning-rate","title":"When to Decrease Learning Rate","text":"<p>Symptoms:</p> <ul> <li>Loss oscillates wildly and doesn't decrease</li> <li>Training loss jumps around or diverges</li> <li>Model parameters become NaN</li> </ul> <p>Use when:</p> <ul> <li>Near the end of training (use learning rate schedulers)</li> <li>Training is unstable</li> <li>Found a good region but need fine-tuning</li> </ul> <p>Intuition: Smaller steps help settle into a minimum without overshooting.</p> <p>Example:</p> <ul> <li>Starting LR: 0.01 \u2192 Loss jumps between 0.5 and 2.0</li> <li>Reduced LR: 0.001 \u2192 Loss smoothly decreases from 0.5 to 0.3</li> <li>Typical values: 0.1 \u2192 0.01 \u2192 0.001 \u2192 0.0001</li> </ul>"},{"location":"ds_techniques/#when-to-increase-learning-rate","title":"When to Increase Learning Rate","text":"<p>Symptoms:</p> <ul> <li>Training progresses extremely slowly</li> <li>Loss decreases by tiny amounts each epoch</li> <li>Stuck in a plateau</li> </ul> <p>Use when:</p> <ul> <li>Early in training with a large dataset</li> <li>Model is underfitting</li> <li>Using learning rate warmup</li> </ul> <p>Intuition: Bigger steps help move faster through the parameter space.</p> <p>Example:</p> <ul> <li>Starting LR: 0.00001 \u2192 1000 epochs to reach decent performance</li> <li>Increased LR: 0.001 \u2192 50 epochs to reach same performance</li> </ul>"},{"location":"ds_techniques/#3-dropout-adjustments","title":"3. Dropout Adjustments","text":"<p>What it addresses: Overfitting by randomly disabling neurons during training</p>"},{"location":"ds_techniques/#when-to-addincrease-dropout","title":"When to Add/Increase Dropout","text":"<p>Symptoms:</p> <ul> <li>Training accuracy 99%, validation accuracy 75%</li> <li>Large gap between training and validation loss</li> <li>Model memorizes training data</li> </ul> <p>Use when:</p> <ul> <li>Limited training data available</li> <li>Model is overfitting</li> <li>Network has many parameters</li> </ul> <p>Intuition: Randomly \"dropping\" neurons forces the network to learn redundant representations and not rely on specific neurons.</p> <p>Example:</p> <ul> <li>Before: No dropout \u2192 Train acc: 98%, Val acc: 72%</li> <li>After: Add dropout=0.3 \u2192 Train acc: 90%, Val acc: 85%</li> <li>Typical dropout rates: 0.2 to 0.5</li> </ul>"},{"location":"ds_techniques/#when-to-decreaseremove-dropout","title":"When to Decrease/Remove Dropout","text":"<p>Symptoms:</p> <ul> <li>Both training and validation accuracy are low</li> <li>Model is underfitting</li> <li>Training is very slow to converge</li> </ul> <p>Use when:</p> <ul> <li>Abundant training data available</li> <li>Model capacity is already limited</li> <li>Underfitting is the problem</li> </ul> <p>Example:</p> <ul> <li>Before: dropout=0.5 \u2192 Train acc: 65%, Val acc: 63%</li> <li>After: dropout=0.2 \u2192 Train acc: 82%, Val acc: 80%</li> </ul>"},{"location":"ds_techniques/#4-data-imputation","title":"4. Data Imputation","text":"<p>What it addresses: Missing values in the dataset</p>"},{"location":"ds_techniques/#when-to-use-meanmedian-imputation","title":"When to Use Mean/Median Imputation","text":"<p>Use when:</p> <ul> <li>Data is Missing Completely at Random (MCAR)</li> <li>&lt;5% of values are missing</li> <li>Feature is numerical and roughly normally distributed</li> <li>Quick, simple solution needed</li> </ul> <p>Intuition: Replace missing values with the \"typical\" value for that feature.</p> <p>Example:</p> <pre><code>Age column: [25, 30, NaN, 35, NaN, 28]\nMean imputation: [25, 30, 31.5, 35, 31.5, 28]\nMedian imputation: [25, 30, 29, 35, 29, 28]\n</code></pre>"},{"location":"ds_techniques/#when-to-use-forwardbackward-fill","title":"When to Use Forward/Backward Fill","text":"<p>Use when:</p> <ul> <li>Data is time-series or sequential</li> <li>Missing values should inherit from nearby values</li> <li>Data has temporal dependencies</li> </ul> <p>Example:</p> <pre><code>Stock prices: [100, 102, NaN, NaN, 108]\nForward fill: [100, 102, 102, 102, 108]\n</code></pre>"},{"location":"ds_techniques/#when-to-use-predictive-imputation","title":"When to Use Predictive Imputation","text":"<p>Use when:</p> <ul> <li> <p>10% of data is missing</p> </li> <li>Missing data has patterns (Missing at Random - MAR)</li> <li>Computational resources available</li> <li>Feature is important for the model</li> </ul> <p>Intuition: Use other features to predict what the missing value should be.</p> <p>Example:</p> <ul> <li>Predict missing Age using: Gender, Income, Occupation</li> <li>Train a model on complete cases, predict missing cases</li> </ul>"},{"location":"ds_techniques/#when-to-create-missing-indicator","title":"When to Create Missing Indicator","text":"<p>Use when:</p> <ul> <li>Missingness itself is informative</li> <li>Missing Not at Random (MNAR)</li> </ul> <p>Example:</p> <pre><code>Income column: Many high earners leave it blank\nCreate: income_missing = [0, 0, 1, 0, 1]\n</code></pre>"},{"location":"ds_techniques/#5-dimensionality-reduction","title":"5. Dimensionality Reduction","text":"<p>What it addresses: Too many features causing computational or statistical problems</p>"},{"location":"ds_techniques/#when-to-use-pca-principal-component-analysis","title":"When to Use PCA (Principal Component Analysis)","text":"<p>Symptoms:</p> <ul> <li>Hundreds or thousands of features</li> <li>Features are highly correlated</li> <li>Training is extremely slow</li> <li>Multicollinearity in linear models</li> </ul> <p>Use when:</p> <ul> <li>Features are numerical and continuous</li> <li>Need to reduce features while preserving variance</li> <li>Interpretability is not critical</li> <li>Visualizing high-dimensional data</li> </ul> <p>Intuition: Find new axes that capture the most variance in the data, discard axes with little variance.</p> <p>Example:</p> <ul> <li>Original: 100 features \u2192 Model trains in 10 minutes</li> <li>After PCA: 20 components (capturing 95% variance) \u2192 Trains in 1 minute</li> <li>Use case: Image data (784 pixels) \u2192 50 principal components</li> </ul>"},{"location":"ds_techniques/#when-to-use-feature-selection","title":"When to Use Feature Selection","text":"<p>Use when:</p> <ul> <li>Need interpretable features</li> <li>Features are a mix of types (categorical, numerical)</li> <li>Domain knowledge is important</li> <li>Understanding feature importance is required</li> </ul> <p>Methods:</p> <ul> <li>Filter methods: Correlation, chi-square (fast, pre-modeling)</li> <li>Wrapper methods: Recursive Feature Elimination (slow, accurate)</li> <li>Embedded methods: L1 regularization/Lasso (automatic)</li> </ul> <p>Example:</p> <ul> <li>Original: 50 features</li> <li>After correlation filter: Remove 15 highly correlated features \u2192 35 features</li> <li>After RFE: Keep top 20 most predictive features</li> </ul>"},{"location":"ds_techniques/#when-to-use-t-sne-or-umap","title":"When to Use t-SNE or UMAP","text":"<p>Use when:</p> <ul> <li>Visualizing high-dimensional data in 2D/3D</li> <li>Exploring cluster structure</li> <li>NOT for model training (only visualization)</li> </ul> <p>Example:</p> <ul> <li>Use case: Visualize 768-dimensional text embeddings in 2D to see topic clusters</li> </ul>"},{"location":"ds_techniques/#6-addressing-overfitting","title":"6. Addressing Overfitting","text":"<p>What it is: Model performs well on training data but poorly on unseen data</p> <p>Symptoms:</p> <ul> <li>Training accuracy &gt;&gt; Validation accuracy (e.g., 95% vs 70%)</li> <li>Training loss keeps decreasing, validation loss increases</li> <li>Model has learned noise instead of signal</li> </ul>"},{"location":"ds_techniques/#solutions-in-order-of-ease","title":"Solutions (in order of ease)","text":""},{"location":"ds_techniques/#1-get-more-training-data","title":"1. Get More Training Data","text":"<ul> <li>Best solution when possible</li> <li>Before: 1,000 samples \u2192 Overfitting</li> <li>After: 10,000 samples \u2192 Better generalization</li> </ul>"},{"location":"ds_techniques/#2-simplify-the-model","title":"2. Simplify the Model","text":"<p>Reduce model complexity:</p> <ul> <li>Before: 5 hidden layers, 500 neurons each</li> <li>After: 2 hidden layers, 100 neurons each</li> </ul> <p>or</p> <ul> <li>Before: Decision tree depth=20</li> <li>After: Decision tree depth=5</li> </ul>"},{"location":"ds_techniques/#3-add-regularization","title":"3. Add Regularization","text":"<p>L1 (Lasso): Pushes some weights to exactly zero</p> <ul> <li>Use when: Feature selection desired</li> <li>Example: Ridge regression with alpha=0.1</li> </ul> <p>L2 (Ridge): Penalizes large weights</p> <ul> <li>Use when: Keep all features but constrain them</li> <li>Example: Linear regression with penalty='l2', C=1.0</li> </ul>"},{"location":"ds_techniques/#4-increase-dropout","title":"4. Increase Dropout","text":"<ul> <li>Before: dropout=0.2</li> <li>After: dropout=0.4</li> </ul>"},{"location":"ds_techniques/#5-early-stopping","title":"5. Early Stopping","text":"<ul> <li>Stop training when validation loss stops improving</li> <li>Monitor validation loss every epoch</li> <li>Stop if no improvement for 10 consecutive epochs</li> </ul>"},{"location":"ds_techniques/#6-data-augmentation-imagestext","title":"6. Data Augmentation (Images/Text)","text":"<ul> <li>Images: Rotate, flip, crop, adjust brightness</li> <li>Text: Synonym replacement, back-translation</li> </ul>"},{"location":"ds_techniques/#7-addressing-underfitting","title":"7. Addressing Underfitting","text":"<p>What it is: Model performs poorly on both training and validation data</p> <p>Symptoms:</p> <ul> <li>Both training and validation accuracy are low (e.g., 65% and 63%)</li> <li>Loss remains high and plateaus quickly</li> <li>Model is too simple to capture patterns</li> </ul>"},{"location":"ds_techniques/#solutions","title":"Solutions","text":""},{"location":"ds_techniques/#1-increase-model-complexity","title":"1. Increase Model Complexity","text":"<ul> <li>Before: Logistic regression</li> <li>After: Random forest or neural network</li> </ul> <p>or</p> <ul> <li>Before: 1 hidden layer, 10 neurons</li> <li>After: 3 hidden layers, 100 neurons each</li> </ul>"},{"location":"ds_techniques/#2-add-more-features","title":"2. Add More Features","text":"<p>Create new features or use feature engineering:</p> <p>Example: For house prices</p> <ul> <li>Original: [bedrooms, bathrooms]</li> <li>Enhanced: [bedrooms, bathrooms, bedrooms*bathrooms, total_sqft/bedrooms, age_of_house]</li> </ul>"},{"location":"ds_techniques/#3-reduce-regularization","title":"3. Reduce Regularization","text":"<ul> <li>Before: alpha=10 (very strong penalty)</li> <li>After: alpha=0.1 (weaker penalty)</li> </ul>"},{"location":"ds_techniques/#4-removereduce-dropout","title":"4. Remove/Reduce Dropout","text":"<ul> <li>Before: dropout=0.5</li> <li>After: dropout=0.1 or remove dropout</li> </ul>"},{"location":"ds_techniques/#5-train-longer","title":"5. Train Longer","text":"<ul> <li>Before: 10 epochs</li> <li>After: 100 epochs (if loss is still decreasing)</li> </ul>"},{"location":"ds_techniques/#6-increase-learning-rate","title":"6. Increase Learning Rate","text":"<ul> <li>Before: lr=0.0001 (too slow)</li> <li>After: lr=0.001 (faster learning)</li> </ul>"},{"location":"ds_techniques/#8-improving-recall-sensitivity","title":"8. Improving Recall (Sensitivity)","text":"<p>What it is: Proportion of actual positives correctly identified</p> <p>Use when: Cost of False Negatives is high (missing cancer, failing to detect fraud)</p>"},{"location":"ds_techniques/#symptoms-of-low-recall","title":"Symptoms of Low Recall","text":"<ul> <li>Model misses too many positive cases</li> <li>High precision but low recall</li> <li>False negatives are costly</li> </ul>"},{"location":"ds_techniques/#solutions_1","title":"Solutions","text":""},{"location":"ds_techniques/#1-lower-classification-threshold","title":"1. Lower Classification Threshold","text":"<ul> <li>Before: Threshold=0.5 \u2192 Recall=60%</li> <li>After: Threshold=0.3 \u2192 Recall=85% (more liberal predictions)</li> </ul>"},{"location":"ds_techniques/#2-adjust-class-weights","title":"2. Adjust Class Weights","text":"<pre><code>class_weight = {0: 1, 1: 5}  # Penalize missing class 1 more\n</code></pre>"},{"location":"ds_techniques/#3-upsample-minority-class","title":"3. Upsample Minority Class","text":"<p>Balance the training data to see more positive examples</p>"},{"location":"ds_techniques/#4-use-recall-oriented-metrics-during-training","title":"4. Use Recall-Oriented Metrics During Training","text":"<p>Optimize for F2-score (weighs recall higher than precision)</p>"},{"location":"ds_techniques/#5-ensemble-methods","title":"5. Ensemble Methods","text":"<p>Use voting classifier with multiple models to catch more positives</p> <p>Example:</p> <p>Medical diagnosis scenario:</p> <ul> <li>Before: Recall=65% \u2192 Missing 35% of disease cases</li> <li>After lowering threshold: Recall=90% \u2192 Missing only 10% of disease cases</li> <li>Trade-off: Precision may decrease (more false alarms)</li> </ul>"},{"location":"ds_techniques/#9-improving-precision","title":"9. Improving Precision","text":"<p>What it is: Proportion of positive predictions that are actually correct</p> <p>Use when: Cost of False Positives is high (spam filters, recommending bad products)</p>"},{"location":"ds_techniques/#symptoms-of-low-precision","title":"Symptoms of Low Precision","text":"<ul> <li>Too many false alarms</li> <li>High recall but low precision</li> <li>Users lose trust due to false positives</li> </ul>"},{"location":"ds_techniques/#solutions_2","title":"Solutions","text":""},{"location":"ds_techniques/#1-increase-classification-threshold","title":"1. Increase Classification Threshold","text":"<ul> <li>Before: Threshold=0.5 \u2192 Precision=60%</li> <li>After: Threshold=0.7 \u2192 Precision=85% (more conservative predictions)</li> </ul>"},{"location":"ds_techniques/#2-add-more-discriminative-features","title":"2. Add More Discriminative Features","text":"<p>Add features that better separate positive from negative cases</p>"},{"location":"ds_techniques/#3-use-ensemble-methods-with-voting","title":"3. Use Ensemble Methods with Voting","text":"<p>Only predict positive if multiple models agree</p>"},{"location":"ds_techniques/#4-better-feature-engineering","title":"4. Better Feature Engineering","text":"<p>Create features that specifically identify true positives</p>"},{"location":"ds_techniques/#5-clean-training-data","title":"5. Clean Training Data","text":"<p>Remove mislabeled positive examples</p> <p>Example:</p> <p>Email spam filter:</p> <ul> <li>Before: Precision=70% \u2192 30% of emails marked as spam are legitimate</li> <li>After raising threshold: Precision=95% \u2192 Only 5% false positives</li> <li>Trade-off: Recall may decrease (some spam gets through)</li> </ul>"},{"location":"ds_techniques/#10-quick-decision-flowchart","title":"10. Quick Decision Flowchart","text":""},{"location":"ds_techniques/#problem-model-accuracy-is-low","title":"Problem: Model Accuracy is Low","text":"<ul> <li>Training AND validation low? \u2192 UNDERFITTING (increase complexity, add features, reduce regularization)</li> <li>Training high, validation low? \u2192 OVERFITTING (add data, simplify model, add regularization)</li> </ul>"},{"location":"ds_techniques/#problem-class-imbalance","title":"Problem: Class Imbalance","text":"<ul> <li>Majority class &gt;10x minority? \u2192 Downsample majority or Upsample minority</li> <li>Use class weights in model training</li> </ul>"},{"location":"ds_techniques/#problem-too-many-features","title":"Problem: Too Many Features","text":"<ul> <li>Need interpretability? \u2192 Feature selection (RFE, Lasso)</li> <li>Don't need interpretability? \u2192 PCA</li> <li>Just visualizing? \u2192 t-SNE/UMAP</li> </ul>"},{"location":"ds_techniques/#problem-missing-data","title":"Problem: Missing Data","text":"<ul> <li>&lt;5% missing, numerical? \u2192 Mean/median imputation</li> <li>Time-series? \u2192 Forward/backward fill</li> <li> <p>10% missing? \u2192 Predictive imputation</p> </li> <li>Missingness is informative? \u2192 Add missing indicator</li> </ul>"},{"location":"ds_techniques/#problem-training-is-unstable","title":"Problem: Training is Unstable","text":"<ul> <li>Loss oscillating? \u2192 Decrease learning rate</li> <li>Loss barely moving? \u2192 Increase learning rate</li> </ul>"},{"location":"ds_techniques/#problem-need-higher-recall","title":"Problem: Need Higher Recall","text":"<ul> <li>Lower classification threshold</li> <li>Increase class weights for positive class</li> <li>Upsample minority class</li> </ul>"},{"location":"ds_techniques/#problem-need-higher-precision","title":"Problem: Need Higher Precision","text":"<ul> <li>Raise classification threshold</li> <li>Improve feature engineering</li> <li>Use ensemble voting (require agreement)</li> </ul>"},{"location":"gcp-aws-compare/","title":"GCP ML Services and AWS Comparables","text":"<p>Section Overview: This section maps all GCP ML services to their AWS equivalents, providing you with a comprehensive reference for service-to-service comparisons.</p> <p>Learning Objectives:</p> <ul> <li>Identify GCP ML services and their AWS counterparts</li> <li>Understand key architectural differences between GCP and AWS approaches</li> <li>Recognize when to use each service based on use case requirements</li> <li>Master key features and integration patterns</li> </ul>"},{"location":"gcp-aws-compare/#11-core-ml-services","title":"1.1 Core ML Services","text":""},{"location":"gcp-aws-compare/#vertex-ai-gcps-unified-ml-platform","title":"Vertex AI (GCP's Unified ML Platform)","text":"<p>AWS Equivalent: Amazon SageMaker</p> <p>Service Overview: Vertex AI is Google Cloud's unified, end-to-end ML platform that consolidates all ML services under one roof. It brings together what was previously AI Platform, AutoML, and other standalone services into a single cohesive experience.</p> <p>Key Components:</p> <ul> <li>Vertex AI Workbench: Managed Jupyter notebooks (AWS: SageMaker Studio/Notebook Instances)</li> <li>Vertex AI Training: Custom and distributed training (AWS: SageMaker Training Jobs)</li> <li>Vertex AI Prediction: Online and batch inference (AWS: SageMaker Endpoints/Batch Transform)</li> <li>Vertex AI Pipelines: ML workflow orchestration (AWS: SageMaker Pipelines)</li> <li>Vertex AI Feature Store: Centralized feature repository (AWS: SageMaker Feature Store)</li> <li>Vertex AI Model Monitoring: Drift and skew detection (AWS: SageMaker Model Monitor)</li> <li>Vertex AI Experiments: Tracking and comparison (AWS: SageMaker Experiments)</li> </ul> <p>Architectural Differences:</p> <p>When comparing GCP and AWS ML platforms, key differences include:</p> <ul> <li>GCP: Unified console and API surface across all ML tasks</li> <li>AWS: More modular approach with separate services for different ML workflows</li> <li>GCP: Strong integration with BigQuery for data processing</li> <li>AWS: Tight integration with S3 and broader AWS data ecosystem</li> </ul> <p>Key Features:</p> <ul> <li>Custom Training Jobs: Containerized training with support for TensorFlow, PyTorch, scikit-learn</li> <li>Hyperparameter Tuning: Automated hyperparameter optimization with configurable search strategies</li> <li>Distributed Training: Built-in support for data and model parallelism</li> <li>Model Registry: Version control and deployment management</li> <li>Metadata Management: Automatic tracking of artifacts, lineage, and experiments</li> </ul> <p>Common Use Cases:</p> <p>Vertex AI is ideal for these production ML scenarios:</p> <ul> <li>End-to-end ML workflows from data preparation to deployment</li> <li>Large-scale distributed training on TPUs or GPUs</li> <li>Production model serving with auto-scaling</li> <li>MLOps pipelines with continuous training and deployment</li> </ul> <p>Integration Points:</p> <ul> <li>BigQuery for data warehousing and feature engineering</li> <li>Cloud Storage for dataset and model artifact storage</li> <li>Cloud Build for CI/CD integration</li> <li>Cloud Monitoring for observability</li> </ul> <p>Terminology Mapping:</p> GCP Term AWS Term Description Training Job Training Job Model training execution Endpoint Endpoint Deployed model for predictions Model Model Trained ML model artifact Pipeline Pipeline ML workflow orchestration Custom Container BYOC User-provided container images <p>Practical Example - Training a Custom Model:</p> <p>GCP (Vertex AI): <pre><code>from google.cloud import aiplatform\n\naiplatform.init(project='my-project', location='us-central1')\n\n# Define custom training job\njob = aiplatform.CustomTrainingJob(\n    display_name='fraud-detection-training',\n    container_uri='gcr.io/my-project/fraud-model:latest',\n    requirements=['pandas', 'scikit-learn'],\n    model_serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest'\n)\n\n# Execute training\nmodel = job.run(\n    dataset=dataset,\n    model_display_name='fraud-detector-v1',\n    args=['--epochs=100', '--batch-size=32'],\n    replica_count=1,\n    machine_type='n1-standard-4',\n    accelerator_type='NVIDIA_TESLA_T4',\n    accelerator_count=1\n)\n</code></pre></p> <p>AWS (SageMaker): <pre><code>import sagemaker\nfrom sagemaker.estimator import Estimator\n\nsession = sagemaker.Session()\nrole = 'arn:aws:iam::123456789:role/SageMakerRole'\n\n# Define training job\nestimator = Estimator(\n    image_uri='123456789.dkr.ecr.us-east-1.amazonaws.com/fraud-model:latest',\n    role=role,\n    instance_count=1,\n    instance_type='ml.p3.2xlarge',\n    output_path='s3://my-bucket/models/',\n    sagemaker_session=session\n)\n\n# Execute training\nestimator.fit({'training': 's3://my-bucket/data/'})\n</code></pre></p> <p>Key Differences:</p> <p>Comparing the two implementations reveals important API design differences:</p> <ul> <li>GCP uses container_uri vs AWS uses image_uri</li> <li>GCP specifies accelerator separately vs AWS includes GPU in instance type</li> <li>GCP has unified init vs AWS requires session and role management</li> <li>GCP integrates model serving container at training time</li> </ul>"},{"location":"gcp-aws-compare/#ai-platform-legacy--deprecated","title":"AI Platform (Legacy) \u26a0\ufe0f DEPRECATED","text":"<p>Status: Discontinued January 31, 2025. All functionality migrated to Vertex AI.</p> <p>Migration Path: All AI Platform features are now available in Vertex AI. AI Platform is the predecessor to Vertex AI.</p> <p>Why This Matters: You may encounter migration scenarios where knowledge of both AI Platform and Vertex AI is relevant.</p>"},{"location":"gcp-aws-compare/#automl-vertex-ai-automl","title":"AutoML (Vertex AI AutoML)","text":"<p>AWS Equivalent: SageMaker Autopilot, SageMaker Canvas</p> <p>Service Overview: AutoML on Vertex AI provides automated machine learning capabilities for tabular, image, text, and video data. It automates feature engineering, model selection, hyperparameter tuning, and deployment with minimal code.</p> <p>Supported Data Types:</p> <p>AutoML supports four primary data modalities:</p> <ul> <li>AutoML Tables: Structured/tabular data (classification, regression)</li> <li>AutoML Vision: Image classification, object detection</li> <li>AutoML Natural Language: Text classification, entity extraction, sentiment analysis</li> <li>AutoML Video: Video classification, object tracking</li> </ul> <p>Key Features:</p> <p>AutoML provides these automation capabilities:</p> <ul> <li>Automatic data preprocessing and feature engineering</li> <li>Neural architecture search for optimal model design</li> <li>Built-in model explainability</li> <li>One-click deployment to production</li> </ul> <p>When to Use AutoML vs Custom Training:</p> <p>Use AutoML When:</p> <ul> <li>Quick proof-of-concept needed</li> <li>Limited ML expertise on the team</li> <li>Standard use cases (classification, regression, common CV/NLP tasks)</li> <li>Want to establish baseline model performance</li> <li>Dataset is well-structured and labeled</li> </ul> <p>Use Custom Training When:</p> <ul> <li>Specialized algorithms or architectures required</li> <li>Need fine-grained control over training process</li> <li>Custom loss functions or metrics needed</li> <li>Unusual data formats or preprocessing requirements</li> <li>Cost optimization through custom resource allocation</li> </ul> <p>AWS Comparison:</p> Feature Vertex AI AutoML SageMaker Autopilot SageMaker Canvas Target Users Data scientists &amp; developers Data scientists Business analysts Data Types Tabular, image, text, video Tabular only Tabular, image, text Code Required Minimal Python Python API No-code UI Explainability Built-in Built-in Built-in Deployment One-click API-based One-click <p>Practical Example - AutoML Tables:</p> <p>GCP: <pre><code>from google.cloud import aiplatform\n\n# Create dataset\ndataset = aiplatform.TabularDataset.create(\n    display_name='customer-churn',\n    gcs_source='gs://my-bucket/churn-data.csv'\n)\n\n# Train AutoML model\njob = aiplatform.AutoMLTabularTrainingJob(\n    display_name='churn-prediction',\n    optimization_prediction_type='classification',\n    optimization_objective='maximize-au-prc'\n)\n\nmodel = job.run(\n    dataset=dataset,\n    target_column='churned',\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    budget_milli_node_hours=1000,  # 1 hour\n    model_display_name='churn-model-v1'\n)\n</code></pre></p> <p>AWS (Autopilot): <pre><code>import sagemaker\nfrom sagemaker import AutoML\n\nsession = sagemaker.Session()\nautoml = AutoML(\n    role='arn:aws:iam::123:role/SageMakerRole',\n    target_attribute_name='churned',\n    output_path='s3://my-bucket/autopilot-output/',\n    sagemaker_session=session\n)\n\nautoml.fit(\n    inputs='s3://my-bucket/churn-data.csv',\n    job_name='churn-prediction',\n    wait=False\n)\n</code></pre></p>"},{"location":"gcp-aws-compare/#bigquery-ml","title":"BigQuery ML","text":"<p>AWS Equivalent: Amazon Redshift ML, Amazon Athena ML</p> <p>Service Overview: BigQuery ML enables data analysts to create and execute machine learning models using SQL queries directly within BigQuery. No need to export data or use separate ML tools.</p> <p>Supported Model Types:</p> <p>BigQuery ML supports a wide range of algorithms for different tasks:</p> <ul> <li>Linear Regression: Numeric prediction</li> <li>Logistic Regression: Binary/multiclass classification</li> <li>K-Means Clustering: Unsupervised grouping</li> <li>Matrix Factorization: Recommendation systems</li> <li>Time Series (ARIMA_PLUS): Forecasting</li> <li>Boosted Trees (XGBoost): Classification and regression</li> <li>Deep Neural Networks (DNN): Complex patterns</li> <li>AutoML Tables: Automated model selection</li> <li>Imported TensorFlow/ONNX models: Use pre-trained models</li> </ul> <p>Key Advantages:</p> <p>BigQuery ML offers significant benefits for data analysts:</p> <ul> <li>SQL-based: No Python/ML expertise required</li> <li>Data stays in place: No data movement overhead</li> <li>Scalable: Leverages BigQuery's processing power</li> <li>Fast iteration: Quick model training and evaluation</li> </ul> <p>Common Use Cases:</p> <p>Typical applications of BigQuery ML include:</p> <ul> <li>Customer segmentation with K-Means</li> <li>Churn prediction with classification models</li> <li>Product recommendations with matrix factorization</li> <li>Demand forecasting with ARIMA_PLUS</li> <li>Anomaly detection with clustering</li> </ul> <p>Practical Example - Customer Churn Prediction:</p> <p>GCP (BigQuery ML): <pre><code>-- Create and train model\nCREATE OR REPLACE MODEL `mydataset.churn_model`\nOPTIONS(\n  model_type='LOGISTIC_REG',\n  input_label_cols=['churned'],\n  auto_class_weights=TRUE,\n  data_split_method='AUTO_SPLIT',\n  max_iterations=50\n) AS\nSELECT\n  customer_age,\n  tenure_months,\n  monthly_charges,\n  total_charges,\n  contract_type,\n  payment_method,\n  churned\nFROM `mydataset.customer_data`;\n\n-- Evaluate model\nSELECT *\nFROM ML.EVALUATE(MODEL `mydataset.churn_model`);\n\n-- Make predictions\nSELECT\n  customer_id,\n  predicted_churned,\n  predicted_churned_probs[OFFSET(1)].prob as churn_probability\nFROM ML.PREDICT(MODEL `mydataset.churn_model`,\n  TABLE `mydataset.new_customers`\n);\n\n-- Feature importance\nSELECT *\nFROM ML.FEATURE_IMPORTANCE(MODEL `mydataset.churn_model`);\n</code></pre></p> <p>AWS (Redshift ML): <pre><code>-- Create and train model\nCREATE MODEL churn_model\nFROM customer_data\nTARGET churned\nFUNCTION predict_churn\nIAM_ROLE 'arn:aws:iam::123:role/RedshiftMLRole'\nSETTINGS (\n  S3_BUCKET 'my-bucket',\n  MAX_RUNTIME 5000\n);\n\n-- Make predictions\nSELECT\n  customer_id,\n  predict_churn(\n    customer_age,\n    tenure_months,\n    monthly_charges,\n    total_charges,\n    contract_type,\n    payment_method\n  ) as predicted_churn\nFROM new_customers;\n</code></pre></p> <p>Key Differences:</p> <p>When comparing BigQuery ML to Redshift ML:</p> <ul> <li>BigQuery ML has richer model options (ARIMA, Matrix Factorization, DNN)</li> <li>BigQuery ML provides built-in model evaluation and feature importance</li> <li>Redshift ML relies more on SageMaker Autopilot in the background</li> <li>BigQuery ML syntax is more ML-focused, Redshift ML simpler</li> </ul> <p>Key Consideration: Choose BigQuery ML for data analysts, SQL-based teams, and data already in BigQuery. Choose Vertex AI for complex ML workflows, Python-based teams, and when you need MLOps capabilities.</p>"},{"location":"gcp-aws-compare/#12-pre-trained-ai-apis","title":"1.2 Pre-trained AI APIs","text":""},{"location":"gcp-aws-compare/#vision-ai","title":"Vision AI","text":"<p>AWS Equivalent: Amazon Rekognition</p> <p>Capabilities:</p> <p>Vision AI provides these pre-trained image analysis features:</p> <ul> <li>Image labeling and classification</li> <li>Face detection and recognition</li> <li>Optical Character Recognition (OCR)</li> <li>Object detection and tracking</li> <li>Explicit content detection</li> <li>Logo detection</li> <li>Landmark recognition</li> </ul> <p>When to Use: Need quick image analysis without training custom models</p>"},{"location":"gcp-aws-compare/#natural-language-ai","title":"Natural Language AI","text":"<p>AWS Equivalent: Amazon Comprehend</p> <p>Capabilities:</p> <p>Natural Language AI offers these text analysis capabilities:</p> <ul> <li>Entity extraction (people, places, organizations)</li> <li>Sentiment analysis</li> <li>Syntax analysis</li> <li>Content classification</li> <li>Entity sentiment</li> </ul> <p>When to Use: Text analysis tasks with standard requirements</p>"},{"location":"gcp-aws-compare/#translation-api","title":"Translation API","text":"<p>AWS Equivalent: Amazon Translate</p> <p>Capabilities:</p> <p>Translation API supports:</p> <ul> <li>Text translation across 100+ languages</li> <li>Batch and real-time translation</li> <li>Custom glossaries</li> </ul>"},{"location":"gcp-aws-compare/#speech-to-text","title":"Speech-to-Text","text":"<p>AWS Equivalent: Amazon Transcribe</p> <p>Capabilities:</p> <p>Speech-to-Text provides:</p> <ul> <li>Audio transcription</li> <li>Real-time streaming recognition</li> <li>Speaker diarization</li> <li>Profanity filtering</li> <li>Custom vocabulary</li> </ul>"},{"location":"gcp-aws-compare/#text-to-speech","title":"Text-to-Speech","text":"<p>AWS Equivalent: Amazon Polly</p> <p>Capabilities:</p> <p>Text-to-Speech enables:</p> <ul> <li>Natural-sounding speech synthesis</li> <li>Multiple voices and languages</li> <li>Custom voice creation (in preview)</li> <li>SSML support</li> </ul>"},{"location":"gcp-aws-compare/#13-ml-implementation-patterns--best-practices","title":"1.3 ML Implementation Patterns &amp; Best Practices","text":""},{"location":"gcp-aws-compare/#model-selection-by-problem-type","title":"Model Selection by Problem Type","text":"Problem Type Algorithms GCP Service AWS Service Binary Classification Logistic Regression, Random Forest, XGBoost, Neural Networks Vertex AI AutoML, BigQuery ML SageMaker Autopilot Multi-class Classification Softmax Regression, Random Forest, Neural Networks Vertex AI AutoML, BigQuery ML SageMaker Built-in Algorithms Regression Linear Regression, Random Forest, XGBoost, Neural Networks Vertex AI AutoML, BigQuery ML SageMaker Autopilot Time Series ARIMA, Prophet, LSTM BigQuery ML ARIMA_PLUS, Vertex AI Amazon Forecast, SageMaker Clustering K-Means, DBSCAN BigQuery ML K-Means SageMaker K-Means Recommendation Matrix Factorization, Neural CF BigQuery ML, Retail API Amazon Personalize"},{"location":"gcp-aws-compare/#bigquery-ml-evaluation-metrics","title":"BigQuery ML Evaluation Metrics","text":"<p>For classification models in BigQuery ML:</p> <pre><code>SELECT *\nFROM ML.EVALUATE(MODEL `project.dataset.classification_model`)\n\n-- Returns:\n-- - precision: TP / (TP + FP)\n-- - recall: TP / (TP + FN)\n-- - accuracy: (TP + TN) / Total\n-- - f1_score: 2 * (precision * recall) / (precision + recall)\n-- - log_loss: -mean(y * log(p) + (1-y) * log(1-p))\n-- - roc_auc: Area under ROC curve\n</code></pre>"},{"location":"gcp-aws-compare/#bias-and-fairness-detection","title":"Bias and Fairness Detection","text":"<p>Vertex AI provides tools to analyze fairness metrics by demographic groups:</p> <pre><code>-- Check for disparate impact\nSELECT\n  gender,\n  race,\n  COUNT(*) as total,\n  AVG(CAST(prediction AS FLOAT64)) as approval_rate,\n  STDDEV(CAST(prediction AS FLOAT64)) as approval_stddev\nFROM `project.dataset.predictions`\nGROUP BY gender, race\n\n-- Disparate Impact Ratio = (Approval Rate for Protected Group) / (Approval Rate for Reference Group)\n-- Should be &gt; 0.8 to avoid discrimination\n</code></pre> <p>Key Metrics:</p> <ul> <li>Demographic parity</li> <li>Equal opportunity</li> <li>Equalized odds</li> </ul>"},{"location":"gcp-aws-compare/#explainability-with-vertex-ai","title":"Explainability with Vertex AI","text":"<p>Integrated Gradients (default for neural networks):</p> <pre><code>from google.cloud import aiplatform\n\n# Configure explanations\nexplanation_metadata = {\n    'inputs': {\n        'features': {\n            'input_tensor_name': 'input_1',\n            'encoding': 'IDENTITY',\n            'modality': 'numeric',\n            'index_feature_mapping': ['age', 'income', 'credit_score']\n        }\n    }\n}\n\n# Deploy with explanations\nmodel.deploy(\n    endpoint=endpoint,\n    explanation_spec={\n        'metadata': explanation_metadata,\n        'parameters': explanation_parameters\n    }\n)\n\n# Get predictions with explanations\ninstances = [{'age': 35, 'income': 75000, 'credit_score': 720}]\nresponse = endpoint.explain(instances=instances)\n</code></pre>"},{"location":"gcp-aws-compare/#bigquery-ml-feature-importance","title":"BigQuery ML Feature Importance","text":"<pre><code>-- Global feature importance\nSELECT *\nFROM ML.FEATURE_IMPORTANCE(MODEL `project.dataset.my_model`)\nORDER BY importance_weight DESC\n</code></pre>"},{"location":"gcp-aws-compare/#hyperparameter-tuning-strategies","title":"Hyperparameter Tuning Strategies","text":"<ul> <li>Grid Search: Exhaustive search over parameter grid</li> <li>Random Search: Random sampling from parameter space</li> <li>Bayesian Optimization: Uses previous results to inform next trials (most efficient)</li> </ul> <p>Best Practices:</p> <ul> <li>Use log scale for learning rates</li> <li>Use linear scale for layer counts</li> <li>Enable early stopping to save compute costs</li> <li>Bayesian optimization requires fewer trials than grid search</li> </ul>"},{"location":"gcp-aws-compare/#training-serving-skew-prevention","title":"Training-Serving Skew Prevention","text":"<p>Use TensorFlow Transform (TFT) to guarantee training-serving consistency:</p> <pre><code>import tensorflow_transform as tft\n\ndef preprocessing_fn(inputs):\n    \"\"\"Shared preprocessing function for training and serving\"\"\"\n    outputs = {}\n\n    # Normalize (uses full-pass statistics)\n    outputs['normalized_feature'] = tft.scale_to_z_score(inputs['feature'])\n\n    # Vocabulary (computed once on training data)\n    outputs['categorical_encoded'] = tft.compute_and_apply_vocabulary(\n        inputs['category'],\n        top_k=1000\n    )\n\n    return outputs\n</code></pre> <p>Common Causes of Skew:</p> <ul> <li>Different preprocessing in training vs serving</li> <li>Missing features in serving</li> <li>Data drift over time</li> <li>Timezone differences</li> <li>Encoding issues</li> </ul> <p>Key Takeaways:</p> <ul> <li>TFT guarantees training-serving consistency</li> <li>Prevention is better than detection</li> <li>Monitor for skew using training data baseline</li> <li>Use same preprocessing code for both training and serving</li> </ul>"},{"location":"gcp-aws-compare/#14-data-processing--storage-services","title":"1.4 Data Processing &amp; Storage Services","text":""},{"location":"gcp-aws-compare/#bigquery","title":"BigQuery","text":"<p>AWS Equivalent: Amazon Redshift, Amazon Athena</p> <p>Service Overview: BigQuery is Google's fully managed, serverless, petabyte-scale data warehouse designed for analytics and ML workloads. It's central to most GCP ML architectures.</p> <p>Key Features:</p> <p>BigQuery provides these capabilities for ML workloads:</p> <ul> <li>Serverless: No infrastructure management</li> <li>Fast queries: Parallel processing across thousands of nodes</li> <li>Streaming inserts: Real-time data ingestion</li> <li>ML integration: Native BigQuery ML support</li> <li>Cost-effective: Pay only for queries run and storage used</li> </ul> <p>Architectural Differences from Redshift:</p> <p>Comparing BigQuery to Redshift reveals fundamental design differences:</p> <ul> <li>GCP (BigQuery): Fully serverless, automatic scaling</li> <li>AWS (Redshift): Cluster-based, manual scaling required</li> <li>GCP: Separates storage and compute billing</li> <li>AWS: Combined cluster pricing</li> </ul> <p>Key Features:</p> <ul> <li>Partitioning: Table partitioning by date/timestamp for query optimization</li> <li>Clustering: Group related data together for better performance</li> <li>Materialized Views: Pre-computed query results for faster access</li> <li>Federated Queries: Query external data sources (Cloud Storage, Cloud SQL)</li> <li>Data Transfer Service: Automated data imports from SaaS applications</li> </ul> <p>Best Practices for ML Workloads:</p> <ul> <li>Partition training data by date for efficient querying</li> <li>Use clustering on high-cardinality columns used in WHERE clauses</li> <li>Create materialized views for frequently used feature engineering queries</li> <li>Leverage slot reservations for predictable ML pipeline costs</li> </ul> <p>Practical Example - Feature Engineering: <pre><code>-- Create partitioned table for ML features\nCREATE OR REPLACE TABLE `project.dataset.customer_features`\nPARTITION BY DATE(feature_timestamp)\nCLUSTER BY customer_id\nAS\nSELECT\n  customer_id,\n  feature_timestamp,\n  -- Aggregated features\n  AVG(transaction_amount) OVER (\n    PARTITION BY customer_id\n    ORDER BY feature_timestamp\n    ROWS BETWEEN 30 PRECEDING AND CURRENT ROW\n  ) as avg_30day_spend,\n  COUNT(*) OVER (\n    PARTITION BY customer_id\n    ORDER BY feature_timestamp\n    RANGE BETWEEN INTERVAL 7 DAY PRECEDING AND CURRENT ROW\n  ) as transactions_last_7days,\n  -- One-hot encoding\n  IF(customer_segment = 'premium', 1, 0) as is_premium\nFROM `project.dataset.transactions`;\n\n-- Create materialized view for common features\nCREATE MATERIALIZED VIEW `project.dataset.daily_customer_stats`\nAS\nSELECT\n  DATE(transaction_time) as date,\n  customer_id,\n  COUNT(*) as transaction_count,\n  SUM(amount) as total_spend,\n  AVG(amount) as avg_transaction\nFROM `project.dataset.transactions`\nGROUP BY date, customer_id;\n</code></pre></p>"},{"location":"gcp-aws-compare/#cloud-storage","title":"Cloud Storage","text":"<p>AWS Equivalent: Amazon S3</p> <p>Service Overview: Cloud Storage is Google's object storage service for unstructured data. It's the primary storage for datasets, model artifacts, and pipeline outputs.</p> <p>Storage Classes:</p> <p>Cloud Storage offers four storage tiers optimized for different access patterns:</p> <ul> <li>Standard: Frequently accessed data (AWS: S3 Standard)</li> <li>Nearline: Monthly access (AWS: S3 Standard-IA)</li> <li>Coldline: Quarterly access (AWS: S3 Glacier Instant Retrieval)</li> <li>Archive: Annual access (AWS: S3 Glacier Deep Archive)</li> </ul> <p>Key Features for ML:</p> <p>Cloud Storage provides these ML-specific features:</p> <ul> <li>Autoclass: Automatic lifecycle management</li> <li>Uniform bucket-level access: Simplified IAM</li> <li>Object versioning: Track model artifact versions</li> <li>Signed URLs: Temporary access for secure data sharing</li> </ul> <p>Best Practices:</p> <ul> <li>Use Standard class for training datasets and active models</li> <li>Use Nearline/Coldline for archived experiments and old model versions</li> <li>Organize with consistent naming: gs://bucket/datasets/{train,val,test}/</li> <li>Enable versioning for model artifacts</li> </ul> <p>Example - Organizing ML Assets: <pre><code>gs://ml-project-bucket/\n\u251c\u2500\u2500 datasets/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u2502   \u2514\u2500\u2500 data-2024-01-15.csv\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 validation/\n\u2502   \u2502   \u2514\u2500\u2500 test/\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 fraud-detector/\n\u2502   \u2502   \u251c\u2500\u2500 v1/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 model.pkl\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 metadata.json\n\u2502   \u2502   \u2514\u2500\u2500 v2/\n\u2502   \u2514\u2500\u2500 churn-predictor/\n\u251c\u2500\u2500 experiments/\n\u2502   \u2514\u2500\u2500 experiment-2024-01-15/\n\u2502       \u251c\u2500\u2500 hyperparameters.json\n\u2502       \u2514\u2500\u2500 metrics.json\n\u2514\u2500\u2500 pipelines/\n    \u2514\u2500\u2500 training-pipeline-v1/\n</code></pre></p>"},{"location":"gcp-aws-compare/#dataflow","title":"Dataflow","text":"<p>AWS Equivalent: AWS Glue (batch), Amazon Kinesis Data Analytics/Apache Flink (streaming)</p> <p>Service Overview: Dataflow is Google's fully managed service for stream and batch data processing based on Apache Beam. It's crucial for data preprocessing, feature engineering, and ETL in ML pipelines.</p> <p>Key Capabilities:</p> <p>Dataflow provides these powerful features:</p> <ul> <li>Unified programming: Same code for batch and streaming</li> <li>Auto-scaling: Dynamically adjust workers based on data volume</li> <li>Exactly-once processing: Guarantees for streaming data</li> <li>Windowing: Time-based aggregations for streaming data</li> </ul> <p>Common ML Use Cases:</p> <p>Dataflow is ideal for these ML scenarios:</p> <ul> <li>Large-scale data preprocessing before training</li> <li>Real-time feature computation from streaming events</li> <li>Batch prediction result processing</li> <li>Data validation with TensorFlow Data Validation (TFDV)</li> <li>Feature transformation with TensorFlow Transform (TFT)</li> </ul> <p>Architectural Pattern - Streaming Feature Engineering: <pre><code>Pub/Sub \u2192 Dataflow \u2192 Feature Store (online)\n                  \u2192 BigQuery (offline)\n</code></pre></p> <p>Practical Example - Feature Engineering Pipeline: <pre><code>import apache_beam as beam\nfrom apache_beam.options.pipeline_options import PipelineOptions\n\ndef compute_features(element):\n    \"\"\"Compute features from raw event\"\"\"\n    return {\n        'user_id': element['user_id'],\n        'avg_session_duration': element['session_duration'] / element['page_views'],\n        'days_since_last_visit': (datetime.now() - element['last_visit']).days,\n        'total_purchases': element['purchase_count']\n    }\n\n# Define pipeline\noptions = PipelineOptions(\n    runner='DataflowRunner',\n    project='my-project',\n    region='us-central1',\n    temp_location='gs://my-bucket/temp'\n)\n\nwith beam.Pipeline(options=options) as pipeline:\n    (pipeline\n     | 'Read from Pub/Sub' &gt;&gt; beam.io.ReadFromPubSub(\n         subscription='projects/my-project/subscriptions/user-events')\n     | 'Parse JSON' &gt;&gt; beam.Map(json.loads)\n     | 'Compute Features' &gt;&gt; beam.Map(compute_features)\n     | 'Write to BigQuery' &gt;&gt; beam.io.WriteToBigQuery(\n         'my-project:dataset.user_features',\n         schema='user_id:STRING,avg_session_duration:FLOAT,...',\n         write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n     ))\n</code></pre></p>"},{"location":"gcp-aws-compare/#cloud-data-fusion","title":"Cloud Data Fusion","text":"<p>AWS Equivalent: AWS Glue Studio, AWS Glue DataBrew</p> <p>Service Overview: Cloud Data Fusion is a fully managed, cloud-native data integration service based on the open-source CDAP project. It provides a visual, code-free environment for building ETL/ELT pipelines, making it ideal for business users and data engineers who prefer GUI-based workflows.</p> <p>Key Capabilities:</p> <p>Data Fusion provides these features:</p> <ul> <li>Visual pipeline builder: Drag-and-drop interface for data transformations</li> <li>Pre-built connectors: 150+ connectors for databases, SaaS apps, file systems</li> <li>Data quality checks: Built-in validation and profiling</li> <li>Pipeline templates: Reusable pipeline patterns</li> <li>Hybrid/multi-cloud: Can connect on-prem and multi-cloud sources</li> </ul> <p>Data Fusion vs Dataflow:</p> Aspect Cloud Data Fusion Dataflow Interface Visual, no-code GUI Code-based (Apache Beam) User Persona Data analysts, business users Data engineers, developers Complexity Simple to moderate ETL Complex streaming/batch processing Customization Limited to available plugins Full programmatic control ML Use Case Data preparation, cleansing Feature engineering, real-time ML Best For Structured data integration Custom ML preprocessing pipelines <p>Common ML Use Cases:</p> <p>Data Fusion is ideal for these ML scenarios:</p> <ul> <li>Consolidating data from multiple sources for ML training datasets</li> <li>Data cleansing and quality checks before model training</li> <li>Scheduled batch data ingestion from databases to BigQuery</li> <li>Simple feature engineering (aggregations, joins, filtering)</li> <li>Data preparation for AutoML or BigQuery ML</li> </ul> <p>Architectural Pattern - ML Data Preparation: <pre><code>Multiple Sources (DB, SaaS, Files)\n           \u2193\n    Cloud Data Fusion (ETL)\n           \u2193\n      BigQuery (ML Training Data)\n           \u2193\n    Vertex AI / BigQuery ML\n</code></pre></p> <p>Practical Example - Customer Data Integration Pipeline:</p> <p>Via UI (visual pipeline): 1. Source: MySQL database connector \u2192 customer_transactions table 2. Transform:    - Joiner \u2192 Join with customer_profiles (BigQuery)    - Group By \u2192 Aggregate purchases by customer_id    - Wrangler \u2192 Clean null values, format dates 3. Sink: BigQuery \u2192 ml_datasets.customer_features</p> <p>When to Choose:</p> <ul> <li>Use Data Fusion when: Simple ETL, visual interface preferred, multiple source connectors needed, business users involved</li> <li>Use Dataflow when: Complex transformations, streaming data, TensorFlow Transform integration, full code control needed</li> </ul>"},{"location":"gcp-aws-compare/#15-compute-services","title":"1.5 Compute Services","text":""},{"location":"gcp-aws-compare/#compute-engine","title":"Compute Engine","text":"<p>AWS Equivalent: Amazon EC2</p> <p>Use for ML:</p> <p>Compute Engine is suitable for these ML scenarios:</p> <ul> <li>Custom training environments with specific configurations</li> <li>Long-running training jobs requiring manual management</li> <li>Legacy ML code migration from on-premises</li> </ul> <p>When to Use vs Vertex AI Training:</p> <ul> <li>Use Compute Engine: Maximum control, custom OS/software, non-standard ML frameworks</li> <li>Use Vertex AI Training: Managed ML workflows, easier scaling, built-in integrations</li> </ul>"},{"location":"gcp-aws-compare/#google-kubernetes-engine-gke","title":"Google Kubernetes Engine (GKE)","text":"<p>AWS Equivalent: Amazon EKS</p> <p>Use for ML:</p> <p>GKE excels at these ML use cases:</p> <ul> <li>Deploying custom model serving infrastructure</li> <li>Running Kubeflow for ML pipelines</li> <li>Multi-tenant ML platforms</li> <li>Batch inference jobs with complex orchestration</li> </ul>"},{"location":"gcp-aws-compare/#cloud-run","title":"Cloud Run","text":"<p>AWS Equivalent: AWS Fargate, AWS App Runner</p> <p>Use for ML:</p> <p>Cloud Run is ideal for:</p> <ul> <li>Serverless model serving</li> <li>Lightweight prediction APIs</li> <li>Batch inference jobs triggered by events</li> <li>Cost-effective for sporadic prediction workloads</li> </ul>"},{"location":"gcp-aws-compare/#16-mlops--development-tools","title":"1.6 MLOps &amp; Development Tools","text":""},{"location":"gcp-aws-compare/#cloud-build","title":"Cloud Build","text":"<p>AWS Equivalent: AWS CodeBuild</p> <p>Use for ML:</p> <p>Cloud Build supports these ML workflows:</p> <ul> <li>Building custom training containers</li> <li>CI/CD for ML code</li> <li>Automated model retraining pipelines</li> <li>Container image creation for Vertex AI</li> </ul>"},{"location":"gcp-aws-compare/#artifact-registry","title":"Artifact Registry","text":"<p>AWS Equivalent: Amazon ECR</p> <p>Use for ML:</p> <p>Artifact Registry provides:</p> <ul> <li>Store Docker images for training and serving</li> <li>Manage ML model artifacts</li> <li>Version control for containers</li> <li>Vulnerability scanning for security</li> </ul>"},{"location":"gcp-aws-compare/#17-monitoring--management","title":"1.7 Monitoring &amp; Management","text":""},{"location":"gcp-aws-compare/#cloud-monitoring-formerly-stackdriver","title":"Cloud Monitoring (formerly Stackdriver)","text":"<p>AWS Equivalent: Amazon CloudWatch</p> <p>Key Metrics for ML:</p> <p>Monitor these critical ML metrics:</p> <ul> <li>Training job progress and resource utilization</li> <li>Prediction latency and throughput</li> <li>Model endpoint health</li> <li>Custom metrics (model accuracy, drift scores)</li> </ul>"},{"location":"gcp-aws-compare/#cloud-logging","title":"Cloud Logging","text":"<p>AWS Equivalent: Amazon CloudWatch Logs</p> <p>ML Logging Best Practices:</p> <p>Follow these logging practices for ML systems:</p> <ul> <li>Log training hyperparameters and final metrics</li> <li>Capture prediction requests for debugging</li> <li>Track data quality issues</li> <li>Monitor feature values for anomalies</li> </ul>"},{"location":"gcp-aws-compare/#18-security--governance","title":"1.8 Security &amp; Governance","text":""},{"location":"gcp-aws-compare/#iam-identity-and-access-management","title":"IAM (Identity and Access Management)","text":"<p>AWS Equivalent: AWS IAM</p> <p>Key Roles for ML:</p> <ul> <li>roles/aiplatform.user: Use Vertex AI resources</li> <li>roles/aiplatform.admin: Manage Vertex AI resources</li> <li>roles/bigquery.dataViewer: Read BigQuery data</li> <li>roles/bigquery.jobUser: Run BigQuery jobs</li> <li>roles/storage.objectViewer: Read Cloud Storage objects</li> <li>roles/ml.developer: Full ML development access</li> </ul>"},{"location":"gcp-aws-compare/#vpc-service-controls","title":"VPC Service Controls","text":"<p>AWS Equivalent: AWS VPC Endpoints, AWS PrivateLink</p> <p>Purpose: Create security perimeters around GCP resources to prevent data exfiltration</p> <p>ML Use Cases:</p> <p>VPC Service Controls protect ML resources:</p> <ul> <li>Protect sensitive training data in BigQuery</li> <li>Secure model artifacts in Cloud Storage</li> <li>Isolate Vertex AI resources within network perimeter</li> </ul>"},{"location":"gcp-aws-compare/#cloud-kms-key-management-service","title":"Cloud KMS (Key Management Service)","text":"<p>AWS Equivalent: AWS KMS</p> <p>Use for ML:</p> <p>Cloud KMS secures ML data and artifacts:</p> <ul> <li>Encrypt training data at rest</li> <li>Protect model artifacts</li> <li>Encrypt BigQuery datasets</li> <li>Secure sensitive feature data</li> </ul>"},{"location":"glossary/","title":"GCP ML Terminology Glossary","text":"<p>A comprehensive reference guide to key terms, services, and acronyms used in Google Cloud Machine Learning, with AWS equivalents where applicable.</p>"},{"location":"glossary/#table-of-contents","title":"Table of Contents","text":"<ul> <li>A | B | C | D | E | F | G | H | I | K | L | M | N | O | P | R | S | T | U | V | W</li> </ul>"},{"location":"glossary/#a","title":"A","text":""},{"location":"glossary/#automl","title":"AutoML","text":"Attribute Details Description Automated machine learning service that automates model selection, hyperparameter tuning, and deployment AWS Equivalent SageMaker Autopilot Use Case Quick ML development without deep ML expertise"},{"location":"glossary/#auc-area-under-curve","title":"AUC (Area Under Curve)","text":"Attribute Details Description Performance metric measuring the area under the ROC curve Range 0.0 to 1.0 (higher is better) Use Case Evaluating binary classification models"},{"location":"glossary/#b","title":"B","text":""},{"location":"glossary/#backward-fill","title":"Backward Fill","text":"Attribute Details Description Time-series imputation that fills missing values with the next observed value Use Case Sequential data where missing values should inherit from future observations Example Stock prices: [100, NaN, NaN, 108] \u2192 [100, 108, 108, 108]"},{"location":"glossary/#c","title":"C","text":""},{"location":"glossary/#class-imbalance","title":"Class Imbalance","text":"Attribute Details Description When one class has significantly more examples than another in classification problems Use Case Fraud detection (99% legitimate, 1% fraud), disease diagnosis Solutions Downsampling, upsampling, SMOTE, class weights"},{"location":"glossary/#class-distribution","title":"Class Distribution","text":"Attribute Details Description The proportion of each class/category in a dataset Use Case Understanding dataset balance, choosing appropriate splitting strategy Example 70% spam, 30% not spam OR 95% negative, 5% positive"},{"location":"glossary/#class-weights","title":"Class Weights","text":"Attribute Details Description Penalties applied during training to make misclassifying certain classes more costly Use Case Handling imbalanced datasets without changing data distribution Example class_weight={0: 1, 1: 5} penalizes misclassifying class 1 five times more"},{"location":"glossary/#cmek-customer-managed-encryption-keys","title":"CMEK (Customer-Managed Encryption Keys)","text":"Attribute Details Description User-controlled encryption keys for data security AWS Equivalent Customer managed keys in KMS Use Case Regulatory compliance, data sovereignty"},{"location":"glossary/#concept-drift","title":"Concept Drift","text":"Attribute Details Description When the statistical properties of the target variable change over time, making old patterns obsolete Use Case Monitoring model performance, determining retraining schedule Example Consumer behavior changes, market dynamics shift, seasonal patterns evolve"},{"location":"glossary/#convergence","title":"Convergence","text":"Attribute Details Description The point during training where model performance stops improving significantly Use Case Determining when to stop training to avoid wasted compute resources"},{"location":"glossary/#ct-continuous-training","title":"CT (Continuous Training)","text":"Attribute Details Description Automated retraining of models based on triggers or schedules Components Data monitoring, training pipeline, deployment automation"},{"location":"glossary/#d","title":"D","text":""},{"location":"glossary/#dag-directed-acyclic-graph","title":"DAG (Directed Acyclic Graph)","text":"Attribute Details Description Graph structure representing ML pipeline dependencies Use Case Workflow orchestration in Vertex AI Pipelines, Kubeflow"},{"location":"glossary/#data-leakage","title":"Data Leakage","text":"Attribute Details Description When information from outside the training set is used to create the model, leading to overly optimistic performance Common Causes Using test data in preprocessing, target leakage, temporal leakage Prevention Proper train/test split, fit transformers only on training data"},{"location":"glossary/#bigquery-ml","title":"BigQuery ML","text":"Attribute Details Description Create and execute ML models using SQL directly in BigQuery AWS Equivalent Redshift ML Key Features SQL-based model training, no data movement required"},{"location":"glossary/#c_1","title":"C","text":""},{"location":"glossary/#cmek-customer-managed-encryption-keys_1","title":"CMEK (Customer-Managed Encryption Keys)","text":"Attribute Details Description User-controlled encryption keys for data security AWS Equivalent Customer managed keys in KMS Use Case Regulatory compliance, data sovereignty"},{"location":"glossary/#ct-continuous-training_1","title":"CT (Continuous Training)","text":"Attribute Details Description Automated retraining of models based on triggers or schedules Components Data monitoring, training pipeline, deployment automation"},{"location":"glossary/#convergence_1","title":"Convergence","text":"Attribute Details Description The point during training where model performance stops improving significantly Use Case Determining when to stop training to avoid wasted compute resources"},{"location":"glossary/#d_1","title":"D","text":""},{"location":"glossary/#dag-directed-acyclic-graph_1","title":"DAG (Directed Acyclic Graph)","text":"Attribute Details Description Graph structure representing ML pipeline dependencies Use Case Workflow orchestration in Vertex AI Pipelines, Kubeflow"},{"location":"glossary/#cloud-data-fusion","title":"Cloud Data Fusion","text":"Attribute Details Description Fully managed, cloud-native data integration service with visual ETL/ELT pipeline builder AWS Equivalent AWS Glue Studio, AWS Glue DataBrew Key Features Visual drag-and-drop, 150+ connectors, data quality checks, pipeline templates Use Case Data preparation for ML, consolidating multiple sources, business-user friendly ETL"},{"location":"glossary/#dataflow","title":"Dataflow","text":"Attribute Details Description Managed service for stream and batch data processing based on Apache Beam AWS Equivalent AWS Glue (batch), Kinesis Analytics (stream) Key Features Unified programming model, auto-scaling, windowing"},{"location":"glossary/#dimensionality-reduction","title":"Dimensionality Reduction","text":"Attribute Details Description Reducing the number of features while preserving information Techniques PCA, feature selection, t-SNE, UMAP Use Case Handling high-dimensional data, visualization, reducing training time"},{"location":"glossary/#downsampling","title":"Downsampling","text":"Attribute Details Description Reducing the number of majority class samples to balance dataset Use Case Class imbalance when majority class has sufficient samples (&gt;10,000) Trade-off Loses majority class information but balances classes quickly"},{"location":"glossary/#dnn-deep-neural-network","title":"DNN (Deep Neural Network)","text":"Attribute Details Description Neural network with multiple hidden layers Use Case Complex pattern recognition, computer vision, NLP"},{"location":"glossary/#e","title":"E","text":""},{"location":"glossary/#epoch","title":"Epoch","text":"Attribute Details Description One complete pass through the entire training dataset Use Case Measuring training progress and controlling training duration"},{"location":"glossary/#f","title":"F","text":""},{"location":"glossary/#f2-score","title":"F2-Score","text":"Attribute Details Description Weighted F-score that emphasizes recall over precision Formula (1 + 2\u00b2) \u00d7 (Precision \u00d7 Recall) / (2\u00b2 \u00d7 Precision + Recall) Use Case When false negatives are more costly than false positives (e.g., medical diagnosis)"},{"location":"glossary/#false-negative-fn","title":"False Negative (FN)","text":"Attribute Details Description Actual positive case incorrectly predicted as negative Use Case Critical in medical diagnosis and fraud detection where missing positives is costly"},{"location":"glossary/#false-positive-fp","title":"False Positive (FP)","text":"Attribute Details Description Actual negative case incorrectly predicted as positive Use Case Important in spam detection where false alarms reduce user trust"},{"location":"glossary/#feature","title":"Feature","text":"Attribute Details Description An individual measurable property or characteristic used as input to a model Use Case Building predictive models from structured data"},{"location":"glossary/#feature-engineering","title":"Feature Engineering","text":"Attribute Details Description Creating new features from existing ones to improve model performance Techniques Polynomial features, interaction terms, binning, encoding Use Case Improving model accuracy, capturing domain knowledge"},{"location":"glossary/#feature-selection","title":"Feature Selection","text":"Attribute Details Description Choosing a subset of relevant features for modeling Methods Filter (correlation, chi-square), Wrapper (RFE), Embedded (Lasso) Use Case Reducing dimensionality while maintaining interpretability"},{"location":"glossary/#feature-store","title":"Feature Store","text":"Attribute Details Description Centralized repository for storing, serving, and managing ML features AWS Equivalent SageMaker Feature Store Components Online store (real-time), Offline store (training)"},{"location":"glossary/#forward-fill","title":"Forward Fill","text":"Attribute Details Description Time-series imputation that fills missing values with the previous observed value Use Case Sequential data where missing values should inherit from past observations Example Stock prices: [100, 102, NaN, NaN, 108] \u2192 [100, 102, 102, 102, 108]"},{"location":"glossary/#g","title":"G","text":""},{"location":"glossary/#generalization","title":"Generalization","text":"Attribute Details Description A model's ability to perform well on new, unseen data Use Case Core objective of machine learning; preventing overfitting"},{"location":"glossary/#gradient-descent","title":"Gradient Descent","text":"Attribute Details Description Optimization algorithm that iteratively adjusts weights to minimize loss Use Case Training neural networks and other ML models"},{"location":"glossary/#h","title":"H","text":""},{"location":"glossary/#holdout-set","title":"Holdout Set","text":"Attribute Details Description A portion of data set aside and not used during training Use Case Final evaluation of model performance, validation during training Example 20% test set held out from 80% training data"},{"location":"glossary/#hyperparameter","title":"Hyperparameter","text":"Attribute Details Description Configuration value set before training that controls the learning process (e.g., learning rate) Use Case Tuning model performance through systematic optimization"},{"location":"glossary/#i","title":"I","text":""},{"location":"glossary/#imputation","title":"Imputation","text":"Attribute Details Description Filling in missing values in a dataset Techniques Mean/median/mode, forward/backward fill, KNN, predictive imputation Use Case Handling missing data before model training"},{"location":"glossary/#iam-identity-and-access-management","title":"IAM (Identity and Access Management)","text":"Attribute Details Description Access control service for GCP resources AWS Equivalent AWS IAM Key Roles aiplatform.user, aiplatform.admin, bigquery.dataViewer"},{"location":"glossary/#iteration","title":"Iteration","text":"Attribute Details Description One update of model weights, typically after processing one batch Use Case Tracking training progress within an epoch"},{"location":"glossary/#k","title":"K","text":""},{"location":"glossary/#kfp-kubeflow-pipelines","title":"KFP (Kubeflow Pipelines)","text":"Attribute Details Description ML workflow framework used in Vertex AI Pipelines AWS Equivalent SageMaker Pipelines (different framework) Components Pipeline definition, components, artifacts"},{"location":"glossary/#kms-key-management-service","title":"KMS (Key Management Service)","text":"Attribute Details Description Service for creating and managing cryptographic keys AWS Equivalent AWS KMS Use Case Encryption key management, CMEK implementation"},{"location":"glossary/#l","title":"L","text":""},{"location":"glossary/#lambda-\u03bb","title":"Lambda (\u03bb)","text":"Attribute Details Description Regularization strength parameter; higher values increase penalty on model complexity Use Case Controlling the tradeoff between fitting training data and model simplicity"},{"location":"glossary/#look-ahead-bias","title":"Look-Ahead Bias","text":"Attribute Details Description Using information that would not have been available at prediction time (common error in time series) Use Case Preventing unrealistic model performance in time series Example Using future stock prices to predict past prices, using tomorrow's data in today's features"},{"location":"glossary/#loss-function","title":"Loss Function","text":"Attribute Details Description Mathematical function that quantifies the difference between predictions and actual values Use Case Guiding model optimization during training"},{"location":"glossary/#m","title":"M","text":""},{"location":"glossary/#mar-missing-at-random","title":"MAR (Missing at Random)","text":"Attribute Details Description Missingness related to observed data but not unobserved data Use Case Understanding missing data patterns for imputation strategy Example Income missing for certain age groups (missingness depends on age, which is observed)"},{"location":"glossary/#mcar-missing-completely-at-random","title":"MCAR (Missing Completely at Random)","text":"Attribute Details Description Missingness has no relationship to any variables Use Case Simplest missing data scenario, allows simple imputation Example Survey responses randomly skipped due to technical glitch"},{"location":"glossary/#mnar-missing-not-at-random","title":"MNAR (Missing Not at Random)","text":"Attribute Details Description Missingness related to the missing value itself Use Case Most complex missing data scenario, requires careful handling Example High earners leaving income field blank (missingness depends on income itself)"},{"location":"glossary/#mae-mean-absolute-error","title":"MAE (Mean Absolute Error)","text":"Attribute Details Description Average of absolute differences between predictions and actual values Formula MAE = (1/n) \u00d7 \u03a3|predicted - actual| Use Case Regression model evaluation"},{"location":"glossary/#mse-mean-squared-error","title":"MSE (Mean Squared Error)","text":"Attribute Details Description Average of squared differences between predictions and actual values Formula MSE = (1/n) \u00d7 \u03a3(predicted - actual)\u00b2 Use Case Regression model evaluation, sensitive to outliers"},{"location":"glossary/#mwaa-managed-workflows-for-apache-airflow","title":"MWAA (Managed Workflows for Apache Airflow)","text":"Attribute Details Description AWS managed Apache Airflow service GCP Equivalent Cloud Composer Use Case Workflow orchestration, ETL pipelines"},{"location":"glossary/#n","title":"N","text":""},{"location":"glossary/#noise","title":"Noise","text":"Attribute Details Description Random variations or errors in data that don't represent true patterns Use Case Understanding data quality and the limits of model accuracy"},{"location":"glossary/#o","title":"O","text":""},{"location":"glossary/#overfitting","title":"Overfitting","text":"Attribute Details Description Model performs well on training data but poorly on new data due to memorization Use Case Identifying when model complexity needs to be reduced or regularization applied"},{"location":"glossary/#p","title":"P","text":""},{"location":"glossary/#parameter","title":"Parameter","text":"Attribute Details Description Internal variable learned by the model from data (e.g., weights, biases) Use Case Understanding model capacity and complexity"},{"location":"glossary/#purge-period","title":"Purge Period","text":"Attribute Details Description Gap between training and test data in time series to prevent temporal leakage Use Case Blocked/purged cross-validation for time series Example 2-day gap between last training day and first test day to allow features to materialize"},{"location":"glossary/#pubsub","title":"Pub/Sub","text":"Attribute Details Description Messaging service for event ingestion and distribution AWS Equivalent Amazon Kinesis, SNS/SQS Key Features At-least-once delivery, ordering, topic-based routing"},{"location":"glossary/#r","title":"R","text":""},{"location":"glossary/#rmse-root-mean-squared-error","title":"RMSE (Root Mean Squared Error)","text":"Attribute Details Description Square root of MSE, in same units as target variable Formula RMSE = \u221aMSE Use Case Regression model evaluation"},{"location":"glossary/#roc-receiver-operating-characteristic","title":"ROC (Receiver Operating Characteristic)","text":"Attribute Details Description Plot of true positive rate vs false positive rate Use Case Visualizing classification performance at different thresholds"},{"location":"glossary/#s","title":"S","text":""},{"location":"glossary/#shap-shapley-additive-explanations","title":"SHAP (SHapley Additive exPlanations)","text":"Attribute Details Description Method for explaining individual predictions Use Case Model interpretability, feature importance"},{"location":"glossary/#smote-synthetic-minority-over-sampling-technique","title":"SMOTE (Synthetic Minority Over-sampling Technique)","text":"Attribute Details Description Technique for handling imbalanced datasets by generating synthetic examples Use Case Improving minority class representation"},{"location":"glossary/#sparsity","title":"Sparsity","text":"Attribute Details Description Having many zero values; L1 regularization creates sparse weight matrices Use Case Feature selection and model interpretability"},{"location":"glossary/#stratification","title":"Stratification","text":"Attribute Details Description Ensuring class proportions are maintained across splits Use Case Train/test splitting, K-fold cross-validation for classification Example If 70% Class A in original data, maintain 70% Class A in both train and test"},{"location":"glossary/#t","title":"T","text":""},{"location":"glossary/#temporal-leakage","title":"Temporal Leakage","text":"Attribute Details Description Using future information to predict the past in time series data Prevention Proper temporal splitting, purge periods, walk-forward validation Example Using next week's prices as features for this week's prediction"},{"location":"glossary/#test-set","title":"Test Set","text":"Attribute Details Description Data used for final model evaluation, should only be used once Use Case Unbiased performance assessment after all training and tuning Best Practice Never touch until final evaluation, typically 10-20% of data"},{"location":"glossary/#training-set","title":"Training Set","text":"Attribute Details Description Data used to fit/train the model Use Case Learning model parameters and patterns Typical Size 60-80% of total dataset"},{"location":"glossary/#t-sne","title":"t-SNE","text":"Attribute Details Description Non-linear dimensionality reduction for visualization Use Case Visualizing high-dimensional data in 2D/3D, exploring clusters Limitation Only for visualization, not for model training"},{"location":"glossary/#tabular-workflows","title":"Tabular Workflows","text":"Attribute Details Description Enhanced AutoML with granular pipeline control AWS Equivalent Custom SageMaker Pipelines Use Case Structured data ML with customization needs"},{"location":"glossary/#tft-tensorflow-transform","title":"TFT (TensorFlow Transform)","text":"Attribute Details Description Library for preprocessing data in TensorFlow Key Feature Ensures training-serving consistency Use Case Feature engineering, preprocessing pipelines"},{"location":"glossary/#tfdv-tensorflow-data-validation","title":"TFDV (TensorFlow Data Validation)","text":"Attribute Details Description Library for exploring and validating ML data Capabilities Schema inference, anomaly detection, drift detection"},{"location":"glossary/#tfx-tensorflow-extended","title":"TFX (TensorFlow Extended)","text":"Attribute Details Description End-to-end platform for deploying production ML pipelines Components Data validation, transform, training, serving"},{"location":"glossary/#threshold","title":"Threshold","text":"Attribute Details Description Decision boundary for converting probabilities to class predictions (default: 0.5) Use Case Tuning precision/recall trade-off in classification Example Lower threshold (0.3) increases recall, higher threshold (0.7) increases precision"},{"location":"glossary/#tpu-tensor-processing-unit","title":"TPU (Tensor Processing Unit)","text":"Attribute Details Description Google's custom chip optimized for ML workloads (GCP exclusive) AWS Equivalent AWS Trainium/Inferentia (similar concept) Best For Large-scale training, matrix operations, TensorFlow models"},{"location":"glossary/#true-negative-tn","title":"True Negative (TN)","text":"Attribute Details Description Actual negative case correctly predicted as negative Use Case Calculating accuracy, specificity, and other classification metrics"},{"location":"glossary/#true-positive-tp","title":"True Positive (TP)","text":"Attribute Details Description Actual positive case correctly predicted as positive Use Case Calculating precision, recall, and other classification metrics"},{"location":"glossary/#u","title":"U","text":""},{"location":"glossary/#umap","title":"UMAP","text":"Attribute Details Description Non-linear dimensionality reduction, faster than t-SNE Use Case Visualizing high-dimensional data, preserving global structure Advantage Faster and better at preserving global structure than t-SNE"},{"location":"glossary/#underfitting","title":"Underfitting","text":"Attribute Details Description Model is too simple to capture data patterns, performing poorly on all data Use Case Identifying when model complexity needs to be increased"},{"location":"glossary/#upsampling","title":"Upsampling","text":"Attribute Details Description Increasing the number of minority class samples (replication or synthesis) Techniques Random oversampling, SMOTE (synthetic generation) Use Case Class imbalance when minority class has very few samples (&lt;1,000)"},{"location":"glossary/#v","title":"V","text":""},{"location":"glossary/#validation-loss","title":"Validation Loss","text":"Attribute Details Description Model's loss calculated on validation set (unseen during training) Use Case Monitoring overfitting, early stopping, model selection Indicator Increasing validation loss while training loss decreases indicates overfitting"},{"location":"glossary/#vertex-ai","title":"Vertex AI","text":"Attribute Details Description Unified ML platform consolidating training, deployment, and monitoring AWS Equivalent Amazon SageMaker Key Components Workbench, Training, Prediction, Pipelines, Feature Store"},{"location":"glossary/#vpc-virtual-private-cloud","title":"VPC (Virtual Private Cloud)","text":"Attribute Details Description Isolated network environment for GCP resources AWS Equivalent Amazon VPC Use Case Network security, resource isolation"},{"location":"glossary/#vpc-sc-vpc-service-controls","title":"VPC-SC (VPC Service Controls)","text":"Attribute Details Description Security perimeter for GCP resources to prevent data exfiltration AWS Equivalent VPC Endpoints, PrivateLink Use Case Data security, compliance requirements"},{"location":"glossary/#w","title":"W","text":""},{"location":"glossary/#weight","title":"Weight","text":"Attribute Details Description Learned parameter that determines the importance of a feature in making predictions Use Case Understanding model decisions and feature importance <p>This glossary provides essential terminology for Google Cloud ML certification preparation, with AWS comparisons to leverage existing cloud knowledge.</p>"},{"location":"kube_ml_flow/","title":"MLFlow vs Kubeflow: Comprehensive Comparison","text":""},{"location":"kube_ml_flow/#overview","title":"Overview","text":"<p>This document provides an in-depth comparison between MLFlow and Kubeflow, two popular open-source platforms for managing machine learning workflows, along with their implementations on AWS and GCP.</p>"},{"location":"kube_ml_flow/#executive-summary","title":"Executive Summary","text":"Aspect MLFlow Kubeflow Primary Focus Experiment tracking, model registry, and deployment End-to-end ML workflow orchestration on Kubernetes Architecture Lightweight, library-based Heavy, Kubernetes-native platform Learning Curve Low - simple Python API High - requires Kubernetes knowledge Best For Small teams, rapid experimentation, model versioning Large teams, production pipelines, scalable workflows Deployment Model Flexible (local, cloud, on-prem) Kubernetes clusters Primary Use Case Tracking experiments and managing models Building and deploying ML pipelines"},{"location":"kube_ml_flow/#1-core-components-comparison","title":"1. Core Components Comparison","text":""},{"location":"kube_ml_flow/#mlflow-components","title":"MLFlow Components","text":"Component Description Key Features MLFlow Tracking Records and queries experiments Parameters, metrics, artifacts, source code versioning MLFlow Projects Packages code in reusable format Conda/Docker environments, reproducibility MLFlow Models Standard format for packaging models Multiple frameworks, deployment to various platforms MLFlow Model Registry Centralized model store Versioning, stage transitions (staging/production), lineage"},{"location":"kube_ml_flow/#kubeflow-components","title":"Kubeflow Components","text":"Component Description Key Features Kubeflow Pipelines Workflow orchestration engine DAG-based pipelines, versioning, scheduling Katib Hyperparameter tuning system AutoML, neural architecture search, early stopping KFServing Model serving platform Serverless inference, canary deployments, autoscaling Notebooks Jupyter notebook environment Multi-user, GPU support, version control Training Operators Distributed training TensorFlow, PyTorch, MXNet operators Metadata Store Pipeline and artifact tracking Lineage tracking, artifact versioning"},{"location":"kube_ml_flow/#2-feature-by-feature-comparison","title":"2. Feature-by-Feature Comparison","text":""},{"location":"kube_ml_flow/#experiment-tracking","title":"Experiment Tracking","text":"Feature MLFlow Kubeflow Metrics Logging Simple Python API: <code>mlflow.log_metric()</code> Through Kubeflow Pipelines metadata Parameter Tracking Automatic and manual logging Pipeline parameters tracked automatically Artifact Storage Local filesystem, S3, Azure Blob, GCS Kubernetes persistent volumes, cloud storage UI Dashboard Built-in web UI for experiment comparison Kubeflow Central Dashboard Integration Effort Minimal - add a few lines of code Moderate - requires pipeline definition"},{"location":"kube_ml_flow/#model-management","title":"Model Management","text":"Feature MLFlow Kubeflow Model Registry Built-in central registry Through KFServing and metadata store Versioning Automatic semantic versioning Pipeline version tracking Stage Management Staging, Production, Archived Custom stages via KFServing Model Lineage Tracks experiments \u2192 models Tracks pipelines \u2192 artifacts \u2192 models Model Serving Built-in serving with REST API KFServing with advanced features"},{"location":"kube_ml_flow/#deployment--serving","title":"Deployment &amp; Serving","text":"Feature MLFlow Kubeflow Deployment Targets Local, Docker, Kubernetes, SageMaker, Azure ML Primarily Kubernetes (KFServing) Autoscaling Depends on deployment target Native Kubernetes autoscaling A/B Testing Manual implementation Built-in canary deployments Model Monitoring Basic logging Integration with Prometheus, Grafana Inference Protocols REST, batch REST, gRPC, serverless"},{"location":"kube_ml_flow/#pipeline-orchestration","title":"Pipeline Orchestration","text":"Feature MLFlow Kubeflow Pipeline Definition MLFlow Projects (limited) Kubeflow Pipelines (comprehensive) DAG Support No native DAG Full DAG support with visualization Scheduling External tools needed Built-in cron scheduling Conditionals &amp; Loops Limited Full programming constructs Component Reusability Docker/Conda environments Container-based components"},{"location":"kube_ml_flow/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"Feature MLFlow Kubeflow Built-in Tuning No (use external libraries) Yes (Katib) Supported Algorithms N/A Random, Grid, Bayesian, Hyperband Parallel Execution Manual implementation Automatic parallel trials Early Stopping Manual Built-in Neural Architecture Search No Yes (Katib NAS)"},{"location":"kube_ml_flow/#3-aws-implementation","title":"3. AWS Implementation","text":""},{"location":"kube_ml_flow/#mlflow-on-aws","title":"MLFlow on AWS","text":"<p>Deployment Architecture:</p> <pre><code>Application Layer:\n  - EC2 instances or ECS containers running MLFlow server\n  - Application Load Balancer for HA\n\nStorage Layer:\n  - RDS (PostgreSQL/MySQL) for backend store\n  - S3 for artifact storage\n\nTracking:\n  - CloudWatch for monitoring\n  - IAM for authentication\n</code></pre> <p>Implementation Options:</p> Deployment Type Components Best For Self-Managed EC2 EC2 + RDS + S3 Full control, customization ECS/Fargate ECS tasks + RDS + S3 Serverless MLFlow server SageMaker Integration SageMaker experiments + S3 Native AWS ML workflow AWS Managed MLFlow AWS-managed service (preview) Minimal ops overhead <p>Setup Example:</p> <pre><code>import mlflow\n\n# Configure MLFlow to use AWS\nmlflow.set_tracking_uri(\"http://mlflow-server.example.com\")\n\n# S3 artifact storage\nmlflow.set_experiment(\"my-experiment\")\n\nwith mlflow.start_run():\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_metric(\"accuracy\", 0.95)\n\n    # Log model to S3\n    mlflow.sklearn.log_model(model, \"model\")\n</code></pre>"},{"location":"kube_ml_flow/#kubeflow-on-aws","title":"Kubeflow on AWS","text":"<p>Deployment Architecture:</p> <pre><code>Infrastructure:\n  - Amazon EKS (Elastic Kubernetes Service)\n  - EC2 worker nodes or Fargate for serverless\n\nStorage:\n  - Amazon EFS for shared storage\n  - S3 for pipeline artifacts\n  - RDS for metadata store\n\nNetworking:\n  - VPC with private subnets\n  - Application Load Balancer\n  - IAM for RBAC\n</code></pre> <p>Implementation Options:</p> Deployment Type Setup Method Complexity EKS + Kubeflow AWS blueprints, Terraform High Amazon SageMaker Pipelines Native AWS service (Kubeflow-compatible) Medium EKS Anywhere On-premises with AWS integration Very High <p>Key AWS Services Integration:</p> <ul> <li>SageMaker Operators for Kubernetes: Train and deploy using SageMaker from Kubeflow</li> <li>AWS Step Functions: Alternative to Kubeflow Pipelines</li> <li>Amazon ECR: Container registry for pipeline components</li> <li>AWS Secrets Manager: Secure credential management</li> </ul> <p>Pipeline Example:</p> <pre><code>import kfp\nfrom kfp import dsl\n\n@dsl.component\ndef train_model(data_path: str) -&gt; str:\n    # Training logic using SageMaker\n    pass\n\n@dsl.pipeline(name='AWS ML Pipeline')\ndef ml_pipeline():\n    train_task = train_model(data_path='s3://bucket/data')\n\npipeline = kfp.Client().create_run_from_pipeline_func(\n    ml_pipeline,\n    arguments={}\n)\n</code></pre>"},{"location":"kube_ml_flow/#4-gcp-implementation","title":"4. GCP Implementation","text":""},{"location":"kube_ml_flow/#mlflow-on-gcp","title":"MLFlow on GCP","text":"<p>Deployment Architecture:</p> <pre><code>Application Layer:\n  - Cloud Run or GKE for MLFlow server\n  - Cloud Load Balancing\n\nStorage Layer:\n  - Cloud SQL (PostgreSQL/MySQL) for backend\n  - Google Cloud Storage for artifacts\n\nIdentity &amp; Monitoring:\n  - Cloud IAM for authentication\n  - Cloud Monitoring for observability\n</code></pre> <p>Implementation Options:</p> Deployment Type Components Best For Cloud Run Cloud Run + Cloud SQL + GCS Serverless, auto-scaling GKE GKE cluster + Cloud SQL + GCS Production workloads Vertex AI Integration Vertex AI Experiments + GCS Native GCP ML workflow Compute Engine VM instances + Cloud SQL + GCS Custom configurations <p>Setup Example:</p> <pre><code>import mlflow\n\n# Configure for GCP\nmlflow.set_tracking_uri(\"https://mlflow.example.com\")\n\n# GCS artifact storage\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"key.json\"\n\nwith mlflow.start_run():\n    mlflow.log_param(\"batch_size\", 32)\n    mlflow.log_metric(\"loss\", 0.05)\n\n    # Artifacts stored in GCS\n    mlflow.log_artifact(\"model.pkl\", artifact_path=\"gs://bucket/artifacts\")\n</code></pre>"},{"location":"kube_ml_flow/#kubeflow-on-gcp","title":"Kubeflow on GCP","text":"<p>Deployment Architecture:</p> <pre><code>Infrastructure:\n  - Google Kubernetes Engine (GKE)\n  - Autopilot or Standard clusters\n\nStorage:\n  - Google Cloud Storage for artifacts\n  - Cloud SQL or Filestore for metadata\n\nIntegration:\n  - Vertex AI Pipelines (managed Kubeflow)\n  - Artifact Registry for containers\n  - Cloud IAM with Workload Identity\n</code></pre> <p>Implementation Options:</p> Deployment Type Setup Method Management Overhead Vertex AI Pipelines Fully managed, Kubeflow-compatible Minimal (recommended) GKE + Kubeflow Self-managed on GKE High AI Platform Pipelines Legacy managed service Low <p>Vertex AI Pipelines (Recommended):</p> <p>Vertex AI Pipelines is Google's managed implementation of Kubeflow Pipelines, eliminating infrastructure management.</p> <p>Key Features:</p> <ul> <li>Serverless execution (no cluster management)</li> <li>Native GCP service integration</li> <li>Built-in monitoring and logging</li> <li>Automatic artifact tracking</li> <li>Cost-effective (pay per execution)</li> </ul> <p>Pipeline Example:</p> <pre><code>from kfp import dsl\nfrom google.cloud import aiplatform\n\n@dsl.component\ndef preprocess_data(input_path: str, output_path: str):\n    # Preprocessing logic\n    pass\n\n@dsl.component\ndef train_model(data_path: str, model_path: str):\n    # Training using Vertex AI\n    pass\n\n@dsl.pipeline(name='vertex-ml-pipeline')\ndef ml_pipeline(\n    project_id: str,\n    region: str,\n    data_path: str\n):\n    preprocess_task = preprocess_data(\n        input_path=data_path,\n        output_path='gs://bucket/processed'\n    )\n\n    train_task = train_model(\n        data_path=preprocess_task.outputs['output_path'],\n        model_path='gs://bucket/model'\n    )\n\n# Compile and run\nfrom kfp import compiler\ncompiler.Compiler().compile(\n    pipeline_func=ml_pipeline,\n    package_path='pipeline.yaml'\n)\n\n# Execute on Vertex AI\njob = aiplatform.PipelineJob(\n    display_name='my-pipeline',\n    template_path='pipeline.yaml',\n    parameter_values={\n        'project_id': 'my-project',\n        'region': 'us-central1',\n        'data_path': 'gs://bucket/data'\n    }\n)\njob.run()\n</code></pre>"},{"location":"kube_ml_flow/#5-when-to-choose-mlflow-vs-kubeflow","title":"5. When to Choose MLFlow vs Kubeflow","text":""},{"location":"kube_ml_flow/#choose-mlflow-when","title":"Choose MLFlow When:","text":"<ul> <li>Small to medium teams focused on experimentation</li> <li>Model tracking and versioning is the primary need</li> <li>Minimal infrastructure requirements</li> <li>Framework agnostic approach preferred</li> <li>Quick setup and ease of use is priority</li> <li>Not using Kubernetes in production</li> <li>Individual data scientists need experiment tracking</li> </ul>"},{"location":"kube_ml_flow/#choose-kubeflow-when","title":"Choose Kubeflow When:","text":"<ul> <li>Large-scale ML operations with complex workflows</li> <li>Production ML pipelines with orchestration needs</li> <li>Already using Kubernetes infrastructure</li> <li>Distributed training is a requirement</li> <li>Automated hyperparameter tuning at scale</li> <li>Multi-step ML workflows with dependencies</li> <li>Enterprise-grade MLOps platform needed</li> </ul>"},{"location":"kube_ml_flow/#hybrid-approach","title":"Hybrid Approach:","text":"<p>Many organizations use both:</p> <ul> <li>MLFlow for experiment tracking and model registry</li> <li>Kubeflow for pipeline orchestration and production deployment</li> </ul> <p>Integration Example:</p> <pre><code># In Kubeflow pipeline component\nimport mlflow\n\n@dsl.component\ndef train_with_mlflow():\n    mlflow.set_tracking_uri(\"http://mlflow-server\")\n\n    with mlflow.start_run():\n        # Training code\n        mlflow.log_metric(\"accuracy\", accuracy)\n        mlflow.sklearn.log_model(model, \"model\")\n</code></pre>"},{"location":"kube_ml_flow/#6-cost-comparison","title":"6. Cost Comparison","text":""},{"location":"kube_ml_flow/#mlflow-costs","title":"MLFlow Costs","text":"Component AWS Cost GCP Cost Compute EC2: $50-200/month Cloud Run: $20-100/month Database RDS: $30-150/month Cloud SQL: $25-120/month Storage S3: $0.023/GB GCS: $0.020/GB Total (Small Setup) ~$100-400/month ~$70-300/month"},{"location":"kube_ml_flow/#kubeflow-costs","title":"Kubeflow Costs","text":"Component AWS Cost GCP Cost Kubernetes Cluster EKS: $73/month + nodes GKE: $73/month + nodes Worker Nodes $100-1000+/month $100-1000+/month Storage EBS + S3: $50-200/month Persistent disk + GCS: $40-180/month Load Balancer $20/month $18/month Total (Small Setup) ~$250-1500/month ~$230-1400/month <p>Vertex AI Pipelines (GCP Managed):</p> <ul> <li>No cluster costs (serverless)</li> <li>Pay per pipeline execution</li> <li>Typical cost: $0.03 per pipeline run + compute</li> <li>More cost-effective for sporadic workloads</li> </ul>"},{"location":"kube_ml_flow/#7-migration-considerations","title":"7. Migration Considerations","text":""},{"location":"kube_ml_flow/#from-mlflow-to-kubeflow","title":"From MLFlow to Kubeflow","text":"<p>Pros:</p> <ul> <li>Better pipeline orchestration</li> <li>Scalable production deployments</li> <li>Advanced features (hyperparameter tuning, distributed training)</li> </ul> <p>Cons:</p> <ul> <li>Infrastructure complexity increases</li> <li>Team needs Kubernetes expertise</li> <li>Migration effort for existing experiments</li> </ul> <p>Migration Strategy:</p> <ol> <li>Set up Kubeflow cluster</li> <li>Integrate MLFlow tracking within Kubeflow components</li> <li>Gradually migrate workflows to Kubeflow Pipelines</li> <li>Keep MLFlow for model registry and experiment tracking</li> </ol>"},{"location":"kube_ml_flow/#from-kubeflow-to-mlflow","title":"From Kubeflow to MLFlow","text":"<p>Pros:</p> <ul> <li>Simplified infrastructure</li> <li>Lower operational overhead</li> <li>Easier for small teams</li> </ul> <p>Cons:</p> <ul> <li>Loss of advanced orchestration features</li> <li>Manual pipeline management</li> <li>Limited distributed training support</li> </ul> <p>Migration Strategy:</p> <ol> <li>Export pipeline logic to scripts</li> <li>Set up MLFlow tracking server</li> <li>Use external tools (Airflow, Prefect) for orchestration</li> <li>Maintain experiment metadata</li> </ol>"},{"location":"kube_ml_flow/#8-best-practices","title":"8. Best Practices","text":""},{"location":"kube_ml_flow/#mlflow-best-practices","title":"MLFlow Best Practices","text":"<ul> <li>Use remote tracking server for team collaboration</li> <li>Store artifacts in cloud storage (S3/GCS) for durability</li> <li>Tag experiments with meaningful metadata</li> <li>Use MLFlow Projects for reproducibility</li> <li>Implement model staging workflow (dev \u2192 staging \u2192 production)</li> <li>Set up authentication and authorization</li> <li>Regular backup of backend database</li> <li>Version control MLFlow project files</li> </ul>"},{"location":"kube_ml_flow/#kubeflow-best-practices","title":"Kubeflow Best Practices","text":"<ul> <li>Use Vertex AI Pipelines on GCP for managed experience</li> <li>Implement pipeline components as containers</li> <li>Version pipeline definitions in Git</li> <li>Use pipeline parameters for flexibility</li> <li>Implement proper resource requests/limits</li> <li>Use Katib for hyperparameter optimization</li> <li>Monitor pipeline execution with Cloud Monitoring/CloudWatch</li> <li>Implement CI/CD for pipeline deployment</li> <li>Use secrets management for credentials</li> <li>Regular cluster maintenance and updates</li> </ul>"},{"location":"kube_ml_flow/#9-comparison-matrix","title":"9. Comparison Matrix","text":""},{"location":"kube_ml_flow/#overall-comparison","title":"Overall Comparison","text":"Criteria MLFlow Kubeflow Winner Ease of Setup \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 MLFlow Experiment Tracking \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 MLFlow Pipeline Orchestration \u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Kubeflow Model Serving \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Kubeflow Scalability \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Kubeflow Learning Curve \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50 MLFlow Cost (Small Scale) \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 MLFlow Production Ready \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Kubeflow Community Support \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 MLFlow Cloud Integration \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Kubeflow"},{"location":"kube_ml_flow/#10-decision-framework","title":"10. Decision Framework","text":""},{"location":"kube_ml_flow/#use-this-decision-tree","title":"Use This Decision Tree:","text":"<pre><code>Do you need complex multi-step pipelines?\n\u251c\u2500 No \u2192 Do you need experiment tracking and model registry?\n\u2502        \u251c\u2500 Yes \u2192 MLFlow\n\u2502        \u2514\u2500 No \u2192 Consider simpler tools\n\u2514\u2500 Yes \u2192 Are you already using Kubernetes?\n         \u251c\u2500 Yes \u2192 Kubeflow\n         \u2514\u2500 No \u2192 Do you have resources to manage Kubernetes?\n                  \u251c\u2500 Yes \u2192 Kubeflow\n                  \u2514\u2500 No \u2192 MLFlow + orchestration tool (Airflow/Prefect)\n                          OR Vertex AI Pipelines (GCP)\n                          OR SageMaker Pipelines (AWS)\n</code></pre>"},{"location":"kube_ml_flow/#team-size-recommendations","title":"Team Size Recommendations:","text":"Team Size Recommendation Rationale 1-5 people MLFlow Low overhead, fast experimentation 5-20 people MLFlow + simple orchestration Balance of features and complexity 20-50 people Kubeflow or managed alternatives Need for standardization 50+ people Kubeflow with managed Kubernetes Enterprise MLOps requirements"},{"location":"kube_ml_flow/#conclusion","title":"Conclusion","text":"<p>MLFlow excels at experiment tracking, model versioning, and simple deployments with minimal infrastructure requirements. It's ideal for data scientists who want to focus on modeling rather than infrastructure.</p> <p>Kubeflow provides a comprehensive MLOps platform with advanced features for production-scale ML workflows, but requires Kubernetes expertise and more infrastructure investment.</p> <p>For GCP users: Vertex AI Pipelines offers the best of both worlds - Kubeflow's pipeline capabilities with Google's managed infrastructure.</p> <p>For AWS users: SageMaker Pipelines provides similar managed experience, though MLFlow + ECS/EKS remains a popular choice.</p> <p>The choice depends on your team size, infrastructure capabilities, and production requirements. Many organizations successfully run both in parallel, using MLFlow for tracking and Kubeflow for production pipelines.</p>"},{"location":"labs/","title":"Hands-On Lab Recommendations","text":""},{"location":"labs/#lab-1-end-to-end-automl-pipeline","title":"Lab 1: End-to-End AutoML Pipeline","text":"<p>Objective: Build complete pipeline from data to deployed model</p> <p>Steps:</p> <ol> <li>Upload dataset to Cloud Storage</li> <li>Create Vertex AI Dataset</li> <li>Train AutoML model</li> <li>Evaluate model performance</li> <li>Deploy to endpoint</li> <li>Make online predictions</li> <li>Run batch predictions</li> </ol> <p>AWS Comparison: Build same pipeline in SageMaker</p>"},{"location":"labs/#lab-2-custom-training-with-tensorflow","title":"Lab 2: Custom Training with TensorFlow","text":"<p>Objective: Train custom model on Vertex AI</p> <p>Steps:</p> <ol> <li>Write TensorFlow training script</li> <li>Create Docker container</li> <li>Push to Artifact Registry</li> <li>Create Custom Training Job</li> <li>Monitor training with Cloud Logging</li> <li>Export model to Cloud Storage</li> <li>Deploy and serve</li> </ol> <p>AWS Comparison: SageMaker Training Job with BYOC</p>"},{"location":"labs/#lab-3-streaming-ml-pipeline","title":"Lab 3: Streaming ML Pipeline","text":"<p>Objective: Real-time feature engineering and prediction</p> <p>Steps:</p> <ol> <li>Set up Pub/Sub topic</li> <li>Create Dataflow streaming pipeline</li> <li>Write to Feature Store (online)</li> <li>Call Vertex AI Endpoint for predictions</li> <li>Write results to BigQuery</li> <li>Monitor pipeline</li> </ol> <p>AWS Comparison: Kinesis + Flink + SageMaker</p>"},{"location":"labs/#lab-4-mlops-with-vertex-ai-pipelines","title":"Lab 4: MLOps with Vertex AI Pipelines","text":"<p>Objective: Build CI/CD/CT pipeline</p> <p>Steps:</p> <ol> <li>Write KFP pipeline definition</li> <li>Set up Cloud Build trigger</li> <li>Configure automated testing</li> <li>Deploy pipeline on code commit</li> <li>Set up model monitoring</li> <li>Create retraining trigger</li> </ol> <p>AWS Comparison: SageMaker Pipelines with CodePipeline</p>"},{"location":"labs/#final-preparation-checklist","title":"Final Preparation Checklist","text":""},{"location":"labs/#-services-understanding","title":"\u2705 Services Understanding","text":"<ul> <li> Know all Vertex AI components</li> <li> Understand BigQuery ML capabilities</li> <li> Know when to use AutoML vs custom training</li> <li> Understand data processing services (Dataflow, Dataproc)</li> </ul>"},{"location":"labs/#-architectural-patterns","title":"\u2705 Architectural Patterns","text":"<ul> <li> Real-time prediction architecture</li> <li> Batch prediction pipeline</li> <li> Streaming ML pipeline</li> <li> MLOps/CI/CD patterns</li> </ul>"},{"location":"labs/#-technical-deep-dives","title":"\u2705 Technical Deep-Dives","text":"<ul> <li> Hyperparameter tuning strategies</li> <li> Distributed training (data vs model parallelism)</li> <li> Feature Store (online vs offline serving)</li> <li> Model monitoring (skew vs drift)</li> </ul>"},{"location":"labs/#-best-practices","title":"\u2705 Best Practices","text":"<ul> <li> Training-serving consistency (TFT)</li> <li> Security (CMEK, VPC-SC, IAM)</li> <li> Cost optimization (preemptible VMs, batch vs online)</li> <li> Performance optimization (TPUs, caching)</li> </ul>"},{"location":"labs/#-aws-comparisons","title":"\u2705 AWS Comparisons","text":"<ul> <li> Know equivalent services for all major GCP services</li> <li> Understand architectural differences</li> <li> Know unique GCP features (TPUs, BigQuery ML, Reduction Server)</li> </ul> <p>Good luck with your Google Cloud Professional Machine Learning Engineer certification!</p>"},{"location":"ml_concepts/","title":"Machine Learning Concepts - Quick Reference","text":""},{"location":"ml_concepts/#overview","title":"Overview","text":"<p>This page provides concise explanations of fundamental machine learning concepts for building, training, and evaluating models. Use this as a quick reference when implementing ML solutions, tuning hyperparameters, or diagnosing model performance issues.</p> <p>Related: See Data Science Concepts for data preparation, preprocessing, and transformation techniques that should be applied before model training.</p>"},{"location":"ml_concepts/#1-core-ml-concepts","title":"1. Core ML Concepts","text":"<p>Fundamental principles underlying machine learning.</p>"},{"location":"ml_concepts/#supervised-learning","title":"Supervised Learning","text":"<p>Learning paradigm where model trains on labeled data (input-output pairs) to learn mapping from inputs to outputs. Includes classification (discrete outputs) and regression (continuous outputs). Requires labeled training data. Examples: predicting house prices, image classification, spam detection.</p>"},{"location":"ml_concepts/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Learning paradigm where model finds patterns in unlabeled data without explicit target variables. Includes clustering, dimensionality reduction, and anomaly detection. No right or wrong answers, discovers hidden structure. Examples: customer segmentation, topic modeling, compression.</p>"},{"location":"ml_concepts/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>Learning paradigm where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward. The agent learns through trial and error, receiving feedback as rewards or penalties. No labeled training data; instead learns optimal behavior through interaction. Applications include game playing, robotics, and autonomous systems.</p>"},{"location":"ml_concepts/#2-ml-algorithm-types","title":"2. ML Algorithm Types","text":"<p>Common machine learning algorithms categorized by problem type and approach.</p>"},{"location":"ml_concepts/#linear-regression","title":"Linear Regression","text":"<p>Supervised learning algorithm for regression that models relationship between input features and continuous output using a linear equation: y = w\u2081x\u2081 + w\u2082x\u2082 + ... + b. Finds best-fit line/hyperplane by minimizing squared error. Simple, interpretable, fast to train. Assumes linear relationship between features and target. Used for price prediction, trend analysis, forecasting.</p>"},{"location":"ml_concepts/#logistic-regression","title":"Logistic Regression","text":"<p>Supervised learning algorithm for binary classification that predicts probability using sigmoid function: p = 1/(1 + e^(-z)). Despite name, used for classification not regression. Outputs probability between 0 and 1. Fast, interpretable, works well with linearly separable data. Used for spam detection, disease diagnosis, customer churn prediction.</p>"},{"location":"ml_concepts/#decision-trees","title":"Decision Trees","text":"<p>Supervised learning algorithm that makes predictions by learning decision rules from features. Creates tree structure where each node represents a feature test, each branch represents an outcome, and each leaf represents a class label or value. Easy to interpret, handles non-linear relationships, no feature scaling needed. Prone to overfitting. Used for credit approval, medical diagnosis, customer segmentation.</p>"},{"location":"ml_concepts/#random-forest","title":"Random Forest","text":"<p>Ensemble learning algorithm that combines multiple decision trees trained on random subsets of data and features. Each tree votes, and majority vote (classification) or average (regression) determines final prediction. Reduces overfitting compared to single decision tree. Handles high-dimensional data well. Provides feature importance. Used for fraud detection, recommendation systems, feature selection.</p>"},{"location":"ml_concepts/#gradient-boosting-xgboost-lightgbm","title":"Gradient Boosting (XGBoost, LightGBM)","text":"<p>Ensemble learning algorithm that builds trees sequentially, where each tree corrects errors of previous trees. XGBoost and LightGBM are optimized implementations with regularization and efficient computation. Often achieves best performance on structured/tabular data. Requires careful tuning to avoid overfitting. Used for competitions, ranking problems, structured data prediction.</p>"},{"location":"ml_concepts/#support-vector-machines-svm","title":"Support Vector Machines (SVM)","text":"<p>Supervised learning algorithm that finds optimal hyperplane separating classes with maximum margin. Uses kernel trick to handle non-linear decision boundaries. Effective in high-dimensional spaces. Works well with clear margin of separation. Memory intensive for large datasets. Used for text classification, image recognition, bioinformatics.</p>"},{"location":"ml_concepts/#k-means-clustering","title":"K-Means Clustering","text":"<p>Unsupervised learning algorithm that partitions data into K clusters by minimizing within-cluster variance. Iteratively assigns points to nearest centroid and updates centroids. Simple, fast, scalable. Requires specifying K in advance. Assumes spherical clusters. Used for customer segmentation, image compression, anomaly detection.</p>"},{"location":"ml_concepts/#k-nearest-neighbors-knn","title":"K-Nearest Neighbors (KNN)","text":"<p>Supervised learning algorithm that classifies based on majority vote of K nearest neighbors or predicts based on their average. Non-parametric, no training phase (lazy learning). Simple to understand. Computationally expensive at prediction time. Sensitive to feature scaling and irrelevant features. Used for recommendation systems, pattern recognition, missing data imputation.</p>"},{"location":"ml_concepts/#neural-networks-deep-learning","title":"Neural Networks (Deep Learning)","text":"<p>Supervised learning algorithms with multiple layers of interconnected neurons that learn hierarchical representations. Includes feedforward networks, CNNs (images), RNNs/LSTMs (sequences), Transformers (NLP). Can model complex non-linear relationships. Requires large amounts of data and computational resources. Used for computer vision, natural language processing, speech recognition, game playing.</p>"},{"location":"ml_concepts/#naive-bayes","title":"Naive Bayes","text":"<p>Supervised learning algorithm based on Bayes' theorem with \"naive\" assumption of feature independence. Fast to train and predict. Works well with high-dimensional data. Performs surprisingly well despite independence assumption. Used for text classification, spam filtering, sentiment analysis.</p>"},{"location":"ml_concepts/#random-cut-forest-rcf","title":"Random Cut Forest (RCF)","text":"<p>Unsupervised learning algorithm for anomaly detection that builds ensemble of trees using random cuts through feature space. Assigns anomaly score based on how isolated a point is. Handles high-dimensional data efficiently. Doesn't require labeled anomalies. Used for fraud detection, system health monitoring, IoT sensor anomaly detection.</p>"},{"location":"ml_concepts/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>Unsupervised learning algorithm for dimensionality reduction that finds orthogonal axes (principal components) capturing maximum variance. Transforms features into uncorrelated components. Reduces feature space while preserving information. Helps with visualization and computational efficiency. Used for data compression, noise reduction, feature extraction.</p>"},{"location":"ml_concepts/#3-model-validation-techniques","title":"3. Model Validation Techniques","text":"<p>Strategies to assess model performance and prevent overfitting.</p>"},{"location":"ml_concepts/#train-test-split","title":"Train-Test Split","text":"<p>Divides dataset into separate training set (to fit model) and test set (to evaluate performance). Common splits: 70-30, 80-20, 90-10 depending on data size. Test set must never be used during training or hyperparameter tuning. Simple but can be unreliable with small datasets.</p>"},{"location":"ml_concepts/#cross-validation","title":"Cross-Validation","text":"<p>Evaluates model by training on multiple different subsets of data and averaging results. Provides more reliable performance estimate than single train-test split. Uses all data for both training and validation, maximizing data efficiency. Essential for small datasets and hyperparameter tuning.</p>"},{"location":"ml_concepts/#k-fold-cross-validation","title":"K-Fold Cross-Validation","text":"<p>Divides data into k equal folds; trains k times, each time using k-1 folds for training and 1 for validation. Averages performance across all k runs for final estimate. Common k values: 5 or 10. Stratified k-fold maintains class proportions in each fold for classification.</p>"},{"location":"ml_concepts/#4-evaluation-metrics","title":"4. Evaluation Metrics","text":"<p>Measures to assess model performance, especially for classification tasks.</p>"},{"location":"ml_concepts/#confusion-matrix","title":"Confusion Matrix","text":"<p>A table showing counts of True Positives, True Negatives, False Positives, and False Negatives for classification models. Rows represent actual classes, columns represent predicted classes. Provides complete picture of classification performance beyond simple accuracy. Foundation for calculating precision, recall, F1, and other metrics.</p>"},{"location":"ml_concepts/#accuracy","title":"Accuracy","text":"<p>The proportion of correct predictions out of total predictions: (TP + TN) / (TP + TN + FP + FN). Simple metric but can be misleading with imbalanced datasets. A model predicting all negatives on 95% negative data achieves 95% accuracy but is useless. Best used when classes are balanced and errors are equally costly.</p>"},{"location":"ml_concepts/#precision","title":"Precision","text":"<p>The proportion of positive predictions that are actually correct: True Positives / (True Positives + False Positives). Answers \"Of all items we labeled as positive, how many were truly positive?\" High precision means low false positive rate. Important when false positives are costly (e.g., spam detection, medical diagnosis).</p>"},{"location":"ml_concepts/#recall-sensitivity","title":"Recall (Sensitivity)","text":"<p>The proportion of actual positives that were correctly identified: True Positives / (True Positives + False Negatives). Answers \"Of all actual positive items, how many did we correctly identify?\" High recall means low false negative rate. Important when missing positives is costly (e.g., cancer detection, fraud detection).</p>"},{"location":"ml_concepts/#f1-score","title":"F1 Score","text":"<p>Harmonic mean of precision and recall: 2 \u00d7 (Precision \u00d7 Recall) / (Precision + Recall). Balances precision and recall into a single metric, useful when both need to be reasonably high. Ranges from 0 to 1, with 1 being perfect. Preferred over accuracy for imbalanced datasets.</p>"},{"location":"ml_concepts/#roc-curve-receiver-operating-characteristic","title":"ROC Curve (Receiver Operating Characteristic)","text":"<p>Graph plotting True Positive Rate (Recall) against False Positive Rate at various classification thresholds. Shows tradeoff between sensitivity and specificity across all possible thresholds. Curve closer to top-left corner indicates better performance. Useful for selecting optimal threshold for the use case.</p>"},{"location":"ml_concepts/#auc-area-under-the-roc-curve","title":"AUC (Area Under the ROC Curve)","text":"<p>Single number (0 to 1) summarizing ROC curve performance; measures probability that model ranks random positive higher than random negative. AUC of 0.5 means random guessing; 1.0 means perfect classification. Threshold-independent metric, useful for comparing models. Robust to class imbalance.</p>"},{"location":"ml_concepts/#auc-pr-area-under-the-precision-recall-curve","title":"AUC-PR (Area Under the Precision-Recall Curve)","text":"<p>Single number (0 to 1) summarizing the precision-recall curve; measures model performance across all classification thresholds by plotting precision against recall. Unlike AUC-ROC, AUC-PR focuses on the positive class performance and is more informative for highly imbalanced datasets where the minority class is of primary interest. Higher AUC-PR indicates better model performance on the positive class. Preferred over AUC-ROC when positive class is rare (fraud detection, rare disease diagnosis, anomaly detection).</p>"},{"location":"ml_concepts/#5-loss-functions","title":"5. Loss Functions","text":"<p>Metrics that quantify how wrong the model's predictions are.</p>"},{"location":"ml_concepts/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>Measures average squared difference between predicted and actual values: (1/n) \u03a3(y - \u0177)\u00b2. Most common loss function for regression problems. Heavily penalizes large errors due to squaring; sensitive to outliers. Differentiable everywhere, making it suitable for gradient-based optimization. Used in linear regression, neural networks for regression tasks. Units are squared (e.g., dollars\u00b2 for price prediction).</p>"},{"location":"ml_concepts/#mean-absolute-error-mae","title":"Mean Absolute Error (MAE)","text":"<p>Measures average absolute difference between predicted and actual values: (1/n) \u03a3|y - \u0177|. More robust to outliers than MSE since errors are not squared. Gives equal weight to all errors regardless of magnitude. Interpretable in original units (e.g., dollars for price prediction). Less sensitive to extreme values; suitable when outliers shouldn't dominate the loss.</p>"},{"location":"ml_concepts/#huber-loss","title":"Huber Loss","text":"<p>Combines benefits of MSE and MAE; acts as MSE for small errors and MAE for large errors. Quadratic for errors below threshold \u03b4, linear for errors above \u03b4. More robust to outliers than MSE while maintaining differentiability. Common \u03b4 values: 1.0 or 1.35. Used in robust regression when dataset contains outliers but gradient-based optimization is needed.</p>"},{"location":"ml_concepts/#binary-cross-entropy-log-loss","title":"Binary Cross-Entropy (Log Loss)","text":"<p>Loss function for binary classification: -[y log(p) + (1-y) log(1-p)], where y is true label (0 or 1) and p is predicted probability. Heavily penalizes confident wrong predictions; small penalty for correct predictions with high confidence. Outputs range from 0 (perfect) to infinity (worst). Equivalent to negative log-likelihood for Bernoulli distribution. Standard loss for logistic regression and binary classification neural networks.</p>"},{"location":"ml_concepts/#categorical-cross-entropy","title":"Categorical Cross-Entropy","text":"<p>Extension of binary cross-entropy for multi-class classification: -\u03a3 y_i log(p_i) across all classes. Compares one-hot encoded true labels with predicted probability distribution from softmax. Minimizing this loss is equivalent to maximizing log-likelihood. Used with softmax activation in neural networks for multi-class problems. Requires mutually exclusive classes (each sample belongs to exactly one class).</p>"},{"location":"ml_concepts/#sparse-categorical-cross-entropy","title":"Sparse Categorical Cross-Entropy","text":"<p>Functionally identical to categorical cross-entropy but accepts integer class labels instead of one-hot encoded vectors. Computationally more efficient for problems with many classes (hundreds or thousands). Example: class label is 5 instead of [0,0,0,0,0,1,0,...]. Commonly used in NLP tasks with large vocabularies and image classification with many categories.</p>"},{"location":"ml_concepts/#hinge-loss","title":"Hinge Loss","text":"<p>Loss function for maximum-margin classification, primarily used in SVMs: max(0, 1 - y\u00b7\u0177) where y \u2208 {-1, 1} and \u0177 is raw prediction. Encourages correct predictions to be beyond a margin; zero loss if prediction is correct and confident. Creates linear decision boundaries. Not probabilistic like cross-entropy; focuses on margin maximization. Used in SVMs and some neural network applications.</p>"},{"location":"ml_concepts/#focal-loss","title":"Focal Loss","text":"<p>Modification of cross-entropy that down-weights easy examples and focuses on hard examples: -\u03b1(1-p)^\u03b3 log(p) for positive class. Parameter \u03b3 (typically 2) controls how much to focus on hard examples; \u03b1 balances positive/negative classes. Addresses extreme class imbalance by reducing loss contribution from well-classified examples. Developed for object detection where easy negatives vastly outnumber hard positives. Particularly effective when 99%+ samples are easy negatives.</p>"},{"location":"ml_concepts/#kullback-leibler-kl-divergence","title":"Kullback-Leibler (KL) Divergence","text":"<p>Measures how one probability distribution differs from another: \u03a3 P(x) log(P(x)/Q(x)). Asymmetric measure (KL(P||Q) \u2260 KL(Q||P)); not a true distance metric. Used in variational autoencoders (VAEs) to match learned distribution to prior. Measures information loss when Q approximates P. Common in generative models and distribution matching tasks.</p>"},{"location":"ml_concepts/#6-optimization-algorithms","title":"6. Optimization Algorithms","text":"<p>Methods for updating model weights during training.</p>"},{"location":"ml_concepts/#sgd-stochastic-gradient-descent","title":"SGD (Stochastic Gradient Descent)","text":"<p>Updates weights using gradient computed from one random sample (or mini-batch) at a time. Faster per iteration than batch gradient descent and can escape local minima due to noise. Converges with fluctuations; learning rate scheduling often needed. Foundation for most modern optimizers.</p>"},{"location":"ml_concepts/#momentum","title":"Momentum","text":"<p>Enhancement to SGD that adds fraction of previous update to current update, helping accelerate in consistent directions. Reduces oscillations and speeds up convergence by building velocity in gradient direction. Typical momentum parameter: 0.9. Think of it as a ball rolling downhill gaining speed.</p>"},{"location":"ml_concepts/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<p>Combines momentum and adaptive learning rates; maintains per-parameter learning rates adapted based on gradient history. Computes adaptive learning rates from first (mean) and second (variance) moments of gradients. Widely used default optimizer; often works well with minimal tuning. Typical hyperparameters: \u03b2\u2081=0.9, \u03b2\u2082=0.999.</p>"},{"location":"ml_concepts/#rmsprop-root-mean-square-propagation","title":"RMSprop (Root Mean Square Propagation)","text":"<p>Adapts learning rate for each parameter based on recent gradient magnitudes using moving average of squared gradients. Divides learning rate by root of this average, preventing oscillations in steep directions. Effective for recurrent neural networks and non-stationary problems. Developed by Geoffrey Hinton.</p>"},{"location":"ml_concepts/#7-training-hyperparameters","title":"7. Training Hyperparameters","text":"<p>Key parameters that control how models learn from data.</p>"},{"location":"ml_concepts/#batch-size","title":"Batch Size","text":"<p>The number of training examples used in one iteration to update model weights. Smaller batches (1-32) provide noisy but frequent updates; larger batches (128-512) provide stable but less frequent updates. Affects training speed, memory usage, and convergence quality. Common values: 32, 64, 128, 256.</p>"},{"location":"ml_concepts/#mini-batch","title":"Mini-batch","text":"<p>A subset of the training data, larger than one example but smaller than the full dataset, used for gradient descent. Combines benefits of stochastic (fast, noisy updates) and batch (stable, accurate) gradient descent. Most common approach in modern deep learning. Typically ranges from 16 to 512 examples.</p>"},{"location":"ml_concepts/#learning-rate","title":"Learning Rate","text":"<p>Controls the step size when updating model weights during training; determines how quickly the model adapts to the problem. Too high causes unstable training or divergence; too low causes slow convergence or getting stuck. Often the most important hyperparameter to tune. Typical starting values: 0.001 to 0.1.</p>"},{"location":"ml_concepts/#8-model-performance-issues","title":"8. Model Performance Issues","text":"<p>Understanding when the model learns too much, too little, or just right.</p>"},{"location":"ml_concepts/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<p>Bias is error from wrong assumptions (underfitting); variance is error from sensitivity to training data fluctuations (overfitting). Models with high bias are too simple; high variance models are too complex. Optimal model minimizes total error = bias\u00b2 + variance + irreducible error. Core challenge in model selection.</p>"},{"location":"ml_concepts/#overfitting","title":"Overfitting","text":"<p>When a model learns training data too well, including its noise and peculiarities, causing poor performance on new data. The model memorizes rather than generalizes. Signs include high training accuracy but low validation accuracy. Solution: Use regularization, more data, or simpler models.</p>"},{"location":"ml_concepts/#underfitting","title":"Underfitting","text":"<p>When a model is too simple to capture underlying patterns in the data, resulting in poor performance on both training and test data. The model lacks the capacity to learn the relationship between features and targets. Signs include low accuracy on both training and validation sets. Solution: Use more complex models, add features, or train longer.</p>"},{"location":"ml_concepts/#9-regularization-techniques","title":"9. Regularization Techniques","text":"<p>Methods to prevent overfitting by constraining model complexity.</p>"},{"location":"ml_concepts/#l1-regularization-lasso","title":"L1 Regularization (Lasso)","text":"<p>Adds penalty equal to absolute value of coefficient magnitudes to the loss function, encouraging sparsity by driving some weights to exactly zero. Useful for feature selection as it automatically eliminates less important features. The penalty term is \u03bb\u2211|w|, where \u03bb controls regularization strength. Creates sparse models that are easier to interpret.</p>"},{"location":"ml_concepts/#l2-regularization-ridge","title":"L2 Regularization (Ridge)","text":"<p>Adds penalty equal to square of coefficient magnitudes to the loss function, discouraging large weights but keeping all features. Distributes weights more evenly across features rather than eliminating them. The penalty term is \u03bb\u2211w\u00b2, where \u03bb controls regularization strength. Preferred when all features are potentially relevant.</p>"},{"location":"ml_concepts/#dropout","title":"Dropout","text":"<p>Randomly ignores (drops) a percentage of neurons during each training iteration in neural networks. Forces network to learn redundant representations, preventing over-reliance on specific neurons. Typical dropout rates: 0.2-0.5. Only applied during training, not inference.</p>"},{"location":"ml_concepts/#early-stopping","title":"Early Stopping","text":"<p>Stops training when validation performance stops improving for specified number of epochs (patience). Prevents overfitting by avoiding unnecessary training iterations. Monitors validation loss and saves best model. Simple yet highly effective regularization technique.</p>"},{"location":"ml_concepts/#10-advanced-ml-techniques","title":"10. Advanced ML Techniques","text":"<p>Specialized machine learning approaches for specific use cases.</p>"},{"location":"ml_concepts/#transfer-learning","title":"Transfer Learning","text":"<p>Technique that leverages knowledge from a pre-trained model (trained on large dataset) and adapts it to a new, related task with limited data. Instead of training from scratch, reuse learned features from the source task. Common approach: freeze early layers (general features like edges, textures) and fine-tune later layers (task-specific features). Dramatically reduces training time, data requirements, and computational costs.</p> <p>When to Use:</p> <ul> <li>Limited labeled data for target task</li> <li>Target task is similar to source task (e.g., both are image classification)</li> <li>Want to leverage state-of-the-art pretrained models</li> </ul> <p>Common Approaches:</p> <ul> <li>Feature Extraction: Freeze all pretrained layers, only train new classifier on top</li> <li>Fine-tuning: Unfreeze some layers and retrain them with small learning rate</li> <li>Progressive Unfreezing: Gradually unfreeze layers from top to bottom during training</li> </ul> <p>Popular Pretrained Models:</p> <ul> <li>Vision: ResNet, VGG, EfficientNet, Vision Transformer (ViT)</li> <li>NLP: BERT, GPT, T5, RoBERTa</li> <li>Multi-modal: CLIP, Flamingo</li> </ul> <p>Example Workflow:</p> <ol> <li>Start with model pretrained on ImageNet (1.4M images, 1000 classes)</li> <li>Remove final classification layer</li> <li>Add new layer for specific classes (e.g., 10 classes)</li> <li>Freeze pretrained layers, train only new layer</li> <li>Optionally fine-tune top layers with low learning rate</li> </ol> <p>Benefits:</p> <ul> <li>Requires 10-100x less data than training from scratch</li> <li>Converges faster (hours vs days/weeks)</li> <li>Often achieves better performance, especially with limited data</li> <li>Reduces computational costs significantly</li> </ul> <p>GCP Implementation: Vertex AI AutoML uses transfer learning automatically with Google's pretrained models. For custom training, TensorFlow Hub and Hugging Face provide pretrained models.</p>"},{"location":"ml_concepts/#collaborative-filtering","title":"Collaborative Filtering","text":"<p>Recommendation technique that makes predictions based on preferences of similar users or items. User-based finds users with similar tastes; item-based finds similar items. Doesn't require explicit feature engineering, works from interaction patterns (ratings, purchases, clicks). Used by Netflix, Amazon, and Spotify for recommendations.</p>"},{"location":"patterns/","title":"Architectural Patterns and Design Principles","text":"<p>Section Overview: This section covers common ML architecture patterns, with detailed implementation guidance for both GCP and AWS.</p> <p>Learning Objectives:</p> <ul> <li>Recognize standard ML architecture patterns</li> <li>Choose appropriate services for different use cases</li> <li>Design scalable and cost-effective solutions</li> <li>Understand trade-offs between architectural approaches</li> </ul>"},{"location":"patterns/#31-real-time-prediction-architecture","title":"3.1 Real-Time Prediction Architecture","text":"<p>Pattern Description: Low-latency (&lt;100ms) prediction serving for interactive applications</p> <p>GCP Implementation: <pre><code>API Gateway / Cloud Load Balancer\n          \u2193\n    Cloud Run (API)\n          \u2193\n    Memorystore (Redis) - [Cache Layer]\n          \u2193\n  Vertex AI Endpoint\n  (Multiple models with traffic splitting)\n          \u2193\n    Model (deployed on n1-standard-4 + GPU)\n</code></pre></p> <p>AWS Implementation: <pre><code>API Gateway / Application Load Balancer\n          \u2193\n    Lambda or ECS (API)\n          \u2193\n    ElastiCache (Redis) - [Cache Layer]\n          \u2193\n  SageMaker Endpoint\n  (Multiple variants with traffic splitting)\n          \u2193\n    Model (deployed on ml.m5.xlarge + GPU)\n</code></pre></p> <p>Key Differences:</p> Aspect GCP AWS API Layer Cloud Run offers automatic scaling with zero-to-scale capability Lambda for serverless or ECS for containerized workloads Cache Memorystore for Redis is fully managed ElastiCache requires more configuration but offers more control Testing Support traffic splitting and A/B testing at the endpoint level Support traffic splitting and A/B testing at the endpoint level <p>When to Use This Pattern:</p> <ul> <li>Latency requirements &lt;100ms</li> <li>High request volume with repeated queries</li> <li>Need global availability</li> <li>Variable traffic patterns</li> </ul>"},{"location":"patterns/#32-batch-prediction-pipeline","title":"3.2 Batch Prediction Pipeline","text":"<p>Pattern Description: Large-scale offline inference for millions of predictions</p> <p>GCP Implementation: <pre><code>Cloud Scheduler\n      \u2193\nCloud Functions (Trigger)\n      \u2193\nBigQuery (Input Data) \u2192 Vertex AI Batch Prediction \u2192 BigQuery (Results)\n      \u2193\nDataflow (Post-processing)\n      \u2193\nBigQuery (Final Results)\n</code></pre></p> <p>AWS Implementation: <pre><code>EventBridge (CloudWatch Events)\n      \u2193\nLambda (Trigger)\n      \u2193\nAthena/Redshift (Input Data) \u2192 SageMaker Batch Transform \u2192 S3 (Results)\n      \u2193\nAWS Glue (Post-processing)\n      \u2193\nRedshift/Athena (Final Results)\n</code></pre></p> <p>Key Differences:</p> Aspect GCP AWS Data Storage BigQuery provides unified data warehouse and prediction input/output Separate services for storage (S3), querying (Athena/Redshift), and batch inference Processing Dataflow offers unified batch and stream processing Glue for batch ETL, separate from streaming (Kinesis) Scheduling Cloud Scheduler is dedicated service EventBridge/CloudWatch Events <p>When to Use This Pattern:</p> <p>Batch prediction is suitable when:</p> <ul> <li>Millions of predictions needed</li> <li>Not time-sensitive (can take hours)</li> <li>Cost optimization important</li> <li>Periodic/scheduled inference</li> </ul>"},{"location":"patterns/#33-streaming-ml-pipeline","title":"3.3 Streaming ML Pipeline","text":"<p>Pattern Description: Real-time feature engineering and prediction on streaming data</p> <p>GCP Implementation: <pre><code>Pub/Sub (Event Stream)\n      \u2193\nDataflow (Feature Engineering)\n      \u2193   \u2193   \u2193\n      |   |   +\u2192 BigQuery (Offline Storage)\n      |   +\u2192 Feature Store (Online)\n      +\u2192 Vertex AI Endpoint (Predictions)\n           \u2193\n      Pub/Sub (Prediction Results)\n           \u2193\n      Application (Actions)\n</code></pre></p> <p>AWS Implementation: <pre><code>Kinesis Data Streams (Event Stream)\n      \u2193\nKinesis Data Analytics / Flink (Feature Engineering)\n      \u2193   \u2193   \u2193\n      |   |   +\u2192 S3 + Athena (Offline Storage)\n      |   +\u2192 SageMaker Feature Store (Online)\n      +\u2192 SageMaker Endpoint (Predictions)\n           \u2193\n      Kinesis Data Streams (Prediction Results)\n           \u2193\n      Lambda / Application (Actions)\n</code></pre></p> <p>Key Differences:</p> Aspect GCP AWS Messaging Pub/Sub is fully managed message queue with global availability Kinesis Data Streams requires shard management Stream Processing Dataflow provides unified SDK (Apache Beam) for batch and stream Kinesis Analytics or managed Flink for stream processing Feature Store Feature Store integrated with Vertex AI ecosystem SageMaker Feature Store with separate integration requirements Multi-Destination Single Dataflow job can write to multiple destinations May need separate Lambda functions or Kinesis Firehose for multi-destination writes <p>When to Use This Pattern:</p> <p>Streaming ML pipelines excel when:</p> <ul> <li>Real-time decision making required</li> <li>Continuous event streams</li> <li>Need for feature aggregation across time windows</li> <li>Low-latency requirements (seconds, not milliseconds)</li> </ul>"},{"location":"patterns/#34-end-to-end-automl-workflow","title":"3.4 End-to-End AutoML Workflow","text":"<p>Pattern Description: Automated workflow from raw data to deployed model</p> <p>GCP Implementation (Vertex AI Pipelines): <pre><code>from kfp import dsl\nfrom google.cloud.aiplatform import pipeline_jobs\n\n@dsl.pipeline(name='automl-e2e-pipeline')\ndef automl_pipeline(\n    project_id: str,\n    dataset_uri: str,\n    target_column: str,\n    model_name: str,\n    budget_hours: int = 1\n):\n    # Create dataset, train AutoML model, create endpoint, deploy model\n    pass\n\n# Execute\njob = pipeline_jobs.PipelineJob(\n    display_name='automl-churn-pipeline',\n    template_path='automl_pipeline.yaml',\n    parameter_values={\n        'project_id': 'my-project',\n        'dataset_uri': 'gs://my-bucket/churn_data.csv',\n        'target_column': 'churned',\n        'model_name': 'churn-predictor',\n        'budget_hours': 2\n    }\n)\n\njob.run()\n</code></pre></p> <p>AWS Implementation (SageMaker Pipelines): <pre><code>from sagemaker.workflow.pipeline import Pipeline\nfrom sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep\nfrom sagemaker.sklearn.processing import SKLearnProcessor\nfrom sagemaker.estimator import Estimator\n\n# Define preprocessing step\nsklearn_processor = SKLearnProcessor(\n    framework_version='0.23-1',\n    role=sagemaker_role,\n    instance_type='ml.m5.xlarge',\n    instance_count=1\n)\n\n# Define AutoML training using Autopilot\nfrom sagemaker.automl.automl import AutoML\n\nautoml = AutoML(\n    role=sagemaker_role,\n    target_attribute_name='churned',\n    output_path='s3://my-bucket/autopilot-output',\n    max_candidates=10,\n    max_runtime_per_training_job_in_seconds=3600\n)\n\n# Create and execute pipeline\npipeline = Pipeline(\n    name='automl-churn-pipeline',\n    steps=[preprocessing_step, training_step, model_step, deployment_step]\n)\n\npipeline.upsert(role_arn=sagemaker_role)\nexecution = pipeline.start()\n</code></pre></p> <p>Key Differences:</p> Aspect GCP AWS Pipeline DSL Kubeflow Pipelines DSL with Python decorators (@dsl.pipeline) SageMaker Pipelines with explicit step definitions AutoML Integration AutoML is integrated component in Vertex AI SageMaker Autopilot is separate from Pipelines, requires integration Artifact Storage Vertex AI Pipelines stores artifacts in GCS automatically Must explicitly define S3 paths for all artifacts Model Registry Built-in model registry integration Requires explicit Model Registry step configuration"},{"location":"patterns/#35-mlops-pipeline-cicdct","title":"3.5 MLOps Pipeline (CI/CD/CT)","text":"<p>Pattern Description: Continuous Integration, Deployment, and Training for ML</p> <p>GCP Complete MLOps Architecture: <pre><code>Code Repository (Cloud Source Repo / GitHub)\n      \u2193\nCloud Build (CI: Test, Build, Push)\n      \u2193\nArtifact Registry (Container Storage)\n      \u2193\nVertex AI Pipelines (Training Pipeline)\n      \u2193\nModel Registry (Version Management)\n      \u2193\nVertex AI Endpoints (Deployment)\n      \u2193\nModel Monitoring (Drift Detection)\n      \u2193 (Trigger on Drift)\nCloud Functions \u2192 Re-trigger Training\n</code></pre></p> <p>AWS Complete MLOps Architecture: <pre><code>Code Repository (CodeCommit / GitHub)\n      \u2193\nCodePipeline + CodeBuild (CI: Test, Build, Push)\n      \u2193\nECR (Elastic Container Registry)\n      \u2193\nSageMaker Pipelines (Training Pipeline)\n      \u2193\nSageMaker Model Registry (Version Management)\n      \u2193\nSageMaker Endpoints (Deployment)\n      \u2193\nSageMaker Model Monitor (Drift Detection)\n      \u2193 (Trigger on Drift)\nLambda + EventBridge \u2192 Re-trigger Training\n</code></pre></p> <p>Key Differences:</p> Aspect GCP AWS CI/CD Cloud Build is unified CI/CD service with native GCP integration CodePipeline + CodeBuild require more configuration Artifact Storage Artifact Registry for both containers and language packages ECR for containers, CodeArtifact for packages (separate services) Monitoring Vertex AI provides integrated monitoring with automated triggers SageMaker Model Monitor requires manual EventBridge rule configuration Pipeline Triggering Cloud Functions directly trigger Vertex AI Pipelines Lambda triggers SageMaker Pipelines via SDK calls Console Single Vertex AI console for entire MLOps workflow Multiple consoles (CodePipeline, SageMaker, CloudWatch, Lambda) <p>Continuous Training Triggers:</p> Trigger Type GCP Implementation AWS Implementation Schedule Cloud Scheduler \u2192 Cloud Functions \u2192 Vertex AI Pipeline EventBridge (cron) \u2192 Lambda \u2192 SageMaker Pipeline Drift Detection Vertex AI Model Monitoring \u2192 Pub/Sub \u2192 Cloud Functions SageMaker Model Monitor \u2192 EventBridge \u2192 Lambda Data Quality TFDV + Cloud Functions \u2192 Vertex AI Pipeline Glue Data Quality \u2192 EventBridge \u2192 Lambda Manual Vertex AI Console / gcloud CLI SageMaker Console / AWS CLI <p>Key Takeaways:</p> <ul> <li>Know the difference between CI (code), CD (deployment), CT (continuous training)</li> <li>Understand trigger mechanisms: schedule, drift, data quality</li> <li>Remember approval gates for production deployment</li> <li>Know rollback strategies (traffic splitting, blue/green)</li> <li>GCP offers more integrated MLOps experience; AWS provides more flexibility with separate services</li> </ul>"},{"location":"reference/","title":"Quick Reference Tables","text":""},{"location":"reference/#gcp-ml-services-vs-aws-services","title":"GCP ML Services vs AWS Services","text":"GCP Service AWS Equivalent Primary Use Case Vertex AI Training SageMaker Training Custom model training Vertex AI AutoML SageMaker Autopilot/Canvas Automated ML Vertex AI Prediction SageMaker Endpoints Model serving Vertex AI Pipelines SageMaker Pipelines ML workflow orchestration Vertex AI Feature Store SageMaker Feature Store Feature management Vertex AI Model Monitoring SageMaker Model Monitor Drift detection BigQuery ML Redshift ML, Athena ML SQL-based ML Dataflow AWS Glue, Kinesis Analytics Data processing (code-based) Cloud Data Fusion AWS Glue Studio, Glue DataBrew Data integration (visual ETL) Dataproc Amazon EMR Spark/Hadoop workloads Pub/Sub Kinesis, SNS/SQS Messaging Cloud Storage Amazon S3 Object storage Cloud Build CodeBuild CI/CD Artifact Registry ECR Container registry Cloud Composer MWAA Workflow orchestration Vision AI Rekognition Image analysis Natural Language AI Comprehend Text analysis Translation API Translate Language translation Speech-to-Text Transcribe Speech recognition Text-to-Speech Polly Speech synthesis"},{"location":"reference/#when-to-use-which-service","title":"When to Use Which Service","text":"Use Case GCP Service Why Quick PoC with tabular data BigQuery ML or AutoML No ML expertise needed, SQL-based Custom deep learning Vertex AI Custom Training Full control, TPU access Large-scale batch predictions Vertex AI Batch Prediction Cost-effective, BigQuery integration Real-time predictions Vertex AI Endpoints Auto-scaling, managed infrastructure Streaming data processing Dataflow Unified batch/streaming, Apache Beam Visual ETL for ML data prep Cloud Data Fusion No-code, 150+ connectors, business user friendly Data warehouse analytics BigQuery Serverless, petabyte-scale ML pipeline orchestration Vertex AI Pipelines ML-specific, metadata tracking Feature engineering at scale Dataflow + TensorFlow Transform Training-serving consistency Time series forecasting BigQuery ML ARIMA_PLUS SQL-based, automatic seasonality Image classification (standard) Vision AI API Pre-trained, no training needed Custom image models Vertex AI AutoML Vision Custom classes, transfer learning"},{"location":"scenarios/","title":"Common Scenarios and Solutions","text":""},{"location":"scenarios/#scenario-1-real-time-fraud-detection","title":"Scenario 1: Real-Time Fraud Detection","text":"<p>Question: Design a system for real-time credit card fraud detection with &lt;50ms latency.</p> <p>Answer: <pre><code>User Transaction \u2192 API Gateway \u2192 Cloud Run (preprocessing)\n                                     \u2193\n                              Feature Store (online serving)\n                                     \u2193\n                              Vertex AI Endpoint (model)\n                                     \u2193\n                                   Decision\n</code></pre></p> <p>Key Points:</p> <ul> <li>Feature Store for low-latency feature retrieval</li> <li>Vertex AI Endpoint with GPU for fast inference</li> <li>Cloud Run for stateless API layer</li> <li>Consider caching for repeat requests</li> </ul>"},{"location":"scenarios/#scenario-2-large-scale-batch-predictions","title":"Scenario 2: Large-Scale Batch Predictions","text":"<p>Question: Process 100M predictions daily, cost-effectively.</p> <p>Answer:</p> <p>Follow this cost-effective approach:</p> <ul> <li>Use Vertex AI Batch Prediction (not online endpoints)</li> <li>Input from BigQuery, output to BigQuery</li> <li>Schedule with Cloud Scheduler</li> <li>Use preemptible VMs for 60-80% cost savings</li> <li>Partition output by date in BigQuery</li> </ul>"},{"location":"scenarios/#scenario-3-model-retraining-on-drift","title":"Scenario 3: Model Retraining on Drift","text":"<p>Question: Automatically retrain model when drift detected.</p> <p>Answer: <pre><code>Vertex AI Model Monitoring (drift detection)\n             \u2193\n        Pub/Sub alert\n             \u2193\n    Cloud Function (trigger)\n             \u2193\n  Vertex AI Pipeline (retraining)\n             \u2193\n    Model Registry (new version)\n             \u2193\n   Endpoint (canary deployment)\n</code></pre></p>"},{"location":"scenarios/#scenario-4-sql-analyst-needs-ml","title":"Scenario 4: SQL Analyst Needs ML","text":"<p>Question: Data analyst team wants to build ML models using SQL.</p> <p>Answer:</p> <p>BigQuery ML is the ideal solution:</p> <ul> <li>BigQuery ML is the right choice</li> <li>No Python/ML expertise required</li> <li>Data stays in BigQuery (no movement)</li> <li>Supports common model types (regression, classification, time series)</li> </ul>"},{"location":"scenarios/#scenario-5-training-serving-consistency","title":"Scenario 5: Training-Serving Consistency","text":"<p>Question: Ensure preprocessing same for training and serving.</p> <p>Answer:</p> <p>Use TensorFlow Transform to guarantee consistency:</p> <ul> <li>Use TensorFlow Transform (TFT)</li> <li>Define preprocessing_fn once</li> <li>Use in both training pipeline and serving</li> <li>Save transform function with model</li> <li>Apply same transform at inference time</li> </ul>"},{"location":"technical/","title":"Key Technical Aspects","text":"<p>Section Overview: This section provides deep technical knowledge of critical topics, with detailed AWS comparisons to leverage your existing expertise.</p> <p>Learning Objectives:</p> <ul> <li>Master Vertex AI training, deployment, and monitoring capabilities</li> <li>Understand data engineering patterns for ML</li> <li>Learn MLOps best practices on GCP</li> <li>Recognize optimal service choices for different scenarios</li> </ul>"},{"location":"technical/#1-platform-overview-vertex-ai-gcps-ml-platform","title":"1. Platform Overview: Vertex AI (GCP's ML Platform)","text":""},{"location":"technical/#11-introduction-to-vertex-ai","title":"1.1 Introduction to Vertex AI","text":"<p>Vertex AI is Google Cloud's unified ML platform that brings together all GCP ML services under a single interface. Launched in May 2021, it consolidates AI Platform, AutoML, and other ML tools into an integrated environment for the entire ML lifecycle.</p> <p>Core Philosophy:</p> <ul> <li>Unified platform reducing service fragmentation</li> <li>Managed infrastructure with minimal operational overhead</li> <li>Native integration with GCP data services (BigQuery, Cloud Storage, Dataflow)</li> <li>Focus on production ML workflows and MLOps</li> </ul> <p>Key Components:</p> Component Purpose Key Features Vertex AI Workbench Managed Jupyter notebooks JupyterLab, pre-installed ML libraries, Git integration, executor for scheduled notebooks Vertex AI Training Custom model training Pre-built containers, custom containers, distributed training, hyperparameter tuning Vertex AI Pipelines ML workflow orchestration Kubeflow Pipelines, TFX integration, DAG visualization, artifact tracking Vertex AI Prediction Model serving Online prediction (real-time), batch prediction, autoscaling, traffic splitting Vertex AI Feature Store Feature management Online serving, offline training, feature versioning, point-in-time lookup Vertex AI Model Registry Model versioning Model lineage, version control, deployment tracking, A/B testing Vertex AI Monitoring Model observability Training-serving skew, prediction drift, feature attribution, explanation AutoML No-code ML Tables, Vision, NLP, Video, structured data Vertex AI Experiments Experiment tracking Metrics, parameters, artifacts, comparison across runs Vertex AI Metadata Lineage tracking End-to-end tracking from data to deployed model <p>Architecture Overview:</p> <pre><code>Data Sources (BigQuery, GCS, Databases)\n           \u2193\n    Vertex AI Workbench (Development)\n           \u2193\n    Vertex AI Training (Custom/AutoML)\n           \u2193\n    Vertex AI Model Registry (Versioning)\n           \u2193\n    Vertex AI Endpoints (Serving)\n           \u2193\n    Vertex AI Monitoring (Observability)\n</code></pre>"},{"location":"technical/#12-sagemaker-reference-for-comparison","title":"1.2 SageMaker Reference (For Comparison)","text":"<p>Amazon SageMaker component reference for mapping to Vertex AI equivalents.</p> <p>Key Components:</p> Component Purpose Key Features SageMaker Studio Integrated ML IDE Notebooks, visual workflow, Git integration, debugger, profiler SageMaker Training Model training Built-in algorithms, custom containers, distributed training, spot instances SageMaker Pipelines ML workflow orchestration Python SDK, step caching, conditional execution, parameterized pipelines SageMaker Endpoints Model serving Real-time inference, serverless inference, multi-model endpoints, async inference SageMaker Feature Store Feature management Online/offline stores, feature groups, time-travel queries SageMaker Model Registry Model versioning Model packages, approval workflows, cross-account deployment SageMaker Model Monitor Model observability Data quality, model quality, bias drift, feature attribution SageMaker Autopilot AutoML Automatic model selection, feature engineering, hyperparameter tuning SageMaker Experiments Experiment tracking Run tracking, metric visualization, comparison SageMaker Clarify Bias detection Pre/post training bias, explainability, feature importance"},{"location":"technical/#13-vertex-ai-vs-sagemaker-key-differences","title":"1.3 Vertex AI vs SageMaker: Key Differences","text":""},{"location":"technical/#high-level-comparison","title":"High-Level Comparison","text":"Aspect Vertex AI SageMaker Notes Platform Maturity Newer (2021), consolidation of existing services More mature (2017), battle-tested Built on proven GCP services Ease of Use Simpler, more unified interface More features but steeper learning curve Streamlined experience Integration Seamless with GCP (BigQuery, Dataflow) Deep AWS ecosystem integration Focus on BigQuery integration patterns AutoML Capabilities Strong, built-in AutoML Tables, Vision, NLP Autopilot for structured data Broader AutoML coverage Notebook Experience Workbench with managed notebooks SageMaker Studio with more features Less feature-rich but simpler Pipeline Orchestration Kubeflow Pipelines (industry standard) SageMaker Pipelines (AWS-native) Portable across clouds Feature Store Newer, simpler setup More mature, advanced features Simpler but fewer advanced capabilities Model Monitoring Built-in, automated drift detection Comprehensive monitoring suite More automated, less configuration Explainability Vertex Explainable AI SageMaker Clarify Similar capabilities, different APIs"},{"location":"technical/#detailed-feature-mapping","title":"Detailed Feature Mapping","text":"Feature Vertex AI SageMaker Training \u2713 Pre-built containers\u2713 Custom containers\u2713 Distributed training\u2713 Reduction server for dist. training \u2713 Built-in algorithms (18+)\u2713 Custom containers\u2713 Distributed training\u2713 Spot training\u2713 Managed warm pools Hyperparameter Tuning \u2713 Bayesian, Grid, Random\u2713 Parallel trials\u2713 Early stopping \u2713 Bayesian, Grid, Random, Hyperband\u2713 Parallel jobs\u2713 Warm start Deployment Options \u2713 Online prediction\u2713 Batch prediction\u2713 Private endpoints \u2713 Real-time endpoints\u2713 Serverless inference\u2713 Batch transform\u2713 Async inference\u2713 Multi-model endpoints Autoscaling \u2713 CPU/GPU-based\u2713 Min/max instances \u2713 Target tracking\u2713 Scheduled scaling\u2713 Application autoscaling Data Processing \u2713 Dataflow integration\u2713 BigQuery ML \u2713 SageMaker Processing\u2713 Glue integration MLOps \u2713 Vertex Pipelines\u2713 Model Registry\u2713 Metadata tracking \u2713 SageMaker Pipelines\u2713 Model Registry\u2713 Experiments\u2713 Projects"},{"location":"technical/#14-key-advantages-of-vertex-ai","title":"1.4 Key Advantages of Vertex AI","text":"<p>Vertex AI Strengths:</p> <ul> <li>Unified Interface: Less service fragmentation compared to SageMaker's many specialized tools</li> <li>BigQuery Integration: Direct ML on data warehouse without data movement (similar to Redshift ML but more mature)</li> <li>AutoML Coverage: Broader AutoML support across Tables, Vision, NLP, and Video</li> <li>Kubeflow Pipelines: Industry-standard orchestration (portable across clouds)</li> <li>Simpler Pricing: Easier cost estimation with fewer pricing dimensions</li> </ul> <p>What You'll Miss from SageMaker:</p> <ul> <li>Serverless inference and async endpoints (Vertex AI has online/batch only)</li> <li>Multi-model endpoints (serve multiple models from single endpoint)</li> <li>Spot training for cost optimization</li> <li>More mature Feature Store with advanced capabilities</li> <li>SageMaker Studio's richer IDE experience</li> </ul>"},{"location":"technical/#15-service-name-mapping","title":"1.5 Service Name Mapping","text":"Functionality Vertex AI SageMaker Development Environment Vertex AI Workbench SageMaker Studio Managed Notebooks Workbench Instances SageMaker Notebook Instances Model Training Vertex AI Training SageMaker Training Jobs AutoML Vertex AI AutoML SageMaker Autopilot Model Serving Vertex AI Endpoints SageMaker Endpoints Batch Inference Vertex AI Batch Prediction SageMaker Batch Transform Pipeline Orchestration Vertex AI Pipelines SageMaker Pipelines Experiment Tracking Vertex AI Experiments SageMaker Experiments Model Versioning Vertex AI Model Registry SageMaker Model Registry Feature Management Vertex AI Feature Store SageMaker Feature Store Model Monitoring Vertex AI Model Monitoring SageMaker Model Monitor Explainability Vertex Explainable AI SageMaker Clarify Bias Detection Vertex AI (within monitoring) SageMaker Clarify Data Labeling Vertex AI Data Labeling SageMaker Ground Truth"},{"location":"technical/#2-deep-dive-technical-implementation","title":"2. Deep Dive: Technical Implementation","text":""},{"location":"technical/#21-model-training","title":"2.1 Model Training","text":""},{"location":"technical/#custom-training-on-vertex-ai","title":"Custom Training on Vertex AI","text":"<p>Training Job Types:</p> <ul> <li>Pre-built Containers: Use Google's managed containers for TensorFlow, PyTorch, scikit-learn</li> <li>Custom Containers: Bring your own Docker image for any framework</li> <li>Python Packages: Submit Python training application without containers</li> </ul>"},{"location":"technical/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>GCP - Vertex AI Hyperparameter Tuning: <pre><code>from google.cloud import aiplatform\nfrom google.cloud.aiplatform import hyperparameter_tuning as hpt\n\n# Define hyperparameter search space\njob = aiplatform.CustomTrainingJob(\n    display_name='hp-tuning-job',\n    container_uri='gcr.io/my-project/trainer:latest'\n)\n\nhp_job = aiplatform.HyperparameterTuningJob(\n    display_name='fraud-detection-tuning',\n    custom_job=job,\n    metric_spec={\n        'auc': 'maximize',  # Optimization objective\n    },\n    parameter_spec={\n        'learning_rate': hpt.DoubleParameterSpec(min=0.0001, max=0.1, scale='log'),\n        'batch_size': hpt.DiscreteParameterSpec(values=[16, 32, 64, 128]),\n        'num_layers': hpt.IntegerParameterSpec(min=1, max=5, scale='linear'),\n        'dropout': hpt.DoubleParameterSpec(min=0.1, max=0.5, scale='linear')\n    },\n    max_trial_count=50,\n    parallel_trial_count=5,\n    search_algorithm='random',  # or 'grid', 'bayesian'\n    max_failed_trial_count=10\n)\n\nhp_job.run()\n</code></pre></p>"},{"location":"technical/#distributed-training","title":"Distributed Training","text":"<p>Strategies:</p> <ul> <li>Data Parallelism: Replicate model across multiple workers, split data</li> <li>Model Parallelism: Split large model across multiple workers</li> <li>Reduction Server: GCP-specific feature for efficient gradient aggregation</li> </ul>"},{"location":"technical/#gpu-and-tpu-selection","title":"GPU and TPU Selection","text":"<p>TPU (Tensor Processing Unit) - GCP Exclusive:</p> <ul> <li>Custom-designed for matrix operations in neural networks</li> <li>Significantly faster for large models (BERT, ResNet, etc.)</li> <li>Cost-effective at scale</li> <li>Best for TensorFlow models</li> </ul> <p>When to Use TPUs:</p> <ul> <li>Large batch sizes (&gt;64)</li> <li>Models dominated by matrix multiplications</li> <li>TensorFlow-based training</li> <li>Need for maximum throughput</li> </ul>"},{"location":"technical/#22-model-deployment-and-serving","title":"2.2 Model Deployment and Serving","text":""},{"location":"technical/#vertex-ai-prediction---online-vs-batch","title":"Vertex AI Prediction - Online vs Batch","text":"<p>Online Prediction (Real-time):</p> <ul> <li>Low-latency requirements (&lt;100ms)</li> <li>Interactive applications</li> <li>Real-time fraud detection</li> <li>Recommendation systems</li> </ul> <p>Batch Prediction:</p> <ul> <li>Large-scale inference (millions of predictions)</li> <li>Non-time-sensitive workloads</li> <li>Cost optimization (no always-on infrastructure)</li> <li>Regular scheduled predictions</li> </ul>"},{"location":"technical/#model-versioning-and-traffic-splitting","title":"Model Versioning and Traffic Splitting","text":"<p>Use Case: Deploy new model version alongside existing version for canary or A/B testing</p> <p>Deployment Strategies:</p> <ul> <li>Blue/Green: Deploy new version, test, then switch all traffic</li> <li>Canary: Gradually increase traffic to new version (10% \u2192 25% \u2192 50% \u2192 100%)</li> <li>A/B Testing: Split traffic evenly for statistical comparison</li> </ul>"},{"location":"technical/#23-data-engineering-for-ml","title":"2.3 Data Engineering for ML","text":""},{"location":"technical/#vertex-ai-feature-store","title":"Vertex AI Feature Store","text":"<p>Architecture:</p> <ul> <li>Online Store: Low-latency serving for real-time predictions (milliseconds)</li> <li>Offline Store: Batch access for training data (BigQuery-based)</li> </ul> <p>Concepts:</p> <ul> <li>Feature: Individual measurable property (e.g., age, total_purchases)</li> <li>Entity: Object being modeled (e.g., customer, product)</li> <li>Entity Type: Collection of features for an entity</li> <li>Feature Store: Container for multiple entity types</li> </ul>"},{"location":"technical/#data-preprocessing-with-dataflow-and-tensorflow-transform","title":"Data Preprocessing with Dataflow and TensorFlow Transform","text":"<p>TensorFlow Transform (TFT) provides:</p> <ul> <li>Consistent preprocessing between training and serving</li> <li>Full-pass statistics over entire dataset</li> <li>TensorFlow graph-based transformations</li> </ul>"},{"location":"technical/#24-ml-pipeline-orchestration","title":"2.4 ML Pipeline Orchestration","text":""},{"location":"technical/#vertex-ai-pipelines-kubeflow-pipelines","title":"Vertex AI Pipelines (Kubeflow Pipelines)","text":"<p>Key Concepts:</p> <ul> <li>Component: Reusable, self-contained unit of work</li> <li>Pipeline: DAG of components</li> <li>Artifact: Data produced/consumed by components</li> <li>Metadata: Lineage and execution tracking</li> </ul>"},{"location":"technical/#25-model-monitoring-and-management","title":"2.5 Model Monitoring and Management","text":""},{"location":"technical/#vertex-ai-model-monitoring","title":"Vertex AI Model Monitoring","text":"<p>Types of Monitoring:</p> <ul> <li>Training-Serving Skew: Distribution difference between training and serving data</li> <li>Prediction Drift: Change in prediction distribution over time</li> <li>Feature Attribution: Understanding which features drive predictions</li> </ul> <p>Monitoring Metrics:</p> <ul> <li>Jensen-Shannon divergence: Measure distribution difference</li> <li>Normalized difference: Feature-level comparison</li> <li>Custom thresholds: Per-feature sensitivity</li> </ul>"},{"location":"technical/#26-mlops-and-cicd","title":"2.6 MLOps and CI/CD","text":""},{"location":"technical/#continuous-training-ct-architecture","title":"Continuous Training (CT) Architecture","text":"<p>Components:</p> <ul> <li>Data Monitoring: Detect data quality issues or drift</li> <li>Trigger: Initiate retraining based on schedule or drift</li> <li>Training Pipeline: Execute model training</li> <li>Validation: Evaluate new model performance</li> <li>Deployment: Deploy if better than current model</li> </ul>"},{"location":"technical/#model-registry-and-versioning","title":"Model Registry and Versioning:","text":"<pre><code>from google.cloud import aiplatform\n\n# Register model with version and metadata\nmodel = aiplatform.Model.upload(\n    display_name='fraud-detector',\n    artifact_uri='gs://my-bucket/models/fraud-detector-v1.5/',\n    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest',\n    labels={\n        'version': 'v1.5',\n        'framework': 'scikit-learn',\n        'task': 'classification',\n        'team': 'fraud-ml'\n    },\n    description='Fraud detection model trained on 2024-01-15',\n    model_id='fraud-detector-v15'\n)\n\n# Add version alias\nmodel.add_version_aliases(['production', 'latest'])\n</code></pre> <p>Key Takeaways:</p> <ul> <li>Know triggers for CT: schedule, drift, data changes, manual</li> <li>Understand model versioning and aliases</li> <li>Remember approval workflows (dev \u2192 staging \u2192 production)</li> <li>Know rollback strategies</li> </ul>"}]}